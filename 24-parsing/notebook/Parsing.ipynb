{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "This concept will help you understand the ambiguity of langauge and the need for Parsing to eliminate the same\n",
    "\n",
    "# Overview\n",
    "\n",
    "The concept will introduce you to the concept of Parsing. In the concept you will learn\n",
    "\n",
    "- Language Ambiguity problem\n",
    "\n",
    "- POS tagging\n",
    "\n",
    "- Syntatic Parsing\n",
    "\n",
    "- Statistical Parsing\n",
    "\n",
    "- Evaluating Parsers\n",
    "\n",
    "# Pre-requisite\n",
    "\n",
    "Before you start learning this concept, be sure you have already covered\n",
    "\n",
    "- Data wrangling with Pandas\n",
    "- Manipulating Data with NumPy\n",
    "- Summarizing Data with Statistics\n",
    "- Foundations of Text Analytics\n",
    "\n",
    "\n",
    "# Learning Outcomes\n",
    "\n",
    "By the end of this concept, you will be able to do the following\n",
    "\n",
    "- Understand the ambiguity in natural language sentences\n",
    "\n",
    "- Learn what Parts-of-Speeech tagging is and how to implement it\n",
    "\n",
    "- Learn what CFG and Dependency Grammar are\n",
    "\n",
    "- Learn what Parsing is, what are it's types and how to implement it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. POS tagging\n",
    "\n",
    "Description:\n",
    "\n",
    "This chapter introduces you to the problem of ambiguity in langauge and how parts of speech tagging is the first step towards tackling that problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Ambiguity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we have repeatedly stated from the beginning of NLP course is that human languages are inherently different from programming languages. Let's try to delve a little deep into that and understand how this difference in human languages leads to a problem.\n",
    "\n",
    "\n",
    "**Ambiguity Problem**\n",
    "\n",
    "Consider the following sentence:\n",
    "\n",
    "\"Adam saw Gina with a telescope\"\n",
    "\n",
    "These sentences can have quite a few intepretations like:\n",
    "\n",
    "1. One intepretation where 'Adam saw Gina and she was holding a telescope'\n",
    "\n",
    "2. One intepretation where 'Adam saw Gina using a telescope'\n",
    "\n",
    "\n",
    "One obvious way of removing the ambiguity will be to add better punctuation or brackets.\n",
    "You could rewrite the sentence as:\n",
    "\n",
    "\"{{Adam saw {Gina with a telescope}}\"\n",
    "\n",
    "The problem with this approach is that if you insist on adding commas and brackets everywhere to eliminate ambiguity, you are not a natural language user anymore.\n",
    "\n",
    "\n",
    "\n",
    "This ambiguity problem becomes even worse if you add one more phrase to the sentence? \n",
    "\n",
    "For eg: 'Adam saw Gina on a tree with a telescope'\n",
    "\n",
    "You can see the additional phrase `on a tree` increases the complexity of the sentence.\n",
    "\n",
    "Other examples of ambiguous sentences include:\n",
    "\n",
    "- Time flies like an arrow\n",
    "\n",
    "- Frightening kids can cause trouble\n",
    "\n",
    "- Girl paralyzed after tumor fights back to win boxing championship\n",
    "\n",
    "Note: Try to figure out the different intepretations of the above sentences as an exercise.\n",
    "\n",
    "We try to resolve this ambiguity using a method called `Parsing`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Parsing**\n",
    "\n",
    "Parsing means associating tree structures to a sentence, given a grammar. This structure allow us to compress our description(meaning) of a sentence\n",
    "\n",
    "Following is an example of a parse tree of the sentence: \"Adam kicked the ball\"\n",
    "\n",
    "\n",
    "![](../images/parsetree_1.jpg)\n",
    "\n",
    "\n",
    "Before we go into more details about how Parsing works, let's briefly look at the applications of Parsing:\n",
    "\n",
    "***\n",
    "- ***Grammar Checking:*** Parsing is very useful in checking the validity of grammar is word-prcessing systems. A sentence that cannot be parsed is usually considered to have grammatical errors. \n",
    "\n",
    "\n",
    "- ***Question Answering:*** For the machine to effectively answer questions like \"What movies were directed by Mexican men before 1950?\" it needs to know that \"movies\" is the subject of the sentence. To answer that you need to parse the question to extract the subject and object of the sentence.\n",
    "\n",
    "\n",
    "- ***Language Modeling:*** Another important task that Parsing achieves is it helps calculate the probabilities of occurence of sentences as whole. For tasks like Speech Recognition and Machine Translation, this helps lock down the most likely sentences.\n",
    "\n",
    "***\n",
    "\n",
    "Getting back to our discussion, let's look at the Parse tree again:\n",
    "![](../images/parsetree_1.jpg)\n",
    "\n",
    "\n",
    "What do the symbol N, V, P, VP, NP represent?\n",
    "\n",
    "These symbols are what is known as POS tags in a sentence.\n",
    "\n",
    "\n",
    "Let's understand in detail about these POS tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 What is POS?\n",
    "\n",
    "In human english language, part of speech explains how a word is used in a sentence. \n",
    "\n",
    "There are eight main parts of speech - nouns, pronouns, adjectives, verbs, adverbs, prepositions, conjunctions and interjections. \n",
    "\n",
    "***Noun (N)-*** Adam, paris, chair,sadness\n",
    "\n",
    "***Verb (V)-*** code, learn, play, run\n",
    "\n",
    "***Adjective(ADJ)-*** small, green, old, fun, four\n",
    "\n",
    "***Adverb(ADV)-*** quickly, tomorrow, actively\n",
    "\n",
    "***Preposition (P)-*** at, on, in, from, with\n",
    "\n",
    "***Conjunction (CON)-*** and, or, but, because, so\n",
    "\n",
    "***Pronoun(PRO)-*** I, you, he, she, they\n",
    "\n",
    "***Interjection (INT)-*** Oh! Hey! Hi! Wohoo! \n",
    "\n",
    "\n",
    "This Parts of speech tags are useful because they reveal a lot about a word and its neighbors. \n",
    "\n",
    "Knowing what part of speech a word is helps us know about the likely neighboring words.\n",
    "\n",
    "For e.g. Verbs are preceded by nouns, Nouns are preceded by determiners and adjectives, etc making part-of-speech tagging a very important part of parsing \n",
    "\n",
    "\n",
    "These 8 tags though are not easy to convert in machine language.\n",
    "\n",
    "One of the most widely used tagset(dataset containing tags) in English is the Penn Treebank tagset (Marcus et al., 1993) containing 45 tags.\n",
    "\n",
    "Following are the 45 tags\n",
    "\n",
    "![](../images/pos_tags.PNG)\n",
    "\n",
    "\n",
    "Identifying the correct POS tag for a word is a difficult task because one word can have two different tags depending on the sentence.\n",
    "\n",
    "For eg: The use of the word bear as a noun in the first sentence and as a verb in the second sentence.\n",
    "\n",
    "- The hunter saw a bear.\n",
    "\n",
    "- All your work will eventually bear fruit.\n",
    "\n",
    "**Rule Based Tagging:**\n",
    "\n",
    "One of the earliest methods for POS tagging involved using a large database of hand-written disambiguation rules. \n",
    "\n",
    "For example, if the preceding word is an article then the current word is a noun. \n",
    "\n",
    "This information is coded in the form of rules.\n",
    "\n",
    "Though achieving good enough performance, the problem with this method is the frequent updation of tags and rules. \n",
    "\n",
    "**Most Frequent Class Baseline**\n",
    "\n",
    "One of the most intuitive methods involved assigning tags with the most frequent class they are assigned to.\n",
    "\n",
    "For e.g. The word 'paper' is most likely to be a noun than any other POS tag.\n",
    "\n",
    "That obviously doesn't work for lot of words. For e.g. \"show\" which is likely to be either a verb or a noun. \n",
    "\n",
    "It's performance while training on on the popular [Wall Steet Journal](https://catalog.ldc.upenn.edu/docs/LDC95S24/wsjcam0.html) training corpus get us an accuracy of 92.34% (Rule based methods, Neural Networks, Hidden Markov Models(to be discussed next) all achieved 97% accuracy).\n",
    "\n",
    "\n",
    "**POS tagging using HMM**\n",
    "\n",
    "Hidden Markov Model(HMM) is another method used for POS tagging.\n",
    "\n",
    "Before describing this method, let's first understand what a Markov Model is.\n",
    "\n",
    "You can get a broad understanding what markov model is by going through this video [Markov Model by Udacity](https://www.youtube.com/watch?v=4XqWadvEj2k) \n",
    "\n",
    "***Markov Model***\n",
    "\n",
    "Suppose you are a professional baseball player and baseball world championship is going to be conducted next month. Before that you want to practice every morning to perfect your gameplay. Before setting out to practice every morning, you have to clean all your baseball equipment and pack food accordingly. Consider that there are only three types of weather: Sunny, Cloudy, Rainy. You can only go and practice if it's not rainy. Therefore you want to make prediction of every morning's weather accurately. \n",
    "\n",
    "How can you make prediction of today's weather based on the weather of past N days?\n",
    "\n",
    "Following are the different probabilities you calculated based on the data:\n",
    "\n",
    "- P(Sunny|Sunny)= 0.7\n",
    "- P(Cloudy|Sunny)= 0.15\n",
    "- P(Rainy|Sunny)= 0.15\n",
    "\n",
    "\n",
    "- P(Sunny|Cloudy)= 0.2\n",
    "- P(Cloudy|Cloudy)= 0.5\n",
    "- P(Rainy|Cloudy)= 0.3\n",
    "\n",
    "\n",
    "- P(Sunny|Rainy)= 0.3\n",
    "- P(Cloudy|Rainy)= 0.5\n",
    "- P(Rainy|Rainy)= 0.2\n",
    "\n",
    "\n",
    "\n",
    "It can be written as the following 3 X 3  transition matrix:\n",
    "\n",
    "|-|Sunny|Cloudy|Rainy|\n",
    "|---|---|------|-----|\n",
    "|Sunny|0.7|0.15|0.15|\n",
    "|Cloudy|0.2|0.5|0.3|\n",
    "|Rainy|0.3|0.5|0.2|\n",
    "\n",
    "Here entry (i, j) is the probability of transitioning from state i to state j. \n",
    "\n",
    "Note that The transition matrix must be a stochastic matrix i.e. a matrix whose entries in each row must add up to exactly 1. \n",
    "\n",
    "Following is another example of stoichastic matrix:\n",
    "\n",
    "$P= \\begin{bmatrix} 0.1  & 0.2 & 0.3 & 0.4 \\\\ 0.9 & 0 & 0 & 0 \\\\ 0 & 0.8 & 0 & 0\\\\ 0 & 0 & 0.7 & 0.6 \\end{bmatrix}$\n",
    "\n",
    "\n",
    "*Q:* How does the transition matrix become a stoichastic matrix?\n",
    "\n",
    "*A.* Each row in a transition matrix represents all the transition probabilities from that particular i to different states j. All probabilities for a particular event must add up to 1, hence the row values of transition matrix always adds up to 1.\n",
    "\n",
    "\n",
    "\n",
    "Knowing the states and the transition probabilites, we can create our markov chain in the following way:\n",
    "\n",
    "![](../images/markov_chain.jpg)\n",
    "\n",
    "\n",
    "The circles represent the states(Sunny, Rainy, Cloudy in our case). The values written above the arrows represent the transition probabilities from the `state` at the tail of the arrow to the `state` at the head of the arrow.\n",
    "\n",
    "For e.g. \n",
    "\n",
    "- The transition probability is 0.5 when moving from Rainy to Cloudy.\n",
    "\n",
    "- The transition probability is 0.7 when moving from Sunny to Sunny\n",
    "\n",
    "\n",
    "\n",
    "Using the chain, we can then calculate the probability using the following probability:\n",
    "\n",
    "$ P(q_1,...,q_n)=  \\prod_{i=1}^n P(q_i|q_{i-1}) $\n",
    "\n",
    "You must have noticed that here to calculate the probability of state `i`, we are only looking at the probability of state `i-1` that's because we are assuming `Markov Property` while calculating\n",
    "\n",
    "\n",
    "*Q:* What is Markov Property?\n",
    "\n",
    " \n",
    "*A:*  Conditional probability distribution of future states of the process depends only upon the present state, and none of the previous states have any impact.\n",
    "\n",
    "\n",
    "So if you had to calculate the following probabilty:\n",
    "\n",
    "Given that today is Rainy, what's the probability tomorrow is Sunny and day after is Sunny?\n",
    "\n",
    "We can find the solution in the following way:\n",
    "\n",
    "$ P(q_2,q_3|q_1)$ \n",
    "\n",
    "$= P(q_3|q_2,q_1)\\times P(q_2|q_1)$\n",
    "\n",
    "$= P(q_3|q_2)\\times P(q_2|q_1) $\n",
    "\n",
    "$= P(Sunny|Sunny)\\times P(Sunny|Rainy)$\n",
    "\n",
    "$= (0.7) \\times (0.3)$\n",
    "\n",
    "$= 0.21$\n",
    "\n",
    "Just to again reinforce Markov property, consider another probability:\n",
    "\n",
    "Given day before yesterday the weather was Sunny, yesterday the weather was Sunny, today the weather is Rainy, what's the probability that tomorrow is Sunny?\n",
    "\n",
    "$ P(q_4|q_1,q_2,q_3)$ \n",
    "\n",
    "$= P(q_4|q_3)$\n",
    "\n",
    "$= P(Sunny|Rainy) $\n",
    "\n",
    "$= (0.3)$\n",
    "\n",
    "In the above calculation, we only take into consideration today's weather, nothing more. That is because Markov Model only uses information about the previous day weather and does not consider the weather based on the past 2 days,3 days,etc\n",
    "\n",
    "With Markov Model clear, let's now explore what a Hidden Markov Model is.\n",
    "\n",
    "\n",
    "\n",
    "## 1.3 Hidden Markov Models\n",
    "\n",
    "We just saw Markov chain is useful for calculating the probability for a sequence of observable events. \n",
    "\n",
    "However, there are cases where we are interested in hidden events like our POS tagging\n",
    "\n",
    "We don’t normally see or write part-of-speech tags in a text. We see words and and then infer the tags from the word sequence. For POS tagging, HMM allows us to take into consideration both observed event (words of the sentence) and hidden events(POS Tags) that we know are the causal factors in our probabilistic model. \n",
    "\n",
    "Let's try to understand this concept of `'Hidden'` in an HMM better using the same baseball example.\n",
    "\n",
    "Now seeing your pratice and dedication, your friend decided to help you by offering you his indoor baseball stadium. This means you can practice irrespective of whether it's raining or not(Yay!).\n",
    "\n",
    "Now your coach wants to create an optimum diet for you based on the type of activity you do during the practice session. Every practice session, you plan to do either one of the activities:\n",
    "\n",
    "- Swing practice\n",
    "\n",
    "- Cardio Run\n",
    "\n",
    "- Catch Practice\n",
    "\n",
    "Depending upon the mood, you choose either of them. This decison is also affected by the weather as depending on the stadium, your choice varies(For e.g Practicing swing practice during rainy days is not optimum). This leaves the coach in dilemma. Based on the data, following are the probabilities:\n",
    "\n",
    "\n",
    "|-|Sunny|Rainy|\n",
    "|---|---|------|\n",
    "|Sunny|0.85|0.15|\n",
    "|Rainy|0.7|0.3|\n",
    "\n",
    "From HMM perspective, these are what is called as `Transition Probabilities`(which represent the probability of transitioning to another state given a particular state).\n",
    "\n",
    "**Note:** We have simplified the weather condition to rainy or sunny(not rainy) for better comprehension purposes\n",
    "\n",
    "We also have the following set of probabilities:\n",
    "\n",
    "|-|Swing|Cardio|Catch|\n",
    "|---|---|------|-----|\n",
    "|Sunny|0.6|0.15|0.25|\n",
    "|Rainy|0.1|0.5|0.4|\n",
    "\n",
    "From HMM perspective, these are what is called as `Emission Probabilities`(which represent the probabilities of making certain observations given a particular state).\n",
    "\n",
    "Following is the Hidden Markov Model Chain made using the above set of probabilities:\n",
    "\n",
    "![](../images/hmm_up.jpg)\n",
    "\n",
    "Just like in MM chain, here also we have states(`Rainy`,`Sunny`). Accompanied with them are arrows with transition probabilities. \n",
    "\n",
    "For e.g. The transition probability is 0.15 when moving from `Sunny` to `Rainy`\n",
    "\n",
    "Along with states, we also have observations (`Swing`, `Cardio`, `Catch`) that the states can make. The dotted arrows that go from state to observations are the emission probabilities.\n",
    "\n",
    "For e.g. The probability that given `Rainy` state, you will do `Cardio` is 0.5\n",
    "\n",
    "In Hidden Markov Model as you might have inferred, it is the `observations` that are referred to as `hidden`. They are hidden not in the literal sense but you can infer these observations only after you reach one of the states. In other words, they are hidden until you reach one of the given states. \n",
    "\n",
    "Given the above HMM chain, if you had to calculate the following probabilty:\n",
    "\n",
    "Today is Rainy and you did cardio run, what's the probability tomorrow is Sunny and you will do swing pratice?\n",
    "\n",
    "\n",
    "You would calculate it as follows:\n",
    "\n",
    "\n",
    "$ P(q_2,o_2) $ \n",
    "\n",
    "$= P(q_2|q_1)\\times P(o_2|q_2)$\n",
    "\n",
    "$= P(Sunny|Rainy)\\times P(Swing|Sunny)$\n",
    "\n",
    "$= (0.7) \\times (0.6)$\n",
    "\n",
    "$= 0.35$\n",
    "\n",
    "\n",
    "\n",
    "Mathematically put, given a transition sequence $Q$ and observed sequence $O$, we can calculate the probability using the following formula:\n",
    "\n",
    "$ P(O,Q)=  \\prod_{i=1}^n P(o_i|q_i)  \\times  \\prod_{i=1}^n P(q_i|q_{i-1}) $\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "Let's now look at an example of HMM tagging with respect to POS.\n",
    "\n",
    "Consider the following two POS tagged sentences:\n",
    "\n",
    "![](../images/parsetree_4.jpg)\n",
    "\n",
    "Almost all probabilities in the two sequences are identical except for the word `box`?\n",
    "\n",
    "***\n",
    "Let's understand how HMM tagging helps resolve this issue.\n",
    "\n",
    "**Step 1:**\n",
    "\n",
    "Most of the probabilities will be the same. \n",
    "\n",
    "So we start by calculating the first different tag transition prob.\n",
    "\n",
    "For first sentence it will be the transition prob. from `TO` tag to `VB` tag i.e. $P(VB|TO)$ \n",
    "\n",
    "For second sentence it will be the transition prob. from `TO` tag to `NN` tag $P(NN|TO)$\n",
    "\n",
    "**Step 2:**\n",
    "\n",
    "Next we have to consider the likelihood of the word `box` given that particular speech tag.\n",
    "\n",
    "For first sentence it will be the prob. of the word `box` having `VB` tag i.e. $P(box|VB)$\n",
    "\n",
    "For second sentence it will be the prob. of the word `box` having `NN` tag i.e. $P(box|NN)$\n",
    "\n",
    "**Step 3:**\n",
    "\n",
    "Finally we need to calculate the other different tag sequence prob. for the tag following our ambiguos tag(tag NR in our case)\n",
    "\n",
    "For first sentence it will be the transition prob. from `VB` tag to `NR` tag i.e. $P(NR|VB)$ \n",
    "\n",
    "For second sentence it will be the transition prob. from `NN` tag to `NR` tag $P(NR|NN)$\n",
    "\n",
    "\n",
    "Finally to resolve the ambiguity, we need to multiply the three differnt probabilities and compare them.\n",
    "\n",
    "i.e compare `P(NN|TO) x P(NR|NN) x P(box|NN)` with `P(VB|TO) x P(NR|VB) x P(box|VB)`\n",
    "\n",
    "***\n",
    "To better understand the underlying maths in brief you can through the video on [Hidden Markov Model by Sudhanshu Bahety](https://www.youtube.com/watch?v=uIYSEZNYbgo) \n",
    "\n",
    "\n",
    "*Dive Deeper(Optional):*\n",
    "\n",
    "\n",
    "In the above example, HMM helped resolve the ambiguity because tags we already present. This leads us to the question of how to assign tags to the given/observed words in the sentence.\n",
    "\n",
    "\n",
    "In other words, we understand that we need a task of determining which sequence of variables(these are hidden in HMM) is the underlying source of the given sequence of observations.\n",
    "\n",
    "For e.g. Given a word sentence(words are the observation), we need to identify which is the most likely sequence(POS tag sequence).\n",
    "\n",
    "For that we use a decoding algorithm called Viterbi. You can read about it more [here](http://cecas.clemson.edu/~ahoover/ece854/refs/Gonze-ViterbiAlgorithm.pdf)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging\n",
    "\n",
    "NLTK library has prebuilt pos tagging libraries for you to tag words called `pos_tag`. Following is a snippet for the same:\n",
    "\n",
    "**INPUT**\n",
    "```python\n",
    "import nltk\n",
    "\n",
    "sentence='I love Data'\n",
    "\n",
    "tokens= nltk.word_tokenize(sentence)\n",
    "tagged_sentence= nltk.pos_tag(tokens)\n",
    "print(tagged_sentence)\n",
    "```\n",
    "\n",
    "**OUTPUT**\n",
    "```python\n",
    "[('I', 'PRP'), ('love', 'VBP'), ('Data', 'NNS')]\n",
    "```\n",
    "\n",
    "pos_tags=[]\n",
    "for t in text:\n",
    "    token = nltk.word_tokenize(t)\n",
    "    post = nltk.pos_tag(token)\n",
    "    pos_tags.append(post)\n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "\n",
    "- `'text'` containing different sentences is already given to you\n",
    "\n",
    "- Create an empty list called `'pos_tags'`\n",
    "\n",
    "- Run a loop for each sentence `t` in `text`. Inside the loop:\n",
    "      \n",
    "      - Implement tokenization on 't' using \"word_tokenize()\" method of nltk and save the result in a varible called 'token'\n",
    "      \n",
    "      - Tag pos to 'token' using \"pos_tag()\" method of nltk and save the result in a variable called 'post'\n",
    "      \n",
    "      - Append 'post' to the 'pos_tags' list\n",
    "      \n",
    "      \n",
    "- Print `pos_tags` to see the POS tagged words on the sentences\n",
    "\n",
    "\n",
    "\n",
    "### Dive Deeper(Optional)\n",
    "\n",
    "Another popular model to implement POS tagging is using Spacy library.\n",
    "\n",
    "An interesting fact is the default pos_tag that NLTK uses was adopted from spacy. \n",
    "\n",
    "Matthew Honnibal, author of spacy, wrote a perceptron tagger, which was way faster than NLTK's then tagger. NLTK then decided to adopt it as its own.\n",
    "\n",
    "Following is the code snippet for the Spacy implementation:\n",
    "\n",
    "\n",
    "**Input**\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('Time flies like an arrow')\n",
    "\n",
    "pos_tags={}\n",
    "for token in doc:\n",
    "    pos_tags[token.text]=token.pos_\n",
    "\n",
    "print(pos_tags)\n",
    "```\n",
    "\n",
    "**Output**\n",
    "```python\n",
    "{'Time': 'PROPN', 'flies': 'VERB', 'like': 'ADP', 'an': 'DET', 'arrow': 'NOUN'}\n",
    "```\n",
    "\n",
    "Spacy also comes with visualization tools. You can read more about them [here](https://spacy.io/usage/linguistic-features#pos-tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hints\n",
    "\n",
    "You can implement POS tagging by writing code similar to:\n",
    "\n",
    "```python\n",
    "post = nltk.pos_tag(token)\n",
    "```\n",
    "\n",
    "\n",
    "# Test Cases\n",
    "\n",
    "#pos_tags\n",
    "\n",
    "Variable declaration\n",
    "pos_tags[0]==[('Time', 'NNP'), ('flies', 'NNS'), ('like', 'IN'), ('an', 'DT'), ('arrow', 'NN')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Time', 'NNP'), ('flies', 'NNS'), ('like', 'IN'), ('an', 'DT'), ('arrow', 'NN')], [('small', 'JJ'), ('boys', 'NNS'), ('and', 'CC'), ('girls', 'NNS')]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text=['Time flies like an arrow','small boys and girls']\n",
    "\n",
    "pos_tags=[]\n",
    "for t in text:\n",
    "    token = nltk.word_tokenize(t)\n",
    "    post = nltk.pos_tag(token)\n",
    "    pos_tags.append(post)\n",
    "\n",
    "    \n",
    "print(pos_tags)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Syntatic Parsing\n",
    "\n",
    "In this chapter you will learn in detail about syntatic parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.1 CFG\n",
    "\n",
    "Let's once again look at the parse tree we had seen previously\n",
    "\n",
    "![](../images/parsetree_1.jpg)\n",
    "\n",
    "Hopefully the parsed tree is making a little more sense with respect to POS tagging.\n",
    "\n",
    "Still, we can clearly see that HMM POS tagging is not enough. Consider the following sentence:\n",
    "\n",
    "\"The man attacked the thief in the bedroom with the baseball bat at midnight\"\n",
    "\n",
    "Ambiguous questions:\n",
    "\n",
    "- Does the man have the baseball bat?\n",
    "\n",
    "- Was the baseball bat just lying in the bedroom?\n",
    "\n",
    "We can see that HMM will fail in these kind of sentences because it can't make long range decisions about attachments(i.e. Sentences don't neccesarily follow Markov property)\n",
    "\n",
    "\n",
    "***Constituency***\n",
    "\n",
    "One underlying property of languages is the idea of constituency — groups of words behaving as a single units, or constituents.\n",
    "\n",
    "How do words group together in English? \n",
    "\n",
    "Consider the following examples of the noun phrase(a sequence of words surrounding at least one noun):\n",
    "\n",
    "- A high-paying job profile such as Adam’s \n",
    "- The Chicago Cubs batsman \n",
    "- Four music bands from Japan \n",
    "\n",
    "\n",
    "How can we verify though that these words group together? \n",
    "\n",
    "One check could be that they all precede a verb.\n",
    "\n",
    "- A high-paying job profile such as Adam’s `results`\n",
    "- The Chicago Cubs batsman `wins`\n",
    "- Four music bands from Japan `arrived`\n",
    "\n",
    "\n",
    "While a noun phrase can occur before a verb, interestingly enough, it's not true for the individual words that constitute a noun phrase. \n",
    "\n",
    "For e.g:\n",
    "\n",
    "-  as `results`\n",
    "\n",
    "- from `arrived`\n",
    "\n",
    "One another example is our use of prepositional phrases:\n",
    "\n",
    "- `On August 15th`, the British left the country and India got its freedom.\n",
    "\n",
    "- The British left the country `on August 15th` and India got its freedom.\n",
    "\n",
    "- The British left the country and India got its freedom `on August 15th`. \n",
    "\n",
    "\n",
    "We can see that `On August 15th` is placed in different locations in the above examples and the sentences still make sense.\n",
    "\n",
    "We can once again see that the individual words in the phrase cannot be placed like that\n",
    "\n",
    "- `On August`, the British left the country `15th` and India got its freedom.\n",
    "\n",
    "- `On` the British left the country `August 15th` and India got its freedom.\n",
    "\n",
    "- The British left the country `on August` and India got its freedom `15th`.\n",
    "\n",
    "\n",
    "***Context Free Grammars***\n",
    " \n",
    "One of of the most used formal system for modeling the constituents in English is the Context-Free Grammar(or CFG). CFG specifies a set of tree structures that capture constituency and ordering in language.\n",
    "\n",
    "For eg. Consider the following productions(rules) of CFG:\n",
    "\n",
    "\n",
    "NP → ProperNoun\n",
    "\n",
    "NP → Det Nominal \n",
    "\n",
    "Nominal → Noun | Nominal Noun \n",
    "\n",
    "NP (or noun phrase) can be composed of either a Proper Noun or A determiner (Det) followed by a Nominal; a Nominal in turn can consist of one or more Nouns. \n",
    "\n",
    "\n",
    "Confusing?\n",
    "Let's try to understand using a simple example.\n",
    "\n",
    "Sentence is: 'The library'\n",
    "\n",
    "How can we derive that using CFG? Let's see.\n",
    "\n",
    "So we can start with symbol ::  NP \n",
    "\n",
    "Using the second rule, we can rewrite NP as :: Det Nominal \n",
    "\n",
    "Using the third rule, we can rewrite Nominal as :: Det Noun \n",
    "\n",
    "Lastly replace these POS tags with actual words : The library \n",
    "\n",
    "\n",
    "We can present this in the form of the following tree:\n",
    "\n",
    "\n",
    "![](../images/parsetree_2.jpg)\n",
    "\n",
    "\n",
    "\n",
    "The symbols used in CFG are divided into two classes. \n",
    "\n",
    "**Terminal Symbols:** In terms of natural language modeling, these symbols are those correspond to actual words in the language (\"the\",\"a\" ,\"library\") \n",
    "\n",
    "**Non Terminal Symbols:** In terms of natural language modeling, these symbols are those that express abstractions over these terminals(\"NP\",\"Nominal\") \n",
    "\n",
    "We saw how \"The library\"(both terminal) was derived from the non-terminal NP.\n",
    "\n",
    "Thus,a CFG can be used to generate a set of strings( It's another use is for assigning a structure to a given sentence and will be discussed later)\n",
    "\n",
    "**Start Symbol:** Each grammar must have one designated start symbol, which is often called S.\n",
    "\n",
    "For eg: S → NP VP \n",
    "\n",
    "\n",
    "***\n",
    "**Self exercise:**\n",
    "\n",
    "Given the following rules:\n",
    "\n",
    "\n",
    "\n",
    "S $\\rightarrow$ NP VP \n",
    "\n",
    "VP $\\rightarrow$ Verb NP\n",
    "\n",
    "NP $\\rightarrow$ Proper Noun\n",
    "\n",
    "NP $\\rightarrow$ Det Nominal \n",
    "\n",
    "Nominal $\\rightarrow$ Noun | Nominal Noun \n",
    "\n",
    "Try to derive `Adam prefers a morning coffee`\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "CFG helps us to define something called a formal language(i.e. a set of strings). Sentences that can be derived by a formal grammar are grammatical and sentences that cannot be derived are refered to as ungrammatical. \n",
    "\n",
    "***\n",
    "**Dive Deeper(Optional):**\n",
    "\n",
    "Natural languages exist which are difficult to be expressed formally in a machine. Noam Chomsky therefore categorised formal languages(essentially grammar rules) as follows:\n",
    "\n",
    "![](../images/Chomsky.jpg)\n",
    "\n",
    "The four types of grammar only differ in the type of rewriting rule $\\alpha \\rightarrow \\beta$ that is allowed.\n",
    "\n",
    "Since the restrictions which define the grammar types apply to the rules, let's talk about the four different types of grammar\n",
    "\n",
    "\n",
    "**Regular Grammar:**\n",
    "\n",
    "All rules take one of the two following forms:\n",
    "\n",
    "$N \\rightarrow t$\n",
    " \n",
    "$N \\rightarrow tM$, \n",
    "\n",
    "where N and M are non-terminals,\n",
    "\n",
    "t is a terminal.\n",
    "\n",
    "Regular grammar rules owing to their simplicity are not powerful enough to describe natural languages \n",
    "\n",
    "They are mostly used to describe portions of languages and have the advantage of fast parsing.\n",
    "\n",
    "**Context-Free Grammar:**\n",
    "\n",
    "All rules are of of the form \n",
    "\n",
    "$N \\rightarrow \\alpha$,\n",
    "\n",
    "where N is a nonterminal symbol,\n",
    "\n",
    "$\\alpha$ is a non null string(combination of terminal and non terminal symbols).\n",
    "\n",
    "This is the most popular grammar used for Parsing purposes in NLP. We will talk more about it later\n",
    "\n",
    "\n",
    "**Context Sensitive Grammar:**\n",
    "\n",
    "It's restriction is length($\\alpha$) ≤ length($\\beta$)\n",
    "\n",
    "Equivalently, the rules are of the form :\n",
    "\n",
    "$\\lambda N \\rho \\rightarrow \\lambda \\alpha \\rho$, \n",
    "\n",
    "where N is a nonterminal symbol,\n",
    "\n",
    "$\\alpha$ is a non null string(combination of terminal and non terminal symbols),\n",
    "\n",
    "$\\lambda$ and $\\phi$ are any strings.\n",
    "\n",
    "$\\lambda$ and $\\phi$ are considered the left and right context in which the non-terminal symbol N can be rewritten as the non-null symbol-string $\\alpha$. That's the reason they are known as context-sensitive grammar.\n",
    "\n",
    "Converting active sentence to passive sentences is one of the major uses of Context sensitive grammar rules\n",
    "\n",
    "\n",
    "\n",
    "**Recursively enumerable languages(Unrestricted grammar):**\n",
    "\n",
    "This grammar has no restrictions on the form that the Rules $\\alpha \\rightarrow \\beta$ can take.\n",
    "\n",
    "Unrestricted grammars are not widely used as their extreme power makes them difficult to parse with.\n",
    "\n",
    "\n",
    "It should be stated that these formal languages defined using these rules are but a simpliﬁed model of how natural languages really work. \n",
    "\n",
    "\n",
    "You can read more about them [here](http://www.cse.unsw.edu.au/~billw/cs9414/notes/nlp/grampars/grampars.html)\n",
    "***\n",
    "\n",
    "\n",
    "***Formal Definition of CFG***\n",
    "\n",
    "\n",
    "A context-free grammar G(in NLP) is deﬁned by four parameters: `N`,`Σ`, `R`, `S` \n",
    "\n",
    "\n",
    "Σ is the set of terminal symbols (i.e. words)\n",
    "\n",
    "N is the set of non-terminal symbols\n",
    "\n",
    "R is the set of rules/productions of the form X $\\rightarrow$ y where\n",
    "          \n",
    "          X is a nonterminal, \n",
    "          \n",
    "          y is a sequence of terminals and nonterminals (Σ∪N)∗\n",
    "\n",
    "S is the start symbol (and a member of N)\n",
    "\n",
    "\n",
    "Look at the following extensions of the same sentence:\n",
    "\n",
    "![](../images/p_1.jpg)\n",
    "\n",
    "\n",
    "![](../images/p_2.jpg)\n",
    "\n",
    "\n",
    "![](../images/p_3.jpg)\n",
    "\n",
    "\n",
    "![](../images/p_4.jpg)\n",
    "\n",
    "\n",
    "You can see that the more information you add, the complicated the rules get\n",
    "\n",
    "CFG has also lot of other rules which we will not cover here but just to give you an idea, this is how the sentence tree of  \"That cold, empty sky was full of fire and light\" looks like:\n",
    "\n",
    "![](../images/parsetree_3.jpg)\n",
    "\n",
    "\n",
    "This problem of mapping from a string of words to its parse tree is called `syntactic parsing`. Before we understand what it is, let's look at one another type of grammar known as Dependency Grammar.\n",
    "\n",
    "\n",
    "**Dependency Grammar**\n",
    "\n",
    "In dependency grammar, the structure of a sentence is described in terms of the words in the sentence and the associated set of grammatical relations that exist among the words. \n",
    "\n",
    "\n",
    "Following is an example of sentence represented via dependency grammar tree:\n",
    "\n",
    "![](../images/dp.jpg)\n",
    "\n",
    "Relations among the words are shown with directed, labeled arrows/arcs from heads to dependents.\n",
    "It also includes a root node that explicitly marks the root of the tree, the head of the entire structure. \n",
    "\n",
    "One of the big advantages of dependency grammars is their ability to deal with languages that have a relatively free word order(For eg. Language like Czech). That's because in dependency grammar parsing, word-order information is handled in an abstract way.\n",
    "\n",
    "\n",
    "Let's now understand what syntatic parsing is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Syntatic Parsing\n",
    "\n",
    "\n",
    "Context-free grammars don’t specify how for a given sentence, its parse tree should be computed.\n",
    "Syntactic parsing is the exact task of recognizing a sentence and assigning a syntactic structure to it.\n",
    "\n",
    "While parsing what we are effectively doing is `search` for a syntatic structure(POS tag structure) that fits the given sentence.\n",
    "\n",
    "There are two methods of Parsing tree search:\n",
    "\n",
    "- Top-Down Parsing (goal-directed search)\n",
    "\n",
    "- Bottom-Up Parsing (data-directed search)\n",
    "\n",
    "Let's look at them one by one.\n",
    "\n",
    "**Note:** While exploring the two methods, following are the grammar rules that we are following:\n",
    "\n",
    "![](../images/grammar.PNG)\n",
    "\n",
    "**Top-Down Parsing**\n",
    "\n",
    "A top-down parser searches for a parse tree by trying to build from the root node S down to the leaves. \n",
    "\n",
    "For sentence \"Book that train\",\n",
    "\n",
    "![](../images/phrase.jpg)\n",
    "\n",
    "\n",
    "following will be the expanding top-down search space.\n",
    "\n",
    "![](../images/top_down.jpg)\n",
    "\n",
    "**Bottom-Up Parsing**\n",
    "\n",
    "In bottom-up parsing, the parser starts with the words of the input sentence and builds trees till the final step before `S`.\n",
    "\n",
    "![](../images/bottom_up.jpg)\n",
    "\n",
    "Both the architectures come up with their own flaws. \n",
    "\n",
    "- `Top-Down` will never explore subtree that cannot find a place in some S-rooted tree(i.e. it will spend considerable effort on S trees that are not consistent with the input) \n",
    "\n",
    "- In `Bottom-Up` parsing, trees which will never become S-rooted tree are generated too much(and then abandoned).\n",
    "\n",
    "\n",
    "Dive Deeper(Optional):\n",
    "\n",
    "We have made a simplifying assumption that during both top-down and bottom up parsing, we could explore all the possible parse trees in parallel. Though possible, it involves a large amount of memory to store all the constructed parse trees(Instead of one sentence, consider paragraphs which will inherently contain much more ambiguity to appreciate the possible parse trees possible)\n",
    "\n",
    "An intuitive approach would be to use a backtracking strategy. It states that when a given tree results in an inconsistent grammar, the search continues by returning to an unexplored option.\n",
    "\n",
    "Unfortunately owing to ambiguity in sentences, this backtracking approach is very inefficient.\n",
    "\n",
    "Consider the following tree for the sentence 'a package from Paris to Belgium on EuroAir' :\n",
    "\n",
    "![](../images/prob.jpg)\n",
    "\n",
    "\n",
    "Just like in the above sentence, backtracking parsers will build valid trees for certain portions of the sentence, discard them during backtracking and then rebuild again when new input makes sense with the old tree.\n",
    "\n",
    "The efficient methods to resolve includes dynammic programming methods including:\n",
    "\n",
    "- CYK algorithm [Link](http://web.cs.ucdavis.edu/~rogaway/classes/120/winter12/CYK.pdf)\n",
    "\n",
    "- Earley algorithm [Link](http://cl.lingfil.uu.se/~sara/kurser/5LN455-2014/lectures/5LN455-F5.pdf)\n",
    "\n",
    "- Chart parsing [Link](http://www.inf.ed.ac.uk/teaching/courses/icl/lectures/2006/earley-lec.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed_sentence= (S\n",
      "  (NP Mary/NN)\n",
      "  saw/VBD\n",
      "  (CLAUSE\n",
      "    (NP the/DT cat/NN)\n",
      "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Natural Language Toolkit: code_cascaded_chunker\n",
    "http://www.nltk.org/book/ch07.html#code-cascaded-chunker\n",
    "\"\"\"\n",
    "grammar = r\"\"\"\n",
    "NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
    "PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
    "VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n",
    "CLAUSE: {<NP><VP>}           # Chunk NP, VP\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentence = [(\"Mary\", \"NN\"), (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"),  (\"sit\", \"VB\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
    "parsed_sentence = cp.parse(sentence)\n",
    "print('parsed_sentence=', parsed_sentence) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CFG parsing\n",
    "\n",
    "Let's try to implement CFG parsing.\n",
    "\n",
    "\n",
    "\n",
    "- Grammar for finding the parse tree is already given(`'adhoc_grammar'`)\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "- Sentences to find the parse trees is already given(`'sentence_1'`,`'sentence_2'`).\n",
    "\n",
    "- Function `\"parse()\"` to find parse trees is already defined. Inside the function, pass `'adhoc_grammar'` as the parameter inside `\"ChartParser()\"` method of nltk.  \n",
    "               \n",
    "- Call the `\"parse()\"` function for `'sentence_1'` and save the result of it in `'pt_1'`  \n",
    "\n",
    "- Call the `\"parse()\"` function for `'sentence_2'` and save the result of it in `'pt_2'` \n",
    "\n",
    "- Print `'pt_1'`, `'pt_2'` to see the trees formed.\n",
    "\n",
    "\n",
    "### Visualization instructions:\n",
    "\n",
    "- For better visualization, copy the following code to clean the tree in the desired format:\n",
    "\n",
    "```python\n",
    "pt1=pt1.replace('(','[')\n",
    "pt1=pt1.replace(')',']')\n",
    "\n",
    "\n",
    "pt2=pt2.replace('(','[')\n",
    "pt2=pt2.replace(')',']')\n",
    "```\n",
    "\n",
    "You can then copy the contents of the pt1(and pt2) and paste it in this [link](http://ironcreek.net/phpsyntaxtree/) to see the parse tree in tree format. \n",
    "\n",
    "### Dive Deeper(Optional)\n",
    "\n",
    "Though simple to implement and understand,NLTK is slower than some of the other recent implementations of parser.\n",
    "\n",
    "One of them is lark. It is fast and also supports Earley and CYK. \n",
    "\n",
    "Consider the ambigous sentence: \n",
    "'Fruit flies like bananas'\n",
    "\n",
    "Lark can the following two parsing trees:\n",
    "\n",
    "![](../images/lark.PNG)\n",
    "\n",
    "\n",
    "The `simple` one treats 'Fruit flies' as a noun and parses correctly\n",
    "\n",
    "The `comparative` one treats only 'Fruit' as a noun and compares the flying of Fruit to flying of Banana.\n",
    "\n",
    "\n",
    "You can read more about Lark [here](https://lark-parser.readthedocs.io/en/latest/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "You can pass `'adhoc_grammar'` as the parameter inside `\"ChartParser()\"` method of nltk by writing code similar to:  \n",
    "\n",
    "```python\n",
    "parser = nltk.ChartParser(adhoc_grammar)\n",
    "```\n",
    "## Test Cases\n",
    "\n",
    "#pt1\n",
    "variable declaration\n",
    "One more test case(Ask Nikhil)\n",
    "\n",
    "#pt2\n",
    "variable declaration\n",
    "One more test case(Ask Nikhil)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (Det the) (Nom (Adj angry) (Nom (N dog))))\n",
      "  (VP\n",
      "    (V chased)\n",
      "    (NP\n",
      "      (Det the)\n",
      "      (Nom (Adj frightened) (Nom (Adj little) (Nom (N cat)))))))\n",
      "(S\n",
      "  (NP (PropN Adam))\n",
      "  (VP (V rescued) (NP (Det the) (Nom (N squirrel)))))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Grammar defined for our task   \n",
    "\n",
    "adhoc_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "  S  -> NP VP\n",
    "  NP -> Det Nom | PropN\n",
    "  Nom -> Adj Nom | N\n",
    "  VP -> V Adj | V NP | V S | V NP PP\n",
    "  PP -> P NP\n",
    "  PropN -> 'Adam' | 'Nicole' | 'Sam'\n",
    "  Det -> 'the' | 'a'\n",
    "  N -> 'bear' | 'squirrel' | 'tree' | 'fish' | 'log'| 'cat'|'dog' \n",
    "  Adj  -> 'angry' | 'frightened' |  'little' | 'tall'\n",
    "  V ->  'chased'  | 'saw' | 'said' | 'thought' | 'was' | 'put'| 'rescued'\n",
    "  P -> 'on'\n",
    "  \"\"\")\n",
    "\n",
    "# Sentence 1\n",
    "sentence_1 = 'the angry dog chased the frightened little cat'.split()\n",
    "\n",
    "# Sentence\n",
    "sentence_2= 'Adam rescued the squirrel'.split()\n",
    "\n",
    "\n",
    "# Function for parsing\n",
    "\n",
    "def parse(sentence):\n",
    "    \n",
    "    # List for parsed tree\n",
    "    parsed_tree = []  \n",
    "    \n",
    "    # Parsing done by \n",
    "    parser = nltk.ChartParser(adhoc_grammar)\n",
    "    \n",
    "\n",
    "    # Loop for creating appending the trees\n",
    "    for tree in parser.parse(sentence):\n",
    "        parsed_tree.append(tree)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Return the parsed tree\n",
    "    return(str(parsed_tree[0])) \n",
    "\n",
    "\n",
    "# Calling the parse function for sentence 1   \n",
    "pt1=(parse(sentence_1))\n",
    "print(pt1)\n",
    "\n",
    "# Calling the parse function for sentence 1\n",
    "pt2=(parse(sentence_2))\n",
    "print(pt2)\n",
    "\n",
    "# Cleaning of the trees for better visualization purposes\n",
    "\n",
    "pt1=pt1.replace('(','[')\n",
    "pt1=pt1.replace(')',']')\n",
    "\n",
    "\n",
    "pt2=pt2.replace('(','[')\n",
    "pt2=pt2.replace(')',']')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "CFG parsing from NLTK can be a bit slow. Do you also want to show the same with lark library? It's way faster and supports earley, cyk as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Statistical Parsing\n",
    "\n",
    "In this chapter you will learn and understand one of most popular implementations of parsing called statistical parsing and how to evaluate the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 PCFG\n",
    "\n",
    "In the previous topic, we highlighted sophisticated models of syntatic structure and its parsing. \n",
    "\n",
    "\n",
    "There is one another parsing method that uses probabilistic knowledge to help us create effective parsers.\n",
    "\n",
    "A probabilistic parser is the most commonly used modern parsers used to resolve the problem of ambiguity(a problem we have been trying to solve since the beginning of this concept). In very simple terms it computes the probability of each intepretation of a ambiguous sentence and then chooses the most probable intepretation.  \n",
    "\n",
    "**Note:** Owing to this ambiguity present across the problems we want to solve in NLP, most of the NLP applications including summarization, machine translation and question-answering the parsers used are probabilistic. \n",
    "\n",
    "The most commonly used probablistic grammar is the probablistic context-free grammar(PCFG), a variation of CGF in which each rule has a probability attached to it.\n",
    "\n",
    "**PCFG**\n",
    "\n",
    "Recall, a context-free grammar G is deﬁned by four parameters: `N`,`Σ`, `R`, `S` \n",
    "\n",
    "***\n",
    "Σ is the set of terminal symbols (i.e. words)\n",
    "\n",
    "N is the set of non-terminal symbols\n",
    "\n",
    "R is the set of rules/productions of the form X $\\rightarrow$ y where\n",
    "          \n",
    "          X is a nonterminal, \n",
    "          \n",
    "          y is a sequence of terminals and nonterminals (Σ∪N)∗\n",
    "\n",
    "S is the start symbol (and a member of N)\n",
    "***\n",
    "\n",
    "PCFG is also defined by the same four parameters(`N`,`Σ`, `R`, `S`) but with an update to `R`:\n",
    "\n",
    "***\n",
    "Σ is the set of terminal symbols (i.e. words)\n",
    "\n",
    "N is the set of non-terminal symbols\n",
    "\n",
    "R is the set of rules/productions of the form X $\\rightarrow$ y[p] where\n",
    "          \n",
    "          X is a nonterminal, \n",
    "          \n",
    "          y is a sequence of terminals and nonterminals (Σ∪N)∗\n",
    "          \n",
    "          p is a number between 0 and 1 expressing P(y|X)\n",
    "\n",
    "S is the start symbol (and a member of N)\n",
    "***\n",
    "\n",
    "\n",
    "In other words, with each X $\\rightarrow$y, we have a probability that the given non-terminal X will be expanded to sequence y associated with it(P(y|X)).\n",
    "\n",
    "Following is an example of how PCFG representation looks like:\n",
    "\n",
    "![](../images/pcfg.jpg)\n",
    "\n",
    "Notice that the probabilities of all the expansions of non-terminals have a final sum of 1.\n",
    "\n",
    "\n",
    "As mentioned before PCFG can be useful in the following applications:\n",
    "\n",
    "- Ambiguity removal (Compare the probability of parse trees of a single sentence)\n",
    "\n",
    "- Language modeling (Calculate the probability of the sentence)\n",
    "\n",
    "\n",
    "Let's try to implement PCFG using an example:\n",
    "\n",
    "Consider the sentence `'Eat Sushi with Tuna'`. Following are its possible parse trees:\n",
    "\n",
    "![](../images/pcfg_2.jpg)\n",
    "\n",
    "\n",
    "The one on the left represents the meaning that we want- Eat Sushi and Tuna together i.e Both Sushi and Tuna are food to be consumed together\n",
    "\n",
    "The one on the right represents the meaning- Use Tuna to eat Sushi i.e. Tuna is considered to be a fork or a chopstick using which we should eat Sushi\n",
    "\n",
    "\n",
    "The prob. of each tree can be simply calculated by multiplying the corresponding probabilities of rules used in the tree.\n",
    "\n",
    "---\n",
    "\n",
    "$P(T_{left})$= \n",
    "\n",
    "$P(VP \\rightarrow V|NP)$ . $P(V \\rightarrow eat)$ . $P(NP \\rightarrow NP|PP)$ . $P(NP \\rightarrow sushi)$ . $P(PP \\rightarrow P|NP)$ . $P(P \\rightarrow with)$ . $P(NP \\rightarrow tuna)$\n",
    "\n",
    "---\n",
    "\n",
    "$P(T_{right})$= \n",
    "\n",
    "$P(VP \\rightarrow VP|PP)$ . $P(VP \\rightarrow V|NP)$ . $P(V \\rightarrow eat)$ . $P(NP \\rightarrow sushi)$ . $P(PP \\rightarrow P|NP)$ . $P(P \\rightarrow with)$ . $P(NP \\rightarrow tuna)$\n",
    "\n",
    "---\n",
    "\n",
    "The parse with the higher PCFG probability will be chosen to disambiguate the sentence.\n",
    "\n",
    "\n",
    "**Python Implementation:**\n",
    "\n",
    "Following is a sample code snippet to implement PCFG using NLTK:\n",
    "***\n",
    "**Input**\n",
    "\n",
    "```python\n",
    "# Header Files\n",
    "\n",
    "from nltk import PCFG\n",
    "from nltk.probability import DictionaryProbDist\n",
    "from nltk import nonterminals, Nonterminal, Production\n",
    "from nltk.corpus import treebank\n",
    "from nltk import treetransforms\n",
    "from nltk import induce_pcfg\n",
    "from nltk.parse import pchart\n",
    "\n",
    "# PCFG Grammar\n",
    "\n",
    "toy_pcfg1 = PCFG.fromstring(\"\"\"\n",
    "    S -> NP VP [1.0]\n",
    "    NP -> Det N [0.5] | NP PP [0.25] | 'John' [0.1] | 'I' [0.15]\n",
    "    Det -> 'the' [0.8] | 'my' [0.2]\n",
    "    N -> 'man' [0.5] | 'telescope' [0.5]\n",
    "    VP -> VP PP [0.1] | V NP [0.7] | V [0.2]\n",
    "    V -> 'ate' [0.35] | 'saw' [0.65]\n",
    "    PP -> P NP [1.0]\n",
    "    P -> 'with' [0.61] | 'under' [0.39]\n",
    "\"\"\")\n",
    "\n",
    "# Saving all the rules of the grammar in a variable\n",
    "pcfg_prods = toy_pcfg1.productions()\n",
    "\n",
    "\n",
    "# Selecting one probability grammar rule\n",
    "pcfg_prod = pcfg_prods[10]\n",
    "\n",
    "\n",
    "print('A PCFG production:', pcfg_prod)\n",
    "print('pcfg_prod.lhs()  =>', pcfg_prod.lhs())\n",
    "print('pcfg_prod.rhs()  =>', pcfg_prod.rhs())\n",
    "print('pcfg_prod.prob() =>', pcfg_prod.prob())\n",
    "\n",
    "# Taking all productions where LHS=N\n",
    "n_productions = toy_pcfg1.productions(Nonterminal('N'))\n",
    "\n",
    "dict = {}\n",
    "for pr in n_productions: dict[pr.rhs()] = pr.prob()\n",
    "n_probDist = DictionaryProbDist(dict)\n",
    "\n",
    "# Generates random samples depending on the prob.\n",
    "\n",
    "print(n_probDist.generate())\n",
    "\n",
    "print(n_probDist.generate())\n",
    "\n",
    "print(n_probDist.generate())\n",
    "```\n",
    "**Output**\n",
    "\n",
    "```python\n",
    "\n",
    "A PCFG production: VP -> V NP [0.7]\n",
    "pcfg_prod.lhs()  => VP\n",
    "pcfg_prod.rhs()  => (V, NP)\n",
    "pcfg_prod.prob() => 0.7\n",
    "('telescope',)\n",
    "('telescope',)\n",
    "('telescope',)\n",
    "\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "Unfortunately, PCFGs also come up with their own problems:\n",
    "\n",
    "***Lexical Conditioning:***\n",
    "\n",
    "There are cases when owing to similar tree structures, two intepretations can have same probabilities\n",
    "\n",
    "For e.g. \n",
    "\n",
    "Consider the following two valid parse trees for the sentence 'workers dumped sacks into a bin'\n",
    "\n",
    "![](../images/parsing_prob1.jpg)\n",
    "\n",
    "\n",
    "You can see that if you multiply the probabilities of both trees, you will get the same result.\n",
    " \n",
    "This is because PCFG can't take into consideration the lexical dependency.\n",
    "\n",
    "\n",
    "***Poor Assumptions:***\n",
    "\n",
    "PCFG considers independence assumptions of probabilities which often results in poor modeling. \n",
    "\n",
    "For e.g. \n",
    "\n",
    "Consider the dependence of the sentence 'The dog barked at the cat' \n",
    "\n",
    "![](../images/dp.jpg)\n",
    "\n",
    "The verb associated with `at` is `barked` which is in turn related to the noun `cat`.\n",
    "\n",
    "This dependence problem becomes much more difficult to handle by PCFG when we are dealing with the sentence like the following:\n",
    "\n",
    "![](../images/dp_00.jpg)\n",
    "\n",
    "\n",
    "In other words, context of the sentence(which is one of the main factors for ambiguity) is never taken into consideration by PCFG.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Evaluating Parsers\n",
    "    \n",
    "Parsers are evaluated by using something called PARSEVAL measures.\n",
    "\n",
    "PARSEVAL tries to measure how the constituents of the created(hypothesis) parse tree compare with the constituents of the actual(reference) parse tree.\n",
    "\n",
    "**NOTE**: The reference parse tree usually used are gold-standard parses from [Penn Treebank](https://github.com/tomsercu/lstm/tree/master/data)\n",
    "\n",
    "\n",
    "The method used by PARSEVAL are standard methods of Recall and Precision:\n",
    "\n",
    "***Labeled recall (LR):*** $\\frac{\\text{No. of correct constituents in hyp. parse}}{\\text{No. of constituents in reference parse}}$ \n",
    "\n",
    "***Labeled precision (LP):*** $\\frac{\\text{No. of correct constituents in hyp. parse}}{\\text{No. of constituents in hyp. parse}}$ \n",
    "\n",
    "There's also F-measure which combines both precision and recall,\n",
    "\n",
    "$F_{\\beta}= \\frac{(\\beta^2 + 1)LP.LR}{\\beta^2(LP.LR)}$\n",
    " \n",
    "\n",
    "There is also another metric that we use in PARSEVAL which is known as `cross-brackets`\n",
    "\n",
    "\n",
    "***Cross-brackets:*** The number of constituents in which the hypothesis parse has a bracketing as ((A B) C) wherease the reference parse has a bracketing as (A (B C)).  \n",
    "\n",
    "\n",
    "One thing to note is we are not evaluating parsers by calculating how many sentences are parsed correctly. This is done keeping in mind that for long sentences, rarely do most parsers get a perfect parse. Measuring just the sentence accuracy would therefore result in a weak metric as we won't be able to distinguish between a parse that got only one part wrong and parse that got everything wrong\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Before we wrap up our discussion on Parsing, there's another type of parsing that is quite prominent known as Dependency Parsing.\n",
    "\n",
    "**Dependency Parsing**\n",
    "\n",
    "This type of parsing is based on the dependency grammar rules i.e. structure of a sentence is described in terms of the words in the sentence and the associated set of grammatical relations that exist among the words. \n",
    "\n",
    "\n",
    "Following is an example of sentence represented via dependency parsing(Remember we talked about it when we were discussing the shortcomings of PCFG parsing):\n",
    "\n",
    "![](../images/dp.jpg)\n",
    "\n",
    "Relations among the words are shown with directed, labeled arrows/arcs from heads to dependents.\n",
    "It also includes a root node that explicitly marks the root of the tree, the head of the entire structure. \n",
    "\n",
    "One of the big advantages of dependency grammars is their ability to deal with languages that have a relatively free word order(For eg. Language like Czech). That's because in dependency grammar parsing, word-order information is handled in an abstract way.\n",
    "\n",
    "Following is a side by side representation of the same sentence as a constituent parse tree and dependency parse tree\n",
    "\n",
    "\n",
    "![](../images/dp_2.jpg)\n",
    "\n",
    "\n",
    "One of the major applications of Dependency parsing is text classification.\n",
    "\n",
    "Say, you are given a sentence 'Sam is not happy' and assume that the token \"happy\" has a positive sentiment in a specific domain.\n",
    "\n",
    "![](../images/spacy_1.PNG)\n",
    "\n",
    "\n",
    "![](../images/spacy_2.PNG)\n",
    "\n",
    "\n",
    "Its only if you know the dependency `neg(not, is)` that we can classify the above example to have a negative sentiment. Without knowing this dependency we would probably classify the sentence as positive.\n",
    "\n",
    "Dependency parsing offers a better understanding of the text and helps to significantly increase the accuracy of classification tasks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You can read more about dependency parsing [here](https://web.stanford.edu/~jurafsky/slp3/13.pdf)\n",
    "\n",
    "You can also see the dependency parsed tree of any sentence visualized by spacy [here](https://explosion.ai/demos/displacy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
