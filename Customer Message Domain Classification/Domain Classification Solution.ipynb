{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we will load the dataset and perform a basic cleaning in order to simplify our futher steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            message   category\n",
      "MID                                                           \n",
      "0                                      7am everyday  reminders\n",
      "1                                    chocolate cake       food\n",
      "2    closed mortice and tenon joint door dimentions    support\n",
      "3                               train eppo kelambum     travel\n",
      "4      yesterday i have cancelled the flight ticket     travel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# path_train : location of test file\n",
    "# Code starts here\n",
    "\n",
    "#Loading data\n",
    "df = pd.read_csv(\"train_data.csv\",index_col=\"MID\")\n",
    "print(df.head())\n",
    "\n",
    "#Code ends here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the Text Analytics concepts we need to convert this textual data into vectors so that we can apply machine learning algorithms to them. In this task we will now employ a normal TF-IDF vectorizer to vectorize the message column and label encode the category column, essentially making it a classification problem. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sampling only 1000 samples of each category\n",
    "df = df.groupby('category').apply(lambda x: x.sample(n=1000, random_state=0))\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "# Converting all messages to lower case and storing it\n",
    "all_text = df[\"message\"].str.lower()\n",
    "\n",
    "# Initialising TF-IDF object\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Vectorizing data\n",
    "tfidf.fit(all_text)\n",
    "\n",
    "# Storing the TF-IDF vectorized data into an array\n",
    "X = tfidf.transform(all_text).toarray()\n",
    "\n",
    "# Initiating a label encoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fitting the label encoder object on the data\n",
    "le.fit(df[\"category\"])\n",
    "\n",
    "# Transforming the data and storing it\n",
    "y = le.transform(df[\"category\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous tasks we have cleaned the data and converted the textual data into numbers in order to enable us to apply machine learning models. In this task we will apply Logistic Regression , Naive Bayes and Lienar SVM model onto the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7121925559391143 is the f1 score of the logistic regression model\n",
      "0.7098202248522268 is the f1 score of the Naive Bayes model\n",
      "0.7167828373922656 is the f1 of the LinearSVC model\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_val,y_train, y_val = train_test_split(X,y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "# Implementing Logistic Regression model\n",
    "log_reg = LogisticRegression(random_state=0)\n",
    "log_reg.fit(X_train,y_train)\n",
    "y_pred = log_reg.predict(X_val)\n",
    "log_f1 = f1_score(y_val,y_pred, average='macro')\n",
    "print (str(log_f1)+(\" is the f1 score of the logistic regression model\"))\n",
    "\n",
    "# Implementing Multinomial NB model\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train,y_train)\n",
    "y_pred = nb.predict(X_val)\n",
    "nb_f1 = f1_score(y_val,y_pred,average='macro')\n",
    "print (str(nb_f1)+(\" is the f1 score of the Naive Bayes model\"))\n",
    "\n",
    "\n",
    "# Implementing Linear SVM model\n",
    "lsvm = LinearSVC(random_state=0)\n",
    "lsvm.fit(X_train, y_train)\n",
    "y_pred = lsvm.predict(X_val)\n",
    "lsvm_f1 = f1_score(y_val,y_pred,average='macro')\n",
    "print (str(lsvm_f1)+(\" is the f1 of the LinearSVC model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best score is given by LinearSVM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see how well our models run on test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# path_test : Location of test data\n",
    "\n",
    "#Loading the dataframe\n",
    "df_test = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "all_text = df_test[\"message\"].str.lower()\n",
    "\n",
    "# Transforming using the tfidf object - tfidf\n",
    "X_test = tfidf.transform(all_text).toarray()\n",
    "\n",
    "\n",
    "\n",
    "# Predicting using the linear svm model - lsvm\n",
    "\n",
    "submission=pd.DataFrame(lsvm.predict(X_test))\n",
    "\n",
    "# Create the submission file\n",
    "submission.to_csv('submission_domain.csv',index=False,header=['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS ACTIVITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSI Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to attempt topic modelling on our dataset.\n",
    "Let's use LSI for the same.\n",
    "\n",
    "\n",
    "### LSA Modeling\n",
    "\n",
    "In this task, we will try to see how to use LSI on the entire dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.347*\"reminder\" + 0.267*\"like\" + 0.267*\"cancel\" + 0.266*\"would\" + '\n",
      "  '0.256*\"userid\" + 0.256*\"apiname\" + 0.256*\"exotel\" + 0.256*\"offset\" + '\n",
      "  '0.255*\"taskname\" + 0.255*\"reminderlist\"'),\n",
      " (1,\n",
      "  '-0.831*\"want\" + -0.221*\"u\" + -0.187*\"know\" + -0.181*\"movie\" + -0.135*\"book\" '\n",
      "  '+ -0.128*\"ticket\" + -0.114*\"need\" + -0.108*\"hi\" + -0.096*\"please\" + '\n",
      "  '-0.092*\"service\"'),\n",
      " (2,\n",
      "  '0.451*\"reminder\" + -0.328*\"call\" + -0.316*\"u\" + -0.233*\"wake\" + '\n",
      "  '0.205*\"water\" + -0.197*\"march\" + -0.192*\"wakeup\" + 0.185*\"every\" + '\n",
      "  '0.181*\"drink\" + 0.168*\"want\"'),\n",
      " (3,\n",
      "  '0.611*\"u\" + -0.418*\"want\" + 0.244*\"need\" + 0.238*\"reminder\" + '\n",
      "  '0.197*\"please\" + 0.143*\"movie\" + 0.117*\"service\" + -0.102*\"wake\" + '\n",
      "  '0.101*\"near\" + 0.101*\"help\"'),\n",
      " (4,\n",
      "  '0.622*\"need\" + -0.510*\"u\" + 0.491*\"movie\" + 0.189*\"offer\" + -0.137*\"want\" + '\n",
      "  '0.115*\"ticket\" + 0.058*\"know\" + -0.052*\"find\" + 0.051*\"today\" + '\n",
      "  '0.049*\"book\"')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import gensim\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Creating a stopwords list\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize and remove the stopwords\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "# Creating a list of documents from the complaints column\n",
    "list_of_docs = df[\"message\"].tolist()\n",
    "\n",
    "# Implementing the function for all the complaints of list_of_docs\n",
    "doc_clean = [clean(doc).split() for doc in list_of_docs]\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "# Creating the dictionary id2word from our cleaned word list doc_clean\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Creating the corpus\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "\n",
    "# Creating the LSi model\n",
    "lsimodel = LsiModel(corpus=doc_term_matrix, num_topics=5, id2word=dictionary)\n",
    "pprint(lsimodel.print_topics())\n",
    "\n",
    "# Code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's try to do topic modeling using LDA. We will first find the optimum no. of topics using coherence score and then create a model attaining to the optimum no. of topics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3287476298674388, 0.4801812391625579, 0.5306698259321219, 0.5376618801954907, 0.5587078765648961, 0.572049572781549, 0.5663902769474314, 0.5892433863373899]\n",
      "Optimum no. of topics: 36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.167*\"2\" + 0.101*\"mobile\" + 0.053*\"discount\" + 0.035*\"already\" + 0.027*\"information\" + 0.018*\"table\" + 0.016*\"want\" + 0.014*\"cheap\" + 0.014*\"black\" + 0.013*\"kid\"'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# doc_term_matrix - Word matrix created in the last task\n",
    "# dictionary - Dictionary created in the last task\n",
    "\n",
    "# Function to calculate coherence values\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    topic_list : No. of topics chosen\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    topic_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(doc_term_matrix, random_state = 0, num_topics=num_topics, id2word = dictionary, iterations=10)\n",
    "        topic_list.append(num_topics)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return topic_list, coherence_values\n",
    "\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "# Calling the function\n",
    "topic_list, coherence_value_list = compute_coherence_values(dictionary=dictionary, corpus=doc_term_matrix, texts=doc_clean, start=1, limit=41, step=5)\n",
    "print(coherence_value_list)\n",
    "# Finding the index associated with maximum coherence value\n",
    "max_index=coherence_value_list.index(max(coherence_value_list))\n",
    "\n",
    "# Finding the optimum no. of topics associated with the maximum coherence value\n",
    "opt_topic= topic_list[max_index]\n",
    "print(\"Optimum no. of topics:\", opt_topic)\n",
    "\n",
    "# Implementing LDA with the optimum no. of topic\n",
    "lda_model = LdaModel(corpus=doc_term_matrix, num_topics=opt_topic, id2word = dictionary, iterations=10, passes = 30,random_state=0)\n",
    "\n",
    "# pprint(lda_model.print_topics(5))\n",
    "lda_model.print_topic(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
