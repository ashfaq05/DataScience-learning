{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "Understand how human languages are modelled for machines to understand\n",
    "\n",
    "## Overview\n",
    "\n",
    "The concept will introduce you to the language modeling concepts. In the concept you will learn\n",
    "\n",
    "- Problem of Modeling Language\n",
    "\n",
    "- Language Models\n",
    "\n",
    "- N-grams\n",
    "\n",
    "- Perplexity\n",
    "\n",
    "- Smoothing Techniques\n",
    "\n",
    "## Pre-requisite\n",
    "\n",
    "Before you start learning this concept, be sure you have already covered\n",
    "\n",
    "- Data wrangling with Pandas\n",
    "- Manipulating Data with NumPy\n",
    "- Summarizing Data with Statistics\n",
    "- Foundations of Text Analytics\n",
    "\n",
    "\n",
    "## Learning Outcomes\n",
    "\n",
    "By the end of this concept, you will be able to do the following\n",
    "\n",
    "- Understand why language modeling is hard\n",
    "\n",
    "- Learn how to create language models using n-gram\n",
    "\n",
    "- Understand how to evaluate language models using perplexity\n",
    "\n",
    "- Learn the need for smoothing and how to implement it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Problem of Modeling Language\n",
    "\n",
    "Description: In this chapter you will understand the difficulty in modeling languages with respect to machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Difficulty of natural languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Difficulty of natural languages**\n",
    "\n",
    "Consider you are designing a voice assistant like Siri(or Alexa) and you pass a voice command to it. The assistant outputs two sentences.\n",
    "\n",
    "s1= \"It's hard to recognize speech\" \n",
    "\n",
    "s2= \"It's hard to wreck a nice beach\"\n",
    "\n",
    "The above two sentences have the same sound signals. Sentence s1 is more likely but how do you tell the machine to have some intrinsic preference for one sentence over another?\n",
    "\n",
    "Consider another scenario where machine has to understand the meaning of the following sentences:\n",
    "- 7 foot doctors sue the hospital for negligence\n",
    "\n",
    "- The woman had to make a toast with a very old microphone\n",
    "\n",
    "- Andrew saw Max with a telescope\n",
    "\n",
    "- Look at the dog with one eye\n",
    "\n",
    "It's clear that the above sentences are ambigous and can have more than one intepretation.\n",
    "\n",
    "`Natural languages`(Languages spoken by humans) can never be fully specified. Reason for that is natural languages are not designed;they emerge. \n",
    "\n",
    "Formal rules for language does exist but often while conversing natural language that does not confirm is used. It also involves many terms that can be used in ways that result in complex ambiguities. Furthermore, languages change and along with that word usages change. \n",
    "\n",
    "Despite all this, natural languages are understood by other humans.\n",
    "\n",
    "Machines on the other hand usually work with `formal languages` that can be fully specified. All the words(terms) and rules for defining them are precisely defined.  \n",
    "\n",
    "We humans are able to understand natural languages majorly due to `\"context\"`. \n",
    "\n",
    "We intrinsically know that for the sentence \"7 foot doctors sue the hospital for negligence\", the meaning \"Seven doctors specialising in foot sue the hospital\" `is more likely` than the meaning \"Doctors who have seven foot sue the hospital\". We know the first meaning is `more probable` than the second meaning.\n",
    "\n",
    "Put simply, all we are doing is calculating probability of a sentence. Can't we have machines do that?\n",
    "\n",
    "This is what led to the development of something called `Language Models`\n",
    "\n",
    "**What is a Language model?**\n",
    "\n",
    "A Language model is a probabilistic model which estimates the relative likelihood of sentences(sequence of words).\n",
    "\n",
    "Language models were originally developed for the problem of speech recognition( and still play a central role in modern speech recognition systems). A language model learns the probability of word occurrence based on text examples we give. \n",
    "\n",
    "For simplicity, let's say it receives a sentence. The language model score for a sentence x is P(x) which is a score between 0 and 1, that can be interpreted as the probability of composing this sentence in English. \n",
    "\n",
    "Language models are able to capture some interesting language phenomena like the following: \n",
    "\n",
    "Which sentence is grammatically correct? - P(\"he eat pizza\") < P(\"he eats pizza\") \n",
    "\n",
    "Which word order is correct? - P(\"love I cats\") < P(\"I love cats\")\n",
    "\n",
    "Even some logic and world knowledge: What is more likely? - P(\"good British food\") < P(\"good Italian food\")\n",
    "\n",
    "In other words, language models generate score (probability) of a sentence, which will tell us whether the sentence is good or bad\n",
    "\n",
    "**Applications of Language Models**\n",
    "\n",
    "The probabilities returned by a language model are useful in many practical tasks, such as:\n",
    "\n",
    "***\n",
    "\n",
    "***Automatic Speech Recognition***: \n",
    "\n",
    "Speech recognition refers to the task of converting spoken words into written text.\n",
    "\n",
    "It has input in the form of sounds; \n",
    "\n",
    "A first layer in it predicts the candidate words based on the sound.\n",
    "\n",
    "For eg: Candidate words for a sound can be night, knight, right \n",
    "\n",
    "\n",
    "The language model(second layer) then helps in ranking the most likely sequence of words compatible with the candidate words produced by the first layer.\n",
    "\n",
    "For eg: \"I ate a cherry\" is a more likely sentence than \"Eye eight uh jerry\"\n",
    "\n",
    "![](spr.jpg)\n",
    "\n",
    "***\n",
    "\n",
    "***Machine Translation***: \n",
    "\n",
    "Machine Translation is the task of automatically translating one natural language into another while retaining the meaning of the original text. \n",
    "\n",
    "Each word from the source language is mapped to multiple candidate words of the target language; the language model of the target language then can rank the most likely sequence of candidate target words. This works because more likely sentences are probably better translations. \n",
    "\n",
    "![](mt_2.jpg)\n",
    "\n",
    "\n",
    "For eg: Translating a sentence refering about employees who left, model would probably state that `P(former employee) > P(older employee)` as the ‘older’ might also refer to age of the employee and thus, not as probable as ‘former’\n",
    "\n",
    "\n",
    "![](mt.png)\n",
    "\n",
    "***\n",
    "\n",
    "***Spell checking***: \n",
    "\n",
    "\n",
    "The task of spellchecking involves checking for spelling errors and possibly suggesting alternatives depending upon the context.\n",
    "\n",
    "Spell checking is done by the machine when it observes a word which is not recognized as a `known word` (i.e. the word does not occur in a list of known words). It then finds the closest known words to the unknown words.\n",
    "\n",
    "For eg: If someone writes `fomr`, the closest known words will be `from` and `form`. These are the candidate corrections. How can we select among these candidates the most likely correction for the error `fomr`?\n",
    "\n",
    "We then compare the Language Model probability of the sentences:\n",
    "\n",
    "- `P(name into form)`\n",
    "- `P(name into from)`\n",
    "\n",
    "and we hope that the right correction [name into form] will be selected.\n",
    "\n",
    "![](lm.jpg)\n",
    "\n",
    "***\n",
    "Let's now see how we can go about creating a language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Statistical Language Modeling\n",
    "\n",
    "Description: In this chapter, we will learn one of the most popular method of modeling language i.e. Statistical Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we established that one way to model natural languagues is by measuring the probability of sentences. So our aim is to get the probability of a given sentence as being good, but before that let's first focus on getting the probability of given word as being the next word in a given sentence.\n",
    "\n",
    "To build a language model, let's start with simple task of calculating probability $P(w|h)$, the probability of a word w given some history h. \n",
    "\n",
    "Suppose the history h is \"<i>its lake is so clear that</i>\" and we want to know the probability that the next word w is \"the\":\n",
    "\n",
    "\n",
    "$$ P (\\text{the} | \\text{its lake is so transparent that})$$\n",
    "<br/>\n",
    "\n",
    "One of the ways to estimate this probability is from frequency counts.\n",
    "\n",
    "Take a very large corpus(collection of written texts), count the number of times \"<i>its lake is so clear that</i>\" is present, and then count the number of times \"<i>this</i>\" is followed by \"<i>the</i>\". \n",
    "\n",
    "This would be similar to answering the question \"<i>Out of the times we saw the history h, how many times was it followed by the word w</i>\"\n",
    "\n",
    "So the probability will be calculated as:\n",
    "\n",
    "$$ P(\\text{the}|\\text{its lake is so clear that}) = \\frac{C(\\text{its lake is so clear that the})}{C(\\text{its lake is so clear that})} $$\n",
    "\n",
    "With a large enough corpus(***which internet is***), we can compute these counts and estimate the probability from above equation.\n",
    "\n",
    "While this method of estimating probabilities directly from counts is intuitive and works fine in many cases, it turns out that even the web isn’t big enough to give us good estimates in most of the cases. That is because as mentioned before, language is creative and new things emerge; new sentences are created all the time, and we won’t always be able to count entire sentences. Even simple extensions of the example sentence may have counts of 0 (such as “Amsterdam’s lake is so clear that the”).\n",
    "\n",
    "Additionally, if we wanted to know the probability(joint) of the entire sequence of words(which is what a language model has to do) in 'its lake is so clear', the question we need to solve is \"out of all possible sequences of five words, how many of them are 'its lake is so clear'?\" \n",
    "\n",
    "Mathematically, joint probability for a n-word sentence  will look something like:\n",
    "\n",
    "$$ \\begin{align} P(w_1^n) &= P(w_1).P(w_2 | w_1).P(w_3 | w_1^2).P(w_4 | w_1^3) \\dots\\dots P(w_n | w_1^{n-1})\\\\ &= \\prod_{k=1}^{n}P(w_k|w_1^{k-1}) \\end{align} $$\n",
    "\n",
    "\n",
    "We have to find probability of a word as being the next word with this history among all the possible history. \n",
    "\n",
    "\n",
    "Applying that to our sentence we will get\n",
    "\n",
    "\n",
    "$$ \\begin{align} P(\\text{its lake  is  so  clear that the})\\end{align}$$\n",
    "\n",
    "$$\\begin{align} = P(\\text{its}).P(\\text{lake} | \\text{its}).P(\\text{is}| \\text{its lake}).P(\\text{so}| \\text{its lake is}).P(\\text{clear}|\\text{its lake is so}).P(\\text{that}|\\text{its lake is so clear}).P(\\text{the}|\\text{its lake is so clear that})......\\end{align} $$\n",
    "\n",
    "This seems a lot of work doesn't it?\n",
    "\n",
    "Conclusion: We need better ways of estimating the probability of a word w given a history h.\n",
    "\n",
    "Construction of N-grams model is one of the solutions. \n",
    "\n",
    "**What is N-grams?**\n",
    "\n",
    "N-grams are the simplest type of tool available to construct a language model. \n",
    "\n",
    "An N-gram is a sequence of N words.\n",
    "\n",
    "*The intuition of the n-gram model is that instead of computing the probability of a word given its entire history, we can **approximate** the history by just the last few words.*\n",
    "\n",
    "\n",
    "The bigram(2- grams) model, for example, approximates the probability of a word w by using only the conditional probability of the preceding word $P(w_n |w_{n−1})$. Put in other way, instead of computing the probability \n",
    "\n",
    "$$P(\\text{the}|\\text{Amsterdam’s lake is so clear that})$$\n",
    "\n",
    "we approximate it with just the probability:\n",
    "$$P(\\text{the}|\\text{that})$$\n",
    "\n",
    "\n",
    "Similar to bigram, we also have unigram(n=1), trigram(n=3), 4-grams and so on.\n",
    "\n",
    "Following is an image explaining the difference:\n",
    "\n",
    "\n",
    "![](ngram.jpg)\n",
    "\n",
    "\n",
    "Let's try to understand n-grams better using an example.\n",
    " \n",
    " \n",
    "Consider a corpus containing the following four sentences and we would like to find the probability that “You” starts the sentence.\n",
    "\n",
    "$<s>$ You are a data scientist $</s>$\n",
    "\n",
    "$<s>$ Data scientist you are $</s>$\n",
    "\n",
    "$<s>$ You love statistics $</s>$\n",
    "\n",
    "\n",
    "Here $<s>$ and $</s>$ denote the start and end of the sentence respectively.\n",
    "\n",
    "**Note:** We need $<s>$ at the beginning of the sentence to get the bigram context of the first word. Similarly, we need the end-symbol $</s>$ to get the bigram context of the last word. \n",
    "\n",
    "\n",
    "Following will be the conditional probabilities of the words of corpus:\n",
    "\n",
    "\\begin{align}\n",
    "& P(You|<s>) = \\frac{2}{3} = .67  & \\qquad &  P(Data|<s>) = \\frac{1}{3} = .33  \\\\\n",
    "& P(</s>|scientist) = \\frac{1}{2} = .5  & \\qquad &  P(</s>|are) = \\frac{1}{2} = .5  \\\\\n",
    "& P(</s>|statistics) = \\frac{1}{1} = 1  & \\qquad &  P(are|You) = \\frac{2}{3} = .67  \\\\\n",
    "& P(a|are) = \\frac{1}{2} = .5  & \\qquad &  P(data|a) = \\frac{1}{1} = 1  \\\\\n",
    "& P(scientist|data) = \\frac{2}{2} = 1  & \\qquad &  P(statistics|love) = \\frac{1}{1}=1  \\\\\n",
    "& P(love|you) = \\frac{1}{3} = 0.33\n",
    "\\end{align}\n",
    "    \n",
    "Let's take the example of $P(are|You)$ above. First we calculate all possible bigrams in the corpus that is \"<i>You are</i>\". Then we calculate all possible bigrams in corpus with You(or you) as its first word.\n",
    "\n",
    "In the above corpus we have 2 instances of \"<i>You are</i>\" and 3 total instances where you is first word in a bigram(\"<i>You are</i>\" in first line, \"<i>you are</i>\" in second line and \"<i>You love</i>\" in third line). \n",
    "\n",
    "Therefore $P(are|You)=\\frac{2}{3}$\n",
    "\n",
    "\n",
    "\n",
    "When we use a bigram model to predict the probability, we are making the following approximation:\n",
    "$$ P(w_n |w^{n−1}_1 ) \\approx P(w_n |w_{n−1} ) $$ \n",
    "\n",
    "**Here $w_n$ is the $n_{th}$ word and $w^{n−1}_1$ is sequence of all n-1 words**\n",
    "\n",
    "This assumption that the probability of a word depends only on the previous word is called a Markov assumption\n",
    "\n",
    "**Markov Assumption**\n",
    "***\n",
    "A random process has the Markov property if we can predict the probability of  future states of the process without looking at every event in the past i.e next step depends only upon the present state and not on the sequence of events that preceded it. \n",
    "\n",
    "This helps in generalizing the bigram (which looks one word into the past) to trigram (which looks two words into the past) and ultimately to the n-gram (which looks n − 1 words into the past).\n",
    "***\n",
    "\n",
    "\n",
    "Markov assumption fits nicely with the n-gram model because of natural language's underlying property that in most of the cases, the probability of the word depends on its surrounding words.\n",
    "\n",
    "\n",
    "Let's now look at how we can calculate probabilities of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "- Test sentence tokens(Test sentence broken down to words) ,corpus and frequency counts(for unigram and bigram) is already given.(Print and see the values of the different values created)\n",
    "\n",
    "- Function definition of `get_bigram_probability()` with parameters `first`,`second` is given.\n",
    "\n",
    "\n",
    "- Inside the function:\n",
    "    - Store the conditional frequency of the `first` and `second` term(`conditional_freq[first][second]`) in a variable called `'bigram_freq'`.\n",
    "\n",
    "    - Store the frequency of the `first` term(`updated_uni_freq[first]`) in a variable called `'unigram_freq'`.\n",
    "\n",
    "    - Return the value calculated by dividing `'bigram_freq'` with `'unigram_freq'`\n",
    "\n",
    "\n",
    "- Create an empty list called `'prob_list'`. \n",
    "\n",
    "\n",
    "- Create a variable called `'previous'` and save the string `'*start_end*'` in it.(This will be our sentence beginner mark `<s>` that we encountered while learning bigram probabilities)\n",
    "\n",
    "\n",
    "- Run a loop `for token in test_sentence_tokens`. Inside the loop \n",
    "\n",
    "    - Calculate the bigram probability by calling the function `\"get_bigram_probability(previous, token)\"` and store it in a variable called `'next_probability'`\n",
    "    - Save the current `'token'` as `'previous'`\n",
    "    - Append `'next_probability'` to `'prob_list'`\n",
    "    \n",
    "    \n",
    "**Note:** Calculation of the final term is still left.\n",
    "\n",
    "- Calculate the bigram prob. of the final term by calling the function `\"get_bigram_probability()\"` with `'previous'`(This will be store the final term after coming out of the loop) and `'*start_end*'`.\n",
    "\n",
    "\n",
    "- Append the above calculated value to `'prob_list'`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of tokens of the sample sentence: 1161192\n",
      "Frequency of \" this \" is  5145\n",
      "Frequency of \" is \" is  10109\n",
      "Frequency of \" a \" is  23195\n",
      "Frequency of \" sunny \" is  13\n",
      "Frequency of \" day \" is  687\n",
      "Frequency of \" . \" is  49346\n",
      "\n",
      "\n",
      "\n",
      "Calculating bigram probalities for sentence, including bigrams with sentence boundaries, i.e., *start_end*\n",
      "*start_end* this 0.0083\n",
      "this is 0.0503\n",
      "is a 0.0861\n",
      "a sunny 4.51e-05\n",
      "sunny day 0.154\n",
      "day . 0.163\n",
      ". *start_end* 1.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Corpus\n",
    "words = brown.words()\n",
    "words=[w.lower() for w in words]\n",
    "\n",
    "# Unigram frequency \n",
    "uni_freq = nltk.FreqDist(w.lower() for w in words)\n",
    "\n",
    "# Size of corpus\n",
    "total_words = len(words)\n",
    "\n",
    "print('Frequency of tokens of the sample sentence:',total_words)\n",
    "\n",
    "#Sentence \n",
    "test_sentence_tokens=['this','is','a','sunny','day','.']\n",
    "\n",
    "\n",
    "for word in test_sentence_tokens:\n",
    "    print('Frequency of \"',word,'\" is ',uni_freq[word])\n",
    "\n",
    "print('\\n\\n')\n",
    "    \n",
    "# Creating bigrams\n",
    "\n",
    "bigram_words = []\n",
    "previous = 'EMPTY'\n",
    "sentences = 0\n",
    "for word in words:\n",
    "    if previous in ['EMPTY','.','?','!']:\n",
    "        ## insert word_boundaries at beginning of Brown,\n",
    "        bigram_words.append('*start_end*')\n",
    "    else:\n",
    "        bigram_words.append(word)\n",
    "    \n",
    "    previous = word\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "bigram_words.append('*start_end*') ## assume one additional *start_end* at the end of Brown\n",
    "\n",
    "updated_uni_freq  = nltk.FreqDist(w.lower() for w in bigram_words)\n",
    "\n",
    "\n",
    "print('Calculating bigram probalities for sentence, including bigrams with sentence boundaries, i.e., *start_end*')\n",
    "\n",
    "\n",
    "# Bigram corpus\n",
    "bigrams = nltk.bigrams(w.lower() for w in bigram_words)\n",
    "\n",
    "\n",
    "# Bigram probabilities\n",
    "conditional_freq = nltk.ConditionalFreqDist(bigrams)\n",
    "\n",
    "\n",
    "\n",
    "# Code begins here\n",
    "\n",
    "\n",
    "# Function to calculate bigram probability\n",
    "def get_bigram_probability(first,second):\n",
    "    \n",
    "    bigram_freq = conditional_freq[first][second]\n",
    "    unigram_freq = updated_uni_freq[first]\n",
    "\n",
    "    bigram_prob = (bigram_freq)/(unigram_freq)\n",
    "    \n",
    "    return bigram_prob\n",
    "\n",
    "## Calculating the bigram probability\n",
    "\n",
    "prob_list=[]\n",
    "previous = '*start_end*'\n",
    "\n",
    "for token in test_sentence_tokens:\n",
    "    next_probability = get_bigram_probability(previous,token)\n",
    "    print(previous,token,(float('%.3g' % next_probability)))\n",
    "    previous = token\n",
    "    prob_list.append(next_probability)\n",
    "\n",
    "\n",
    "    \n",
    "# For the final term    \n",
    "next_probability = get_bigram_probability(previous,'*start_end*')\n",
    "print(previous,'*start_end*',next_probability)\n",
    "prob_list.append(next_probability)    \n",
    "\n",
    "# print(prob_list)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hints\n",
    "\n",
    "You can find the bigram probabilities by writing code similar to:\n",
    "\n",
    "```python\n",
    "prob_list=[]\n",
    "previous = '*start_end*'\n",
    "for token in test_sentence_tokens:\n",
    "    next_probability = get_bigram_probability(previous,token)\n",
    "    print(previous,token,(float('%.3g' % next_probability)))\n",
    "    previous = token\n",
    "    prob_list.append(next_probability)\n",
    "\n",
    "    \n",
    "# For the final term    \n",
    "next_probability = get_bigram_probability(previous,'*start_end*')\n",
    "print(previous,'*start_end*',next_probability)\n",
    "prob_list.append(next_probability)    \n",
    "\n",
    "print(prob_list)    \n",
    "```\n",
    "\n",
    "# Test Cases\n",
    "\n",
    "#prob_list\n",
    "\n",
    "\n",
    "Variable declaration\n",
    "\n",
    "\n",
    "round(prob_list[1],2)==0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Success Message\n",
    "\n",
    "Congrats! You have successfully found out bigram probabilities of the sentence tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Language model using n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous topic, we constructed an N-gram model(bigram model in our case) for words, how will we now obtain a complete language model on the basis of this ?\n",
    "\n",
    "Simple, we will calculate the `joint probability` by multiplying the respective n-gram probabilites.\n",
    "\n",
    "Applying that to our sentence(from the previous chapter) we see that the following calculation: \n",
    "\n",
    "\n",
    "$$ \\begin{align} P(\\text{its lake  is  so  clear that the})\\end{align}$$\n",
    "\n",
    "$$\\begin{align} = P(\\text{its}).P(\\text{lake} | \\text{its}).P(\\text{is}| \\text{its lake}).P(\\text{so}| \\text{its lake is}).P(\\text{clear}|\\text{its lake is so}).P(\\text{that}|\\text{its lake is so clear}).P(\\text{the}|\\text{its lake is so clear that})\\end{align}..... $$\n",
    "\n",
    "gets reduced to\n",
    "\n",
    "$$ \\begin{align} P(\\text{its lake  is  so  clear that the})\\end{align}$$\n",
    "$$\\begin{align} = P(\\text{its}|P\\text{<s>}).P(\\text{lake} | \\text{its}).P(\\text{is}| \\text{lake}).P(\\text{so}| \\text{is}).P(\\text{clear}|\\text{so}).P(\\text{that}|\\text{clear}).P(\\text{the}|\\text{that})\\end{align}.... $$\n",
    "\n",
    "\n",
    "***\n",
    "**Deep Dive(Optional)**\n",
    "\n",
    "*Mathematical representation of how joint probability is calculated:*\n",
    "\n",
    "\n",
    "Joint-probability of n-word sentence is:\n",
    "\n",
    "$$ \\begin{align} P(w_1^n) &= P(w_1).P(w_2 | w_1).P(w_3 | w_1^2).P(w_4 | w_1^3) \\dots\\dots P(w_n | w_1^{n-1})\\\\\n",
    " &= \\prod_{k=1}^{n}P(w_k|w_1^{k-1}) \\end{align} $$\n",
    "\n",
    "Using Markov's assumption, we know\n",
    "\n",
    "$$ P(w_n |w^{n−1}_1 ) \\approx P(w_n |w_{n−1} ) $$\n",
    "\n",
    "\n",
    "That will help obtain the following approximation:\n",
    "\n",
    "\\begin{align} P(w_1^n) = \\prod_{n=1}^{n}P(w_k|w_{k-1}) \\end{align}\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "Let us now try to understand how language model work better using a bigram example:\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>Given below is the count of random 8 words(out of 100 distinct words) from a food delivery app(Also known as the unigram table)</center>\n",
    "\n",
    "\n",
    "|i|want|to|eat|italian|food|lunch|breakfast\n",
    "|-----|-----|-----|-----|-----|-----|-----|-----|\n",
    "|2532|928|2419|746|158|1012|342|277|\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center>Following is the bigram table for the same words</center> \n",
    "\n",
    "<br>\n",
    "\n",
    "|-|i|want|to|eat|italian|food|lunch|buy\n",
    "|-----|-----|-----|-----|-----|-----|-----|-----|-----|\n",
    "|i|6|818|0|9|0|0|0|2|\n",
    "|want|2|0|608|1|6|7|6|1|\n",
    "|to|2|0|4|685|2|0|6|211|\n",
    "|eat|0|0|2|0|15|3|42|0|\n",
    "|italian|1|0|0|0|0|82|1|0|\n",
    "|food|14|0|15|0|1|5|0|0|\n",
    "|lunch|2|0|0|0|0|0|0|1|\n",
    "|buy|1|0|0|0|0|14|0|0|\n",
    "\n",
    "<br>\n",
    "    \n",
    "<center>Here row word is the first word and column word is the second word.</center>\n",
    "\n",
    "\n",
    "<center>For example \"<i>i want</i>\" bigram appears 818 times in the corpus, \"<i>eat italian</i>\" bigram appears 15 times in the corpus.</center> \n",
    "\n",
    "\n",
    "You can make a lot of interesting observations when comparing unigram table with the bigram.\n",
    "\n",
    "For e.g. Out of the 928 times the word `want` appears, 818 times it appears after the word `I`. \n",
    "\n",
    "\n",
    "<br><br>\n",
    "<center>After calculating the bigram probability($P(w_n |w_{n−1} )$) we get the following table</center>\n",
    "<br>\n",
    "\n",
    "|-|i|want|to|eat|italian|food|lunch|buy\n",
    "|-----|-----|-----|-----|-----|-----|-----|-----|-----|\n",
    "|i|0.002|0.32|0|0.003|0|0|0|0.0007|\n",
    "|want|0.002|0|0.65|0.001|0.006|0.007|0.006|0.001|\n",
    "|to|0.0008|0|0.001|0.28|0.0008|0|0.002|0.08|\n",
    "|eat|0|0|0.002|0|0.02|0.004|0.05|0|\n",
    "|italian|0.006|0|0|0|0|0.51|0.006|0|\n",
    "|food|0.01|0|0.01|0|0.0009|0.01|0|0|\n",
    "|lunch|0.005|0|0|0|0|0|0|0.002|\n",
    "|breakfast|0.004|0|0|0|0|0.05|0|0|\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>To get the probabilities we divide each count value of our bigram table with the unigram count of the first word of the bigram.<\\center>\n",
    "\n",
    "<br><br>\n",
    "For eg: To get the bigram probability of \"<i>eat italian</i>\", we divide the bigram count of \"<i>eat italian</i>\" which is 15 by the total no. of times(unigram count), eat(first word of bigram) appears in the corpus which is 746.\n",
    "\n",
    "Therefore, $$ \\begin{align} P(\\text{eat italian}) = \\frac{15}{746}= 0.02\\end{align}$$\n",
    "\n",
    "Using the bigram probability table, we can now easily compute the probability of sentences like `I want italian lunch` by simply multiplying the appropriate bigram probabilities together, as follows:\n",
    "\n",
    "$$\\begin{align}\n",
    "P(\\text{<s> i want italian lunch </s>})\n",
    "&= P(\\text{i|<s>}).P(\\text{want|i}).P(\\text{italian|want}).P(\\text{lunch|italian}).P(\\text{</s>|lunch}) \\\\\n",
    "&= .22 \\times .32 \\times .006 \\times 0.006 \\times 0.7 \\\\\n",
    "&= 0.0000017\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Note:** $P(\\text{i|<s>})$ and $P(\\text{</s>|lunch})$ were not in the above bigram probability table but can be easily calculated from the corpus in a similar way.\n",
    "\n",
    "\n",
    "You can refresh about n-gram model by going through this video on [N-gram by Machine Learning TV](https://www.youtube.com/watch?v=GiyMGBuu45w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Evaluating LMs: Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just successfully constructed one language model using n-grams.\n",
    "\n",
    "We now need an evaluation method to check how good it is(Especially because we have taken the Markov assumption) from the \"actual\" probability of the sentences?\n",
    "\n",
    "Therefore we need a measure to evaluate language models.Following are the two popular ways:\n",
    "\n",
    "**Extrinsic evaluation:** This is the best and most intuitive way to evaluate a model . It involves testing different models in how much they help the application \n",
    "\n",
    "For e.g. We want to evaluate the language model for a spell checker. Thus, for spell checker, we can compare the performance of two language models by running the spell checker twice, once with each language model, and seeing which gives the more accurate correction. \n",
    "\n",
    "Unfortunately, running big NLP systems end-to-end is an expensive form of evaluation.\n",
    "\n",
    "**Intrinsic evaluation**: \n",
    "\n",
    "It would be convenient to have a method that can be used to quickly evaluate potential improvements in a language model. \n",
    "\n",
    "An intrinsic evaluation method is one that measures the quality of a model independent of any application.\n",
    "\n",
    "Just like most of the statistical models in data science field, the probabilities of an n-gram model come from the corpus it is trained on, known as the training set. We can then measure the performance of the n-gram model by its performance on the unseen data also known as the test set.\n",
    "\n",
    "Whichever model assigns a higher probability to the sentences present in the test set is a better model. \n",
    "\n",
    "\n",
    "*Q:* So should we just compare the raw probabilities of different models to decide which model is intrinsically better?\n",
    "\n",
    "*A:* NO\n",
    "\n",
    "The reason for it is that not all probability distributions are created equal.\n",
    "\n",
    "For eg: \n",
    "\n",
    "There's a lot more uncertainty about the outcome of the word `surprise` in a 1000 word article as compared to a novel.(Novel has a bigger corpus than the Article) \n",
    "\n",
    "\n",
    "Another reason is if two distributions have the same number of outcomes, how likely those outcomes are also affects your uncertainty.\n",
    "\n",
    "For eg:\n",
    "Given two 500 word essays written one on 'Global Warming' and 'Formula F1 Race', you are a lot less uncertain about the word  'Polar Bears' on the first essay than you are in the second essay.\n",
    "\n",
    "\n",
    "In practice we don’t use raw probability as our metric for evaluating language models, but a variant called **perplexity**. \n",
    "\n",
    "Perplexity gives measures of complexity in a way that accounts for the above two reasons.\n",
    "\n",
    "The perplexity of a language model on a test set is the inverse probability of the test set, normalized by the number of words. \n",
    "\n",
    "For a test set $W = w_1 w_2 \\dots w_N ,$: \n",
    "\n",
    "$$\\begin{align} PP(W) &= {(P(w_1 w_2 \\dots w_N))}^{-\\frac{1}{N}}\\\\\\end{align}$$\n",
    "<br>\n",
    "$$\\begin{align}&= \\sqrt[N]{\\frac{1}{P(w_1 w_2 \\dots w_N)}} \\\\\\end{align}$$\n",
    "<br>\n",
    "$$\\begin{align}&= \\sqrt[N]{\\prod_{i=1}^{N}\\frac{1}{P(w_i|w_1 w_2 \\dots w_{i-1})}}\\\\\\end{align}$$\n",
    "<br>\n",
    "\n",
    "$$\\begin{align}&\\text{Replacing the perplexity with a n-gram model, say a bigram language model, we get :}\\\\\\end{align}$$\n",
    "$$\\begin{align}PP(W) &= \\sqrt[N]{\\prod_{i=1}^{N}\\frac{1}{P(w_i|w_{i-1})}}\\\\\\end{align}$$\n",
    "\n",
    "\n",
    "**Note:** \n",
    "\n",
    "1. The inverse in the formula means the higher the conditional probability of the word sequence, the lower the perplexity. Therefore, minimizing perplexity is equivalent to maximizing the test set probability of the language model.\n",
    "\n",
    "2. The term 1/N where N is the number of words, helps normalize for the length of the probability by the number of words. This way the longer the sentence the less probable it will be.     \n",
    "\n",
    "3. Based on multiple experiments, it's observed that of all the n-gram models, trigram(n=3) models perform the best in predicting the 'real' world probabilities  \n",
    "\n",
    "You can have a better understanding about Evaluating language models by going through the video on [Evaluation and Perplexity by André Ribeiro de Miranda](https://www.youtube.com/watch?v=BAN3NB_SNHY)\n",
    "\n",
    "\n",
    "**Evaluation problem**\n",
    "\n",
    "Consider the following bigram table from the previous topic:\n",
    "\n",
    "|-|i|want|to|eat|italian|food|lunch|buy\n",
    "|-----|-----|-----|-----|-----|-----|-----|-----|-----|\n",
    "|i|0.002|0.32|0|0.003|0|0|0|0.0007|\n",
    "|want|0.002|0|0.65|0.001|0.006|0.007|0.006|0.001|\n",
    "|to|0.0008|0|0.001|0.28|0.0008|0|0.002|0.08|\n",
    "|eat|0|0|0.002|0|0.02|0.004|0.05|0|\n",
    "|italian|0.006|0|0|0|0|0.51|0.006|0|\n",
    "|food|0.01|0|0.01|0|0.0009|0.01|0|0|\n",
    "|lunch|0.005|0|0|0|0|0|0|0.002|\n",
    "|breakfast|0.004|0|0|0|0|0.05|0|0|\n",
    "\n",
    "\n",
    "\n",
    "In the table majority of the values are zero. A matrix selected from a random set of 10 words would be even more sparse. \n",
    " \n",
    "\n",
    "The model we have assumed so far suffers from two drastic problems:\n",
    "\n",
    "**1. Sparsity**\n",
    "\n",
    "For any n-gram that has occurred a sufficient no. of times, we might have a good estimate of its probability. But because any test corpus is limited, some perfectly acceptable word sequences are bound to be missing from it. \n",
    "\n",
    "Since there are a combinatorial no. of possible strings, many rare(but not impossible) combinations never occur in training resulting in system incorrectly assigning zero probability to many parameters\n",
    "\n",
    "For eg: If the bank data training set has the following sentences(among many others):\n",
    "\n",
    "\"he was denied the loan\"\n",
    "\n",
    "\"he was denied the loan offer\"\n",
    "\n",
    "\"loan was refused to him\"\n",
    "\n",
    "But suppose our test set had a phrase like\n",
    "\n",
    "\"loan was denied to him\"\n",
    "\n",
    "That sentence makes perfect sense but the $ P(\\text {to|denied}) $ will be 0 resulting in the overall probability of the test sentence to be equal to 0.\n",
    "\n",
    "\n",
    "**2. Limited vocabulary**\n",
    "\n",
    "We assume our model knows all the words in the vocabulary which is rarely the case.\n",
    "\n",
    "Consider the following sentences of news data training set:\n",
    "\n",
    "\"denied the rumours\"\n",
    "\n",
    "\"denied the report\"\n",
    "\n",
    "\"denied the allegations\"\n",
    "\n",
    "\"denied the news\"\n",
    "\n",
    "But suppose our test set had a phrase like\n",
    "\n",
    "\"denied the speculations\"\n",
    "\n",
    "Even though the test phrase makes perfect sense, since \"speculations\" word was not in the training set, $ P(\\text {speculations|the})$ will be 0 resulting in the overall probability of the test sentence to be equal to 0.\n",
    "\n",
    "We could choose a vocabulary (word list) that is fixed in advance but in doing so we are limiting our model immensely.\n",
    "\n",
    "\n",
    "How do we solve the dual problem of limited words and limited sentence combinations that are in train set but appear in a test set in an unseen context?\n",
    "\n",
    "To keep a language model from assigning zero probability to these unseen events, we’ll have to take a bit of probability from some more frequent events and give it to the events we’ve never seen.\n",
    "\n",
    "This modification is called smoothing. \n",
    "\n",
    "Let's look at smoothing in detail in the next topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "- `prob_list_1` contains the bigram probabilities of `this is a sunny day`.\n",
    "\n",
    "- Multiply all the values of `'prob_list_1'` to find the bigram model probability of the sentence and store the result in a variable called `total_prob_1`\n",
    "\n",
    "***\n",
    "Following is a sample code calculation of perplexity\n",
    "\n",
    "**Input:**\n",
    "\n",
    "```python\n",
    "\n",
    "prob_list=[0.1, 0.023 ,0.09]\n",
    "\n",
    "\n",
    "perplexity=1\n",
    "\n",
    "# Calculating N\n",
    "N=len(prob_list)-2\n",
    "\n",
    "\n",
    "# Calculating the perplexity\n",
    "for val in prob_list:\n",
    "    perplexity = perplexity * (1/val)\n",
    "\n",
    "perplexity = pow(perplexity, 1/float(N)) \n",
    "\n",
    "print(\"Perplexity= :\",perplexity)\n",
    "```\n",
    "**Output:**\n",
    "\n",
    "```python\n",
    "Perplexity= : 69.5048046856916\n",
    "```\n",
    "***\n",
    "\n",
    "- Calculate the perplexity of the values of `'prob_list_1'`(similar to the above code) and store the result in a variable called `'perplexity_1'`. \n",
    "\n",
    "\n",
    "- `prob_list_2` contains the bigram probabilities of `this place is beautiful`.\n",
    "\n",
    "- Multiply all the values of `'prob_list_2'` to find the bigram model probability of the sentence and store the result in a variable called `total_prob_2`\n",
    "\n",
    "- Calculate the perplexity of the values of `'prob_list_2'`(similar to the above code) and store the result in a variable called `'perplexity_2'`\n",
    "\n",
    "\n",
    "**Things to ponder upon:**\n",
    "\n",
    "- Which sentence has a lower perplexity?\n",
    "\n",
    "- Between perplexity and total probability, which metric gives a better intuitive understanding of more probable sentence?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity= : 69.5048046856916\n"
     ]
    }
   ],
   "source": [
    "prob_list=[0.1, 0.023 ,0.09]\n",
    "\n",
    "\n",
    "perplexity=1\n",
    "\n",
    "# Calculating N\n",
    "N=len(prob_list)-1\n",
    "\n",
    "\n",
    "# Calculating the perplexity\n",
    "for val in prob_list:\n",
    "    perplexity = perplexity * (1/val)\n",
    "\n",
    "perplexity = pow(perplexity, 1/float(N)) \n",
    "\n",
    "print(\"Perplexity= :\",perplexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the sentence- 'this is a sunny day'\n",
      "Total probability: 2.494655687321879e-10\n",
      "Perplexity: 251.62126814544143\n",
      "\n",
      "\n",
      "For the sentence- 'this place is beautiful'\n",
      "Total probability:  4.009684736463708e-11\n",
      "Perplexity:  2921.6616783932823\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"For the sentence: 'this is a sunny day' \"\"\" \n",
    "prob_list_1=[0.008303975842979365, 0.05030826140567201, 0.08609535184632229, 4.5083630133898384e-05, 0.15384615384615385]\n",
    "\n",
    "\n",
    "\n",
    "total_prob_1 = 1\n",
    "\n",
    "# Multiplying all the values of the probability and storing it\n",
    "for val in prob_list_1:\n",
    "    total_prob_1 *= val\n",
    "\n",
    "\n",
    "print(\"For the sentence- 'this is a sunny day'\")\n",
    "print(\"Total probability:\",total_prob_1)\n",
    "\n",
    "\n",
    "perplexity_1=1\n",
    "\n",
    "# Calculating N\n",
    "N=len(prob_list_1)-1\n",
    "\n",
    "\n",
    "# Calculating the perplexity\n",
    "for val in prob_list_1:\n",
    "    perplexity_1 = perplexity_1 * (1/val)\n",
    "\n",
    "perplexity_1 = pow(perplexity_1, 1/float(N)) \n",
    "\n",
    "print(\"Perplexity:\",perplexity_1)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"For the sentence: 'this place is beautiful' \"\"\"\n",
    "prob_list_2=[0.008303975842979365, 0.0022194821208384712, 0.02185792349726776, 9.953219866626854e-05]\n",
    "\n",
    "total_prob_2 = 1\n",
    "\n",
    "# Multiplying all the values of the probability and storing it\n",
    "for val in prob_list_2:\n",
    "    total_prob_2 *= val\n",
    "\n",
    "print(\"\\n\\nFor the sentence- 'this place is beautiful'\")    \n",
    "print(\"Total probability: \",total_prob_2)\n",
    "\n",
    "\n",
    "perplexity_2=1\n",
    "\n",
    "# Calculating N\n",
    "N=len(prob_list_2)-1\n",
    "\n",
    "# Calculating perplexity\n",
    "for val in prob_list_2:\n",
    "    perplexity_2 = perplexity_2 * (1/val)\n",
    "\n",
    "perplexity_2 = pow(perplexity_2, 1/float(N)) \n",
    "\n",
    "print(\"Perplexity: \",perplexity_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hints\n",
    "You can find perplexity of sentence 1 by writing code similar to:\n",
    "```python\n",
    "for val in prob_list_1:\n",
    "    perplexity_1 = perplexity_1 * (1/val)\n",
    "\n",
    "perplexity_1 = pow(perplexity_1, 1/float(N)) \n",
    "```\n",
    "Similarly, you can find perplexity of the other sentence.\n",
    "\n",
    "\n",
    "\n",
    "# Test Cases\n",
    "\n",
    "\n",
    "#total_prob_1\n",
    "Variable declaration\n",
    "round(total_prob_1,10)==2e-10\n",
    "\n",
    "\n",
    "#perplexity_1\n",
    "Variable declaration\n",
    "round(perplexity_1,2)==251.62\n",
    "\n",
    "#total_prob_2\n",
    "Variable declaration\n",
    "round(total_prob_2,11)==4e-11\n",
    "\n",
    "#perplexity_2\n",
    "Variable declaration\n",
    "round(perplexity_2,2)==2921.66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Success Message\n",
    "\n",
    "Congrats! You have successfully found the perplexity and total probabilities of the given sentences!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Smoothing\n",
    "\n",
    "Description: In this chapter, we will learn the different types of Smoothing that can be done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Add-K Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just understood that to keep a language model from assigning zero probability to these unseen events, we could take a bit of probability from some more frequent events and give it to the events we’ve never seen by a method called smoothing. \n",
    "\n",
    "Following are some of the popular smoothing techniques:\n",
    "\n",
    "- Laplace Smoothing/Add-K smoothing\n",
    "\n",
    "- Interpolation\n",
    "\n",
    "- Backoff\n",
    "\n",
    "Let's try to understand them one by one.\n",
    "\n",
    "**Laplace Smoothing**\n",
    "\n",
    "The simplest way to do smoothing would be to add one to all the bigram(or any n-gram) counts, before we normalize them into probabilities. All the counts that used to be 0 will now have a count of 1, the counts of 1 will be 2, and so on and so forth. \n",
    "\n",
    "This kind of smoothing is called Laplace smoothing. \n",
    "\n",
    "Let’s start with the application of Laplace smoothing to unigram(single word) probabilities. \n",
    "\n",
    "Mathematically If unsmoothed unigram probability of the word $w_i$ is its count $c_i$ normalized by the total number of word tokens $N$:\n",
    "\n",
    "$$P(w_i) = \\frac{c_i}{N}$$\n",
    "\n",
    "Laplace smoothing merely adds one to each count (Its also called one-smoothing). If there are V words in the vocabulary and each one is incremented, we also need to adjust the denominator to take into account the extra V observations.\n",
    "\n",
    "$$P_{Laplace}(w_i) = \\frac{c_i + 1}{N + V}$$\n",
    "\n",
    "Let us try to understand Laplace smoothing of bigrams with the previous food delivery app example.\n",
    "\n",
    "<br>\n",
    "<center>Following is the original bigram count table</center>\n",
    "\n",
    "|-|i|want|to|eat|italian|food|lunch|buy\n",
    "|-----|-----|-----|-----|-----|-----|-----|-----|-----|\n",
    "|i|6|818|0|9|0|0|0|2|\n",
    "|want|2|0|608|1|6|7|6|1|\n",
    "|to|2|0|4|685|2|0|6|211|\n",
    "|eat|0|0|2|0|15|3|42|0|\n",
    "|italian|1|0|0|0|0|82|1|0|\n",
    "|food|14|0|15|0|1|5|0|0|\n",
    "|lunch|2|0|0|0|0|0|0|1|\n",
    "|buy|1|0|0|0|0|14|0|0|\n",
    "\n",
    "<br><br>\n",
    "<center>After Laplace smoothing, the table transforms to</center>\n",
    "\n",
    "|-|i|want|to|eat|italian|food|lunch|buy\n",
    "|-----|-----|-----|-----|-----|-----|-----|-----|-----|\n",
    "|i|7|819|1|10|1|1|1|3|\n",
    "|want|3|1|609|2|7|8|7|2|\n",
    "|to|3|1|5|686|3|1|7|212|\n",
    "|eat|1|1|3|1|16|4|43|1|\n",
    "|italian|2|1|1|1|1|83|2|1|\n",
    "|food|15|1|16|1|2|6|1|1|\n",
    "|lunch|3|1|1|1|1|1|1|2|\n",
    "|buy|2|1|1|1|1|15|1|1|\n",
    "\n",
    "\n",
    "We know that normal bigram probabilities are computed using the following:\n",
    "$$ P(w_n|w_{n-1}) = \\frac{C(w_{n-1}w_n)}{C(w_{n-1})} $$\n",
    "\n",
    "This resulted in the following bigram probability table:\n",
    "\n",
    "|-|i|want|to|eat|italian|food|lunch|buy\n",
    "|-----|-----|-----|-----|-----|-----|-----|-----|-----|\n",
    "|i|0.002|0.32|0|0.003|0|0|0|0.0007|\n",
    "|want|0.002|0|0.65|0.001|0.006|0.007|0.006|0.001|\n",
    "|to|0.0008|0|0.001|0.28|0.0008|0|0.002|0.08|\n",
    "|eat|0|0|0.002|0|0.02|0.004|0.05|0|\n",
    "|italian|0.006|0|0|0|0|0.51|0.006|0|\n",
    "|food|0.01|0|0.01|0|0.0009|0.01|0|0|\n",
    "|lunch|0.005|0|0|0|0|0|0|0.002|\n",
    "|breakfast|0.004|0|0|0|0|0.05|0|0|\n",
    "\n",
    "\n",
    "\n",
    "For add-one smoothed bigram counts, we just need to normalize the unigram count(denominator) by the number of distinct words V(in this case V=100) in the vocabulary:\n",
    "$$ P^*_{Laplace}(w_n|w_{n-1}) = \\frac{C(w_{n-1}w_n) + 1}{C(w_{n-1}) + V} $$\n",
    "\n",
    "This will result in the following bigram probability table\n",
    "\n",
    "|-|i|want|to|eat|italian|food|lunch|buy\n",
    "|-----|-----|-----|-----|-----|-----|-----|-----|-----|\n",
    "|i|0.002|0.31|0.0003|0.003|0.0003|0.0003|0.0003|0.001|\n",
    "|want|0.002|0.0009|0.59|0.001|0.006|0.007|0.006|0.001|\n",
    "|to|0.001|0.0003|0.001|0.27|0.001|0.0003|0.002|0.08|\n",
    "|eat|0.001|0.001|0.003|0.001|0.01|0.004|0.05|0.001|\n",
    "|italian|0.007|0.003|0.003|0.003|0.003|0.32|0.007|0.003|\n",
    "|food|0.01|0.0008|0.01|0.0008|0.001|0.005|0.0008|0.0008|\n",
    "|lunch|0.006|0.002|0.002|0.002|0.002|0.002|0.002|0.004|\n",
    "|breakfast|0.005|0.002|0.002|0.002|0.002|0.03|0.002|0.002|\n",
    "\n",
    "\n",
    "You can see that 0 probabilities have been converted to some non zero value and at the same time, the value of earlier non zero probabilities has also reduced for overall probability distribution(For e.g. $P(\\text{to|want}$) changed from 0.65 to 0.59)\n",
    "\n",
    "The sharp change in probabilities occur because too much probability mass is moved to all the zeros\n",
    "\n",
    "\n",
    "Let's calculate the probability of the sentence \"i want italian lunch\" again \n",
    "\n",
    "$$\\begin{align}\n",
    "P(\\text{<s> i want italian lunch </s>})\n",
    "&= P(\\text{i|<s>}).P(\\text{want|i}).P(\\text{italian|want}).P(\\text{lunch|italian}).P(\\text{</s>|lunch}) \\\\\n",
    "&= .22 \\times .31 \\times .006 \\times 0.007 \\times 0.7 \\\\\n",
    "&= 0.000002\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "Though a practical smoothing algorithm for tasks like text classification, unfortunately, Laplace smoothing doesn't perform well for n-gram models.\n",
    "\n",
    "**Add-K Smoothing**\n",
    "\n",
    "One way to move a bit less of the `probability mass` from the seen to the unseen events is instead of adding `1` to each count, we just add a fractional count `k` (.5? .02?). \n",
    "\n",
    "This modified add-1(Laplace) smoothing is called add-k smoothing.\n",
    "\n",
    "Here instead of incrementing count by 1 we increment count by a fractional value, helping us transfer a lesser amount of probability from seen values of corpus to the unseen values of corpus. \n",
    "\n",
    "Mathematical formula of finding probabilities after add-k smoothing is \n",
    "\n",
    "$$ P^*_{add-k}(w_n|w_{n-1}) = \\frac{C(w_{n-1}w_n) + k}{C(w_{n-1}) + kV} $$\n",
    "\n",
    "There are multiple methods for selecting the optimum k value. For example, by optimizing  it on the testset. \n",
    "\n",
    "Though better than Add-1 smoothing, Add-k smoothing still doesn’t work well for language modeling often leading to poor variance\n",
    "\n",
    "Let's look at some other alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK\n",
    "\n",
    "- The working code for the first task you completed is given with a new sentence `sunset looks magnificient.`\n",
    "\n",
    "- Run the code once as it is.\n",
    "\n",
    "**We get an error of `division by 0` because magnificient is not in our corpus.**\n",
    "\n",
    "**Let's resolve that using Laplace Smoothing**\n",
    "\n",
    "\n",
    "- Calculate Vocabulary of the corpus by finding the no. of unique words in the list `'words'` and save the the count in a variable called `'V'`\n",
    "\n",
    "- Inside the function:\n",
    "        -Update the calculation  of the term `'bigram_prob'` by adding `1` to `'bigram_freq'` and `V` to `'unigram_freq'`\n",
    "        \n",
    "- Multiply all the values of `'prob_list'` to find the bigram model probability of the sentence and store the result in a variable called `total_prob`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of tokens of the sample sentence:\n",
      "this 5145\n",
      "is 10109\n",
      "a 23195\n",
      "sunny 13\n",
      "day 687\n",
      ". 49346\n",
      "\n",
      "Calculating bigram counts for sentence, including bigrams with sentence boundaries, i.e., *BEGIN* and *END*\n",
      "*start_end* sunset 9.48e-06\n",
      "sunset looks 2.01e-05\n",
      "looks magnificient 2e-05\n",
      "magnificient . 2.01e-05\n",
      ". *start_end* 0.49764524359375156\n",
      "[9.48307744829352e-06, 2.0068634730779264e-05, 2.004329351399022e-05, 2.007427481682224e-05, 0.49764524359375156]\n",
      "\n",
      "Total probability: 3.8106225670516194e-20\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Corpus\n",
    "words = brown.words()\n",
    "words=[w.lower() for w in words]\n",
    "\n",
    "# Unigram frequency \n",
    "uni_freq = nltk.FreqDist(w.lower() for w in words)\n",
    "\n",
    "# Size of corpus\n",
    "total_words = len(words)\n",
    "\n",
    "print('Frequency of tokens of the sample sentence:')\n",
    "\n",
    "for word in test_sentence_tokens:\n",
    "    print(word,uni_freq[word])\n",
    "\n",
    "    \n",
    "# Creating bigrams\n",
    "\n",
    "bigram_words = []\n",
    "previous = 'EMPTY'\n",
    "sentences = 0\n",
    "for word in words:\n",
    "    if previous in ['EMPTY','.','?','!']:\n",
    "        ## insert word_boundaries at beginning of Brown,\n",
    "        bigram_words.append('*start_end*')\n",
    "    else:\n",
    "        bigram_words.append(word)\n",
    "    \n",
    "    previous = word\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "bigram_words.append('*start_end*') ## assume one additional *start_end* at the end of Brown\n",
    "\n",
    "updated_uni_freq  = nltk.FreqDist(w.lower() for w in bigram_words)\n",
    "\n",
    "\n",
    "print('\\nCalculating bigram counts for sentence, including bigrams with sentence boundaries, i.e., *BEGIN* and *END*')\n",
    "\n",
    "\n",
    "# Bigram corpus\n",
    "bigrams = nltk.bigrams(w.lower() for w in bigram_words)\n",
    "\n",
    "\n",
    "# Bigram probabilities\n",
    "conditional_freq = nltk.ConditionalFreqDist(bigrams)\n",
    "\n",
    "#Sentence \n",
    "test_sentence_tokens=['sunset','looks','magnificient','.']\n",
    "\n",
    "# Code begins here\n",
    "\n",
    "\n",
    "\n",
    "V=len(set(words))\n",
    "\n",
    "\n",
    "# Function to calculate bigram probability\n",
    "def get_bigram_probability(first,second):\n",
    "    \n",
    "    bigram_freq = conditional_freq[first][second]\n",
    "    unigram_freq = updated_uni_freq[first]\n",
    "\n",
    "    bigram_prob = (bigram_freq + 1)/(unigram_freq + V)\n",
    "    \n",
    "    return bigram_prob\n",
    "\n",
    "# Calculating the bigram probability\n",
    "\n",
    "prob_list=[]\n",
    "previous = '*start_end*'\n",
    "for token in test_sentence_tokens:\n",
    "    next_probability = get_bigram_probability(previous,token)\n",
    "    print(previous,token,(float('%.3g' % next_probability)))\n",
    "    previous = token\n",
    "    prob_list.append(next_probability)\n",
    "\n",
    "    \n",
    "# For the final term    \n",
    "next_probability = get_bigram_probability(previous,'*start_end*')\n",
    "print(previous,'*start_end*',next_probability)\n",
    "prob_list.append(next_probability)    \n",
    "\n",
    "print(prob_list)    \n",
    "\n",
    "\n",
    "\n",
    "# Calculating the total probability\n",
    "\n",
    "total_prob = 1\n",
    "for val in prob_list:\n",
    "    total_prob *= val\n",
    "\n",
    "print(\"\\nTotal probability:\",total_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hints\n",
    "\n",
    "Inside the function, `bigram_prob` has to be updated in the following way:\n",
    "\n",
    "```python\n",
    "    bigram_prob = (bigram_freq + 1)/(unigram_freq + V)\n",
    "```\n",
    "\n",
    "# Test Cases\n",
    "\n",
    "#prob_list\n",
    "Variable declaration\n",
    "round(prob_list[4],2)==0.5\n",
    "\n",
    "\n",
    "#total_prob\n",
    "Variable declaration\n",
    "round(total_prob,20)==4e-20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Success Message\n",
    "\n",
    "Congrats! You have successfully applied Laplace Smoothing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Other methods of smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw the inefficiency of Add k smoothing.\n",
    "\n",
    "This is because different n-grams have different problems. \n",
    "\n",
    "\n",
    "The unigram estimate will never have the problem of its numerator or denominator having value equal to 0. \n",
    "However, the unigram ignores the context (previous n words), and hence discards valuable information. \n",
    "\n",
    "In contrast, the n-gram models where n>=2 estimate do make use of context but has the sparsity problem. As n increases, the power of n-gram model increases but the smoothing problem too gets worse.\n",
    "\n",
    "Instead of relying on a single model, what if we tried to solve this problem using the strength of different models?\n",
    "\n",
    "**Interpolation(Jelinek-Mercer smoothing)**\n",
    "\n",
    "The idea in linear interpolation is to use all the available models in a linear combination.\n",
    "\n",
    "For eg: For estimating the trigram probability $P(w_n |w_{n−2}w_{n−1})$ we will mix(interpolate) together the unigram, bigram, and trigram probabilities, each weighted by a weight λ :\n",
    "\n",
    "\\begin{align} \\hat{P}(w_n | w_{n−2}w_{n−1}) &= \\lambda_1 P(w_n | w_{n−2} w_{n−1}) \\\\\n",
    "&+ \\lambda_2 P(w_n | w_{n−1}) \\\\\n",
    "&+ \\lambda_3 P(w_n) \\end{align}\n",
    "\n",
    "such that the λ's sum to 1:\n",
    "$$\\sum_{i}^{} \\lambda_i = 1$$\n",
    "\n",
    "\n",
    "How do we calculate $\\lambda$ values set? \n",
    "\n",
    "There are multiple ways to do that:\n",
    "\n",
    "*1. Calculation using counts*\n",
    "\n",
    "If we have a high count of trigrams then we give them relatively higher weight otherwise more weight is put on the unigram and bigram models.\n",
    "\n",
    "\n",
    "*2. Calculation using held out corpus*\n",
    "\n",
    "A held-out corpus is an additional training corpus that we use to set hyperparameters like the $\\lambda$ values, by choosing the $\\lambda$ values that maximize the likelihood of the held-out corpus. \n",
    "\n",
    "In this method we fix the n-gram model and then search for the $\\lambda$ values that when plugged into the equation will give us the highest probability in the held-out set. So if we have particularly accurate counts for  unigram, we assume that the counts  based on this unigram will be more trustworthy, so we can make the λ s for that unigram higher and thus give that unigram more weight in the final interpolation. \n",
    "\n",
    "\n",
    "\n",
    "Consider the same corpus example we encountered before:\n",
    "\n",
    "$<s>$ You are a data scientist $</s>$\n",
    "\n",
    "$<s>$ Data scientist you are $</s>$\n",
    "\n",
    "$<s>$ You love statistics $</s>$\n",
    "\n",
    "\n",
    "Here $<s>$ and $</s>$ denote the start and end of the sentence respectively.\n",
    "\n",
    "Let's assume $\\lambda_1 = \\lambda_1= \\frac{1}{2} $\n",
    "\n",
    "If we wanted to calculate the probability of the bigram 'you love', we get:\n",
    "\n",
    "\\begin{align} \\hat{P}(\\text{you love})&=\\hat{P}(w_n | w_{n−1}) = \\lambda_1 P(w_n | w_{n−1}) + \\lambda_2 P(w_n) = \\frac{1}{2}.\\frac{1}{3} + \\frac{1}{2}.1 =\\frac{1}{6} + \\frac{1}{2} =\\frac{4}{6} \\end{align} \n",
    "\n",
    "\n",
    "**Backoff(Katz Smoothing)**\n",
    "\n",
    "This method is another way we can use multiple n-gram models to our advantage\n",
    "\n",
    "In this method, if the n-gram we are calculating has zero counts, we approximate it by backing off to the (N-1)-gram. \n",
    "\n",
    "We continue backing off until we reach a model that has some counts.\n",
    "\n",
    "\n",
    "So if we are trying to compute trigram probability $P(w_n |w_{n−2} w_{n−1})$ but we have no examples of a particular trigram $w_{n−2} w_{n−1} w_n$ , we \"backoff\" and estimate its probability by using the bigram probability $P(w_n |w_{n−1})$. \n",
    "\n",
    "Similarly, if we don’t have counts to compute for the bigram $P(w_n|w_{n−1})$, we go to the unigram $P(w_n)$[Which will never be 0].\n",
    "\n",
    "\n",
    "For eg:\n",
    "\n",
    "We are using 6-grams to calculate the probability of a word in text. You have \"<i>this is a very rainy</i>\" followed by \"night\". Let's suppose \"<i>night</i>\" never ocurred in this context in our corpus \"<i>this is a very rainy</i>\" so for the 5-grams model \"<i>night</i>\" has 0 probability which is not good because we know \"<i>night</i>\" is more probable than something like \"<i>peacock</i>\".\n",
    "\n",
    "In other words, $P(\\text{night|this is a very rainy})=0$ \n",
    "\n",
    "To resolve that, we will use a 5-gram model or a 4-gram model to calculate the probability of the sentence.\n",
    "\n",
    "We see that if we use 4-gram model i.e \"<i>night</i>\" in the context \"<i>a very rainy</i>\", we are able to get a non-zero probability. We will hence use $P(\\text{night|a very rainy})$ instead of the 6-gram prob\n",
    "\n",
    "This method works because sometimes using less context is more benificial(like we saw in our rainy night example), helping to tackle for contexts that the model hasn’t learned much about. \n",
    "\n",
    "\n",
    "**Note:** For a backoff model to give the correct probability distribution, we have to `discount` the higher-order n-grams to save some probability mass for the lower order n-grams. Similar to how we changed the denominator with add-one smoothing. If higher-order n-grams aren’t discounted, the total probability assigned to all possible strings by the language model would be greater than 1! \n",
    "\n",
    "\n",
    "Studies shows that the two most widely used techniques are Interpolation and Backoff. Both of which perform consistently well across training set sizes for both bigram and trigram models, with Backoff technique performing better on trigram models in large training sets and on bigram models in general.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "\n",
    "- The working code for the first task you completed is given with a new sentence `this is a very sunny day.`\n",
    "\n",
    "- Run the code once as it is.\n",
    "\n",
    "**We get the final probability value as 0 because the bigram 'very sunny' is not in our corpus.**\n",
    "\n",
    "**Let's try to resolve that using Backoff**\n",
    "\n",
    "Inside the function `get_bigram_probability()` we  need to implement a condition such that if bigram probability is 0, it should return the unigram probability of the `second` term. \n",
    "\n",
    "- Inside the function `get_bigram_probability()`:\n",
    "        \n",
    "        - Just above the bigram probability calculation, put up an if condition `\"if not second in conditional_freq[first]:\"` to check if that particular bigram exists\n",
    "        \n",
    "        - Inside the if condition calculate the unigram probability `\"unigram_prob\"` by dividing `\"updated_uni_freq[second]\"`(Unigram frequencies are stored in the dictionary `\"updated_uni_freq\"`) by `\"len(words)\"`. Return the variable `\"unigram_prob\"`\n",
    "       \n",
    "(**Note:** Don't remove the previous code from `get_bigram_probability()`, you just need to add an extra if condition in the beginning of the function )\n",
    "        \n",
    "\n",
    "**Things to ponder upon**\n",
    "\n",
    "- Try to calculate the sentence probability by using Laplace Smoothing instead of Backoff method. Do we get different results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of tokens of the sample sentence:\n",
      "this 5145\n",
      "is 10109\n",
      "a 23195\n",
      "very 796\n",
      "sunny 13\n",
      "day 687\n",
      ". 49346\n",
      "\n",
      "Calculating bigram counts for sentence, including bigrams with sentence boundaries, i.e., *BEGIN* and *END*\n",
      "*start_end* this 0.0083\n",
      "this is 0.0503\n",
      "is a 0.0861\n",
      "a very 0.00613\n",
      "Backing Off to Unigram Probability for sunny\n",
      "very sunny 1.12e-05\n",
      "sunny day 0.154\n",
      "day . 0.163\n",
      ". *start_end* 1.0\n",
      "[0.008303975842979365, 0.05030826140567201, 0.08609535184632229, 0.00613137369821018, 1.1195392320994288e-05, 0.15384615384615385, 0.16251830161054173, 1.0]\n",
      "\n",
      "Total probability: 6.172926606098926e-14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "#Sentence \n",
    "test_sentence_tokens=['this','is','a','very','sunny','day','.']\n",
    "\n",
    "\n",
    "# Corpus\n",
    "words = brown.words()\n",
    "words=[w.lower() for w in words]\n",
    "\n",
    "# Unigram frequency \n",
    "uni_freq = nltk.FreqDist(w.lower() for w in words)\n",
    "\n",
    "# Size of corpus\n",
    "total_words = len(words)\n",
    "\n",
    "print('Frequency of tokens of the sample sentence:')\n",
    "\n",
    "for word in test_sentence_tokens:\n",
    "    print(word,uni_freq[word])\n",
    "\n",
    "    \n",
    "# Creating bigrams\n",
    "\n",
    "bigram_words = []\n",
    "previous = 'EMPTY'\n",
    "sentences = 0\n",
    "for word in words:\n",
    "    if previous in ['EMPTY','.','?','!']:\n",
    "        ## insert word_boundaries at beginning of Brown,\n",
    "        bigram_words.append('*start_end*')\n",
    "    else:\n",
    "        bigram_words.append(word)\n",
    "    \n",
    "    previous = word\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "bigram_words.append('*start_end*') ## assume one additional *start_end* at the end of Brown\n",
    "\n",
    "updated_uni_freq  = nltk.FreqDist(w.lower() for w in bigram_words)\n",
    "\n",
    "\n",
    "print('\\nCalculating bigram counts for sentence, including bigrams with sentence boundaries, i.e., *BEGIN* and *END*')\n",
    "\n",
    "\n",
    "# Bigram corpus\n",
    "bigrams = nltk.bigrams(w.lower() for w in bigram_words)\n",
    "\n",
    "\n",
    "# Bigram probabilities\n",
    "conditional_freq = nltk.ConditionalFreqDist(bigrams)\n",
    "\n",
    "\n",
    "# Code begins here\n",
    "\n",
    "\n",
    "V=len(set(words))\n",
    "\n",
    "\n",
    "# Function to calculate bigram probability\n",
    "def get_bigram_probability(first,second):\n",
    "\n",
    "    if not second in conditional_freq[first]:\n",
    "        print('Backing Off to Unigram Probability for',second)\n",
    "        unigram_prob = updated_uni_freq[second]/len(words)\n",
    "        return unigram_prob \n",
    "    \n",
    "\n",
    "    bigram_freq = conditional_freq[first][second]\n",
    "    unigram_freq = updated_uni_freq[first]\n",
    "    bigram_prob = bigram_freq/unigram_freq\n",
    "    \n",
    "    return bigram_prob\n",
    "\n",
    "\n",
    "# Calculating the bigram probability\n",
    "\n",
    "prob_list=[]\n",
    "previous = '*start_end*'\n",
    "for token in test_sentence_tokens:\n",
    "    next_probability = get_bigram_probability(previous,token)\n",
    "    print(previous,token,(float('%.3g' % next_probability)))\n",
    "    previous = token\n",
    "    prob_list.append(next_probability)\n",
    "\n",
    "    \n",
    "# For the final term    \n",
    "next_probability = get_bigram_probability(previous,'*start_end*')\n",
    "print(previous,'*start_end*',next_probability)\n",
    "prob_list.append(next_probability)    \n",
    "\n",
    "print(prob_list)    \n",
    "\n",
    "\n",
    "\n",
    "# Calculating the total probability\n",
    "\n",
    "total_prob = 1\n",
    "for val in prob_list:\n",
    "    total_prob *= val\n",
    "\n",
    "print(\"\\nTotal probability:\",total_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hints\n",
    "\n",
    "You can write the function `get_bigram_probability()` in the following way\n",
    "\n",
    "```python\n",
    "\n",
    "def get_bigram_probability(first,second):\n",
    "\n",
    "    if not second in cfd[first]:\n",
    "        print('Backing Off to Unigram Probability for',second)\n",
    "        unigram_prob = updated_uni_freq[second]/len(words)\n",
    "        return unigram_prob \n",
    "    \n",
    "    else:\n",
    "        bigram_freq = conditional_freq[first][second]\n",
    "        unigram_freq = updated_uni_freq[first]\n",
    "        bigram_prob = bigram_freq/unigram_freq\n",
    "    \n",
    "    return bigram_prob\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "# Test Cases\n",
    "\n",
    "#prob_list\n",
    "Variable declaration\n",
    "round(prob_list[0],3)==0.008\n",
    "round(prob_list[1],3)==0.05\n",
    "\n",
    "\n",
    "#total_prob\n",
    "Variable declaration\n",
    "round(total_prob,14)==6e-14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Success Message\n",
    "\n",
    "Congrats! You have successfully implemented backoff smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF NOTEBOOK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Spell Check Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You often would have observed when you google anything on web and make a spelling mistake, google corrects the spelling mistake for you and perform the search on the corrected word afterwards. The full details of an industrial-strength spell corrector are quite complex, though we can still make a toy spelling corrector that achieves 80 or 90% accuracy at a processing speed of at least 10 words per second in about half a page of code.\n",
    "\n",
    "<img src=\"lm17.png\" width=\"700\">\n",
    "\n",
    "**Aim**\n",
    "\n",
    "Here our aim is to construct a function *correction()*, such that *correction(speling)* would output *spelling*. \n",
    "\n",
    "We will achieve this task in few steps described below.\n",
    "\n",
    "Your first step is to understand how to make a list of candidates words which may be the correct word for any misspelled word.\n",
    "\n",
    "From now on we refer to the word w which we have to get correct spelling for as query word. We solve the above problem in three steps.\n",
    "\n",
    "First we collect all the words which can be formed from query word using a simple edit. A simple edit to a word is a deletion (remove one letter), a transposition (swap two adjacent letters), a replacement (change one letter to another) or an insertion (add a letter). You have been provided with a function edits1 which outputs list of all possible words formed with only one simple edit. You have to implement using this given function a function which output lists of all the strings (whether words or not) that can be made with one simple edit. Note that most of the strings made using one simple edit would not be a word.\n",
    "\n",
    "One important thing to note here is that our spell checker should return the word itself if it is in vocabulary. That is if that word is in vocabulary so that is the correct word, similarly if you get a word with one simple edit which is in vocabulary then you don't need to look further for words with 2 simple edits and look at their probability. This is an assumption we have made here that any word with one simple edit is always better substitute for a mis-spelled word than a number with 2 simple edits and similar is the case in words with 2 and 3 edits. In the end if we can find no meaningful word using 1, 2 or 3 edits then we simply return the query word itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T05:47:51.745647Z",
     "start_time": "2018-12-28T05:47:51.742442Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T05:47:52.376130Z",
     "start_time": "2018-12-28T05:47:52.323936Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inside functions (utility)\n",
    "\n",
    "def edits1(word):\n",
    "    #All edits that are one edit away from `word`.\n",
    "    \n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "def known(words): \n",
    "    return list(w for w in words if w in WORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T05:47:53.097131Z",
     "start_time": "2018-12-28T05:47:53.070387Z"
    }
   },
   "outputs": [],
   "source": [
    "# PROBLEM CODE\n",
    "\n",
    "# Function to get list of all possible correct words\n",
    "# from the given word\n",
    "\n",
    "def edits(word):\n",
    "    # word : Contain the query word\n",
    "    # return two lists, words_1edit and words_2edit\n",
    "    # which are list of all words with one edit \n",
    "    # and 2 edits respectively.\n",
    "    \n",
    "    # Initialization\n",
    "    words_1edit = []\n",
    "    words_2edit = []\n",
    "    \n",
    "    # Write code here\n",
    "    #####################\n",
    "    \n",
    "    # In line 1 \n",
    "    \n",
    "    words_1edit = None\n",
    "    \n",
    "    for word in None:\n",
    "        for word2 in None:\n",
    "            # Append word2 into list words_2edit below\n",
    "            None\n",
    "    \n",
    "    #####################\n",
    "    # End code here\n",
    "    \n",
    "    return [words_1edit, words_2edit]\n",
    "\n",
    "\n",
    "# A function to find probability of a list of given words\n",
    "# Given below N is the total number of words in corpus\n",
    "def P(word, N=sum(WORDS.values())):\n",
    "    \n",
    "    # Initialize prob_list which stores probability of \n",
    "    # corresponding word in the word list\n",
    "    prob_list = []\n",
    "    \n",
    "    # Write code here\n",
    "    #####################\n",
    "    \n",
    "    # 1. In first line iterate over \"word\" list\n",
    "    # 2. In 2nd line find probability of each \n",
    "    # word by taking ratio of count of that word\n",
    "    # and total words in corpus (N)\n",
    "    # Hints: WORDS['apple'] give count of word \"apple\" in corpus \n",
    "    # 3. In 3rd line append probability of each word into \n",
    "    # list prob_list which contain probability of all words.\n",
    "    \n",
    "    for w in None:\n",
    "        p_w = None\n",
    "        None\n",
    "        \n",
    "    #####################\n",
    "    # End code here\n",
    "        \n",
    "    return prob_list\n",
    "\n",
    "\n",
    "\n",
    "def correction(word):\n",
    "    \n",
    "    # Write code here\n",
    "    #####################\n",
    "    \n",
    "    # Follow given steps below.\n",
    "    # 1. First get lists of all possible words you can\n",
    "    #  get by making one simple edit and two simple edits\n",
    "    #  in the query word and store it in words_1edit and \n",
    "    #  words_2edit respectively. Use the 'edits()' function we \n",
    "    #  implemented earlier.\n",
    "    # 2. From lists found in above step you have to slect only\n",
    "    #  correct words, use 'known()' function which takes any \n",
    "    #  list of words and return only known words from vocabulary.\n",
    "    #  From list 'words_1edit' and 'words_1edit' select only \n",
    "    #  known words using 'known()' function and save it in lists\n",
    "    #  'known_words_1edit' and 'known_words_2edit' respectively.\n",
    "    #  Observe that 'known_word' variable is a list which will \n",
    "    #  contain the query word if it is known in vocabulary and\n",
    "    #  would be empty otherwise\n",
    "    # 3. Using if-else condition you have to implement three possible\n",
    "    #  conditions\n",
    "    #  (a) return query word itself if it is in vocabulary,\n",
    "    #      so check if length of 'known_word' list is 0, \n",
    "    #      if not zero it means word is in vocabulary and so\n",
    "    #      return it. If it is zero then move to next condition.\n",
    "    #  (b) if list 'known_words_1edit' is non-empty then\n",
    "    #      you don't need to look further return word with maximum\n",
    "    #      probability in this list\n",
    "    #      in 1st line get probability of all the words in this list\n",
    "    #      in 2nd line get the index with maximum value of probability\n",
    "    #      in 3rd line return word with maximum probability\n",
    "    #  (c) perform same operation with 'known_words_2edit' and\n",
    "    #      'known_words_3edit'.\n",
    "    #  (d) return query word if none of these last four cases\n",
    "    #      give a known word\n",
    "    \n",
    "    words_1edit, words_2edit = None\n",
    "    \n",
    "    known_word = known([word])\n",
    "    known_words_1edit = None\n",
    "    known_words_2edit = None\n",
    "    \n",
    "    if len(known_word) != None:\n",
    "        return None\n",
    "    \n",
    "    elif len(None) != 0:\n",
    "        probability_list = None\n",
    "        max_index = None\n",
    "        return None\n",
    "    \n",
    "    elif len(None) != 0:\n",
    "        probability_list = None\n",
    "        max_index = None\n",
    "        return None\n",
    "    \n",
    "    elif len(None) != 0:\n",
    "        probability_list = None\n",
    "        max_index = None\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "    #####################\n",
    "    # End code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T05:47:53.881093Z",
     "start_time": "2018-12-28T05:47:53.855730Z"
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION CODE\n",
    "\n",
    "def edits(word):\n",
    "    # All edits that are two edits away from `word`.\n",
    "    \n",
    "    words_1edit = []\n",
    "    words_2edit = []\n",
    "    words_3edit = []\n",
    "    \n",
    "    words_1edit = edits1(word)\n",
    "    \n",
    "    for word in words_1edit:\n",
    "        for word2 in edits1(word):\n",
    "            words_2edit.append(word2)\n",
    "            \n",
    "    for word in words_2edit:\n",
    "        for word2 in edits1(word):\n",
    "            words_3edit.append(word2)\n",
    "\n",
    "    return [words_1edit, words_2edit, words_3edit]\n",
    "\n",
    "def P(word, N=sum(WORDS.values())):\n",
    "    prob_list = []\n",
    "    for w in word:\n",
    "        p_w = WORDS[w] / N\n",
    "        prob_list.append(p_w)\n",
    "\n",
    "    return prob_list\n",
    "\n",
    "\n",
    "\n",
    "def correction(word):\n",
    "    words_1edit, words_2edit, words_3edit = edits(word)\n",
    "    \n",
    "    known_word = known([word])\n",
    "    known_words_1edit = known(words_1edit)\n",
    "    known_words_2edit = known(words_2edit)\n",
    "    known_words_3edit = known(words_2edit)\n",
    "                       \n",
    "        \n",
    "    if len(known_word) != 0:\n",
    "        return word\n",
    "    \n",
    "    elif len(known_words_1edit) != 0:\n",
    "        probability_list = P(known_words_1edit)\n",
    "        max_index = np.argmax(probability_list)\n",
    "        return known_words_1edit[max_index]\n",
    "    \n",
    "    elif len(known_words_2edit) != 0:\n",
    "        probability_list = P(known_words_2edit)\n",
    "        max_index = np.argmax(probability_list)\n",
    "        return known_words_2edit[max_index]\n",
    "    \n",
    "    elif len(known_words_3edit) != 0:\n",
    "        probability_list = P(known_words_3edit)\n",
    "        max_index = np.argmax(probability_list)\n",
    "        return known_words_3edit[max_index]\n",
    "    \n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T05:47:59.950455Z",
     "start_time": "2018-12-28T05:47:54.520647Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'able'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction('aple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T05:34:35.540922Z",
     "start_time": "2018-12-28T05:34:35.500583Z"
    }
   },
   "outputs": [],
   "source": [
    "# INITIAL CODE OF SPELL CHECK\n",
    "# LINK:- https://github.com/norvig/pytudes/blob/master/py/spell.py\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS\n",
    "\n",
    "def unit_tests():\n",
    "    assert correction('speling') == 'spelling'              # insert\n",
    "    assert correction('korrectud') == 'corrected'           # replace 2\n",
    "    assert correction('bycycle') == 'bicycle'               # replace\n",
    "    assert correction('inconvient') == 'inconvenient'       # insert 2\n",
    "    assert correction('arrainged') == 'arranged'            # delete\n",
    "    assert correction('peotry') =='poetry'                  # transpose\n",
    "    assert correction('peotryy') =='poetry'                 # transpose + delete\n",
    "    assert correction('word') == 'word'                     # known\n",
    "    assert correction('quintessential') == 'quintessential' # unknown\n",
    "    assert words('This is a TEST.') == ['this', 'is', 'a', 'test']\n",
    "    assert Counter(words('This is a test. 123; A TEST this is.')) == (\n",
    "           Counter({'123': 1, 'a': 2, 'is': 2, 'test': 2, 'this': 2}))\n",
    "    assert len(WORDS) == 32198\n",
    "    assert sum(WORDS.values()) == 1115585\n",
    "    assert WORDS.most_common(10) == [\n",
    "     ('the', 79809),\n",
    "     ('of', 40024),\n",
    "     ('and', 38312),\n",
    "     ('to', 28765),\n",
    "     ('in', 22023),\n",
    "     ('a', 21124),\n",
    "     ('that', 12512),\n",
    "     ('he', 12401),\n",
    "     ('was', 11410),\n",
    "     ('it', 10681)]\n",
    "    assert WORDS['the'] == 79809\n",
    "    assert P('quintessential') == 0\n",
    "    assert 0.07 < P('the') < 0.08\n",
    "    return 'unit_tests pass'\n",
    "\n",
    "def spelltest(tests, verbose=False):\n",
    "    \"Run correction(wrong) on all (right, wrong) pairs; report results.\"\n",
    "    import time\n",
    "    start = time.clock()\n",
    "    good, unknown = 0, 0\n",
    "    n = len(tests)\n",
    "    for right, wrong in tests:\n",
    "        w = correction(wrong)\n",
    "        good += (w == right)\n",
    "        if w != right:\n",
    "            unknown += (right not in WORDS)\n",
    "            if verbose:\n",
    "                print('correction({}) => {} ({}); expected {} ({})'\n",
    "                      .format(wrong, w, WORDS[w], right, WORDS[right]))\n",
    "    dt = time.clock() - start\n",
    "    print('{:.0%} of {} correct ({:.0%} unknown) at {:.0f} words per second '\n",
    "          .format(good / n, n, unknown / n, n / dt))\n",
    "    \n",
    "def Testset(lines):\n",
    "    \"Parse 'right: wrong1 wrong2' lines into [('right', 'wrong1'), ('right', 'wrong2')] pairs.\"\n",
    "    return [(right, wrong)\n",
    "            for (right, wrongs) in (line.split(':') for line in lines)\n",
    "            for wrong in wrongs.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of tokens in sample sententence in Brown according to NLTK:\n",
      "this 5145\n",
      "is 10109\n",
      "a 23195\n",
      "very 796\n",
      "sunny 13\n",
      "day 687\n",
      ". 49346\n",
      "Given that there are 1161192 in the Brown Corpus, the unigram probability of these words\n",
      "is as follows (rounded to 3 significant digits):\n",
      "this 0.00443\n",
      "is 0.00871\n",
      "a 0.02\n",
      "very 0.000686\n",
      "sunny 1.12e-05\n",
      "day 0.000592\n",
      ". 0.0425\n",
      "There are 15673 instances of OOVs\n",
      "Unigram probabilities including OOV probabilities.\n",
      "this 0.00443\n",
      "is 0.00871\n",
      "a 0.02\n",
      "very 0.000686\n",
      "sunny 1.12e-05\n",
      "day 0.000592\n",
      ". 0.0425\n",
      "Calculating bigram counts for sentence, including bigrams with sentence boundaries, i.e., *BEGIN* and *END*\n",
      "Assuming some idealizations: all periods, questions and exclamation marks end sentences;\n",
      "*start_end* this 0.0196\n",
      "this is 0.0842\n",
      "is a 0.0858\n",
      "a very 0.00604\n",
      "Backing Off to Unigram Probability for sunny\n",
      "very sunny 1.12e-05\n",
      "sunny day 0.154\n",
      "day . 0.162\n",
      ". *start_end* 1.0\n",
      "Total Probability 2.38e-13\n"
     ]
    }
   ],
   "source": [
    "## This file assumes Python 3\n",
    "## To work with Python 2, you would need to adjust\n",
    "## at least: the print statements (remove parentheses)\n",
    "## and the instances of division (convert\n",
    "## arguments of / to floats), and possibly other things\n",
    "## -- I have not tested this.\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# test_sentence_tokens = ['a','fact','about','the','unicorn','is','the','same','as','an','alternative','fact','about','the','unicorn','.']\n",
    "\n",
    "test_sentence_tokens=['this','is','a','very','sunny','day','.']\n",
    "\n",
    "words = brown.words()\n",
    "fdist1 = nltk.FreqDist(w.lower() for w in words)\n",
    "\n",
    "total_words = len(words)\n",
    "\n",
    "print('Frequency of tokens in sample sententence in Brown according to NLTK:')\n",
    "\n",
    "for word in test_sentence_tokens:\n",
    "    print(word,fdist1[word])\n",
    "\n",
    "# input('Pausing: Hit Return when Ready.')\n",
    "\n",
    "print('Given that there are',total_words,'in the Brown Corpus, the unigram probability of these words')\n",
    "print('is as follows (rounded to 3 significant digits):')\n",
    "\n",
    "for word in test_sentence_tokens:\n",
    "    unigram_probability = fdist1[word]/total_words\n",
    "    print(word,float('%.3g' % unigram_probability))\n",
    "    ## print(word,round((fdist1[word]/total_words),3))\n",
    "\n",
    "# input('Pausing: Hit Return when Ready.')\n",
    "\n",
    "## ADD convert single count items to OOV\n",
    "## make simple assumption about sentence endings,\n",
    "## and the position of START and END (sentence boundaries)\n",
    "\n",
    "words2 = []\n",
    "previous = 'EMPTY'\n",
    "sentences = 0\n",
    "for word in words:\n",
    "    if previous in ['EMPTY','.','?','!']:\n",
    "        ## insert word_boundaries at beginning of Brown,\n",
    "        ## and after end-of-sentence markers (overgenerate due to abbreviations, etc.)\n",
    "        words2.append('*start_end*')\n",
    "    if fdist1[word]==1:\n",
    "        ## words occurring only once are treated as Out of Vocabulary Words\n",
    "        words2.append('*oov*')\n",
    "    else:\n",
    "        words2.append(word)\n",
    "    previous = word\n",
    "words2.append('*start_end*') ## assume one additional *start_end* at the end of Brown\n",
    "\n",
    "fdist2 = nltk.FreqDist(w.lower() for w in words2)\n",
    "## get Unigram counts for all words occuring more than once\n",
    "## and also a count for OOV words\n",
    "\n",
    "print('There are',fdist2['*oov*'],'instances of OOVs')\n",
    "\n",
    "print('Unigram probabilities including OOV probabilities.')\n",
    "\n",
    "def get_unigram_probability(word):\n",
    "    if word in fdist1:\n",
    "        unigram_probability = fdist2[word]/total_words\n",
    "    else:\n",
    "        unigram_probability = fdist2['*oov*']/total_words\n",
    "    return(unigram_probability)\n",
    "\n",
    "for word in test_sentence_tokens:\n",
    "    unigram_probability = get_unigram_probability(word)\n",
    "    print(word,float('%.3g' % unigram_probability))\n",
    "\n",
    "# input('Pausing: Hit Return when Ready.')\n",
    "## make new version that models Out of Vocabulary (OOV) words\n",
    "\n",
    "print('Calculating bigram counts for sentence, including bigrams with sentence boundaries, i.e., *BEGIN* and *END*')\n",
    "print('Assuming some idealizations: all periods, questions and exclamation marks end sentences;')\n",
    "\n",
    "bigrams = nltk.bigrams(w.lower() for w in words2)\n",
    "## get bigrams for words2 (words plus OOV)\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(bigrams)\n",
    "\n",
    "# for token1 in cfd:\n",
    "#     if not '*oov*' in cfd[token1]:\n",
    "#         cfd[token1]['*oov*']=1\n",
    "#         ## fudge so there can be no \n",
    "#         ## 0 bigram\n",
    "\n",
    "def multiply_list(inlist):\n",
    "    out = 1\n",
    "    for number in inlist:\n",
    "        out *= number\n",
    "    return(out)\n",
    "\n",
    "def get_bigram_probability(first,second):\n",
    "    if not second in cfd[first]:\n",
    "        print('Backing Off to Unigram Probability for',second)\n",
    "        unigram_probability = get_unigram_probability(second)\n",
    "        return(unigram_probability)\n",
    "    else:\n",
    "        bigram_frequency = cfd[first][second]\n",
    "    unigram_frequency = fdist2[first]\n",
    "    bigram_probability = bigram_frequency/unigram_frequency\n",
    "    return(bigram_probability)\n",
    "\n",
    "def calculate_bigram_freq_of_sentence_token_list(tokens):\n",
    "    prob_list = []\n",
    "    ## assume that 'START' precedes the first token\n",
    "    previous = '*start_end*'\n",
    "    for token in tokens:\n",
    "        if not token  in fdist2:\n",
    "            token = '*oov*'\n",
    "        next_probability = get_bigram_probability(previous,token)\n",
    "        print(previous,token,(float('%.3g' % next_probability)))\n",
    "        prob_list.append(next_probability)\n",
    "        previous = token\n",
    "    ## assume that 'END' follows the last token\n",
    "    next_probability = get_bigram_probability(previous,'*start_end*')\n",
    "    print(previous,'*start_end*',next_probability)\n",
    "    prob_list.append(next_probability)\n",
    "    probability = multiply_list(prob_list)\n",
    "    print('Total Probability',float('%.3g' % probability))\n",
    "    return(probability)\n",
    "\n",
    "\n",
    "\n",
    "result = calculate_bigram_freq_of_sentence_token_list(test_sentence_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add perplexity\n",
    "# import collections, nltk\n",
    "# # we first tokenize the text corpus\n",
    "# corpus =\"\"\"\n",
    "# Monty Python (sometimes known as The Pythons) were a British surreal comedy group who created the sketch comedy show Monty Python's Flying Circus,\n",
    "# that first aired on the BBC on October 5, 1969. Forty-five episodes were made over four series. The Python phenomenon developed from the television series\n",
    "# into something larger in scope and impact, spawning touring stage shows, films, numerous albums, several books, and a stage musical.\n",
    "# The group's influence on comedy has been compared to The Beatles' influence on music.\"\"\"\n",
    "\n",
    "# tokens = nltk.word_tokenize(corpus)\n",
    "# #here you construct the unigram language model \n",
    "# def unigram(tokens):    \n",
    "#     model = collections.defaultdict(lambda: 0.01)\n",
    "#     for f in tokens:\n",
    "#         try:\n",
    "#             model[f] += 1\n",
    "#         except KeyError:\n",
    "#             model [f] = 1\n",
    "#             continue\n",
    "#     for word in model:\n",
    "#         model[word] = model[word]/float(sum(model.values()))\n",
    "#     return model\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#computes perplexity of the unigram model on a testset  \n",
    "# def perplexity(testset):\n",
    "#     testset = testset.split()\n",
    "#     perplexity = 1\n",
    "#     N = 0\n",
    "#     previous='*start_end*'\n",
    "#     for word in testset:\n",
    "#         N += 1\n",
    "#         next_probability = get_bigram_probability(previous,word)\n",
    "#         previous = word\n",
    "#         print(word, next_probability)\n",
    "#         perplexity = perplexity * (1/next_probability)\n",
    "    \n",
    "#     perplexity = pow(perplexity, 1/float(N)) \n",
    "#     return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Neural Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Recently, the use of neural networks in the development of language models has become very popular, to the point that it may now be the preferred approach. The use of neural networks in language modeling is often called Neural Language Modeling, or NLM for short. Neural network approaches are achieving better results than classical methods both on standalone language models and when models are incorporated into larger models on challenging tasks like speech recognition and machine translation. A key reason for the leaps in improved performance may be the method’s ability to generalize.\n",
    "\n",
    "Specifically, a word embedding is adopted that uses a real-valued vector to represent each word in a project vector space. This learned representation of words based on their usage allows words with a similar meaning to have a similar representation. This generalization is something that the representation used in classical statistical language models can not easily achieve. \n",
    "\n",
    "The neural network approach to language modeling can be described using the three following model properties:\n",
    "\n",
    "* Associate each word in the vocabulary with a distributed word feature vector. That is each word is represented using a feature vector.\n",
    "* Express the joint probability function of word sequences in terms of the feature vectors of these words in the sequence.\n",
    "* This feature vector of words which we talked abobve and parameters of the probability function are learnt simultaneously.\n",
    "\n",
    "Summary:\n",
    "\n",
    "* Language models offer a way to assign a probability to a sentence or other sequence of words, and to predict a word from preceding words.\n",
    "* n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing\n",
    "* n-gram language models are evaluated extrinsically in some task, or intrinsically using perplexity.\n",
    "* The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.\n",
    "* Smoothing algorithms provide a more sophisticated way to estimate the probability of n-grams. Commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through backoff or interpolation.\n",
    "* In neural models probability is given by a neural network model.\n",
    "* We get feature vector representation of words instead of sparse representations which are later used in the model to get probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk.lm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-f2ef1e36ca88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# f_in = open(\"science.txt\", 'r');\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# ln = f_in.read()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk.lm'"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk.lm\n",
    "# f_in = open(\"science.txt\", 'r');\n",
    "# ln = f_in.read()    \n",
    "\n",
    "# words = nltk.word_tokenize(ln)\n",
    "my_bigrams = nltk.bigrams(words)\n",
    "my_trigrams = nltk.trigrams(words)\n",
    "\n",
    "s=\"\"\n",
    "tText = Text(words)\n",
    "tText1 = Text(my_bigrams)\n",
    "tText2 = Text(my_trigrams)\n",
    "estimator = lambda fdist, bins: LidstoneProbDist(fdist, 0.2)\n",
    "\n",
    "\n",
    "tt=NgramModel(1, tText, estimator)\n",
    "tt1=NgramModel(2, tText1, estimator)\n",
    "tt2=NgramModel(3, tText2, estimator)\n",
    "\n",
    "\n",
    "print (tt.perplexity(tText))\n",
    "print (tt1.perplexity(tText1))\n",
    "print (tt2.perplexity(tText2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
