{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Learn what feature selection is, its different methods and how it helps improve the working of ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "- Definition\n",
    "\n",
    "- Examples\n",
    "\n",
    "- Wrapper Methods\n",
    "\n",
    "- Filter Methods\n",
    "\n",
    "- Embedded Methods\n",
    " \n",
    "- PCA\n",
    "\n",
    "- Feature Selection Checklist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "- Basic of Pandas Dataframe\n",
    "- Basic of Descriptive and Inferential Statistics\n",
    "- Linear Regression and regularisation\n",
    "- Basic of Data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Importance of feature selection\n",
    "- Where to use feature selection?\n",
    "- Implementation of Filter,wrapper,Embedded Method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1 : Introduction to Feature Selection\n",
    "\n",
    "In this chapter you will learn about what is Feature selection ,what are the various techniques of feature selection and intuition building on dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 1.1 What is Feature Selection?\n",
    "**Problem Statement**\n",
    "\n",
    "Let’s continue solving the same problem we encountered in the regression modules.\n",
    "\n",
    "We have with us the complete [`Iowa housing dataset`](https://www.kaggle.com/c/house-prices-advanced-regression-techniques).\n",
    "\n",
    "Each row in the dataset describes the properties of a single house as well as the amount it was sold for.The data set contains 81 features and 1300 data points.\n",
    "\n",
    "Let's apply simple Linear Regression model on it:\n",
    "\n",
    "```python\n",
    "#Data Loaded\n",
    "\n",
    "#Data Split\n",
    "\n",
    "model=LinearRegression()\n",
    "model.fit(X_train,y_train)\n",
    "score=model.score(X_test,y_test)\n",
    "print(score)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```python\n",
    "0.6665487890480106\n",
    "```\n",
    "\n",
    "We get `(r2_score)` as `0.66`.\n",
    "\n",
    "***\n",
    "Knowing this score to be low, we apply **one Feature Selection technique** and we get a `subset` of 30 features.\n",
    "***\n",
    "\n",
    "After subsetting the data to incorporate only those 30 features, we apply Linear Regression model \n",
    "```python\n",
    "#Data Loaded\n",
    "\n",
    "#Feature Selection method applied to dataset.\n",
    "\n",
    "#Data Split\n",
    "\n",
    "model=LinearRegression()\n",
    "model.fit(X_train,y_train)\n",
    "score=model.score(X_test,y_test)\n",
    "print(score)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```python\n",
    "0.768239\n",
    "```\n",
    "\n",
    "The `(r2_score)` is now `0.76` .\n",
    "\n",
    "That is a increase by **10%** despite removal of 50 features.\n",
    "\n",
    "Q: Doesn't that contradict the ML assumption that more features is equivalent to more information?\n",
    "\n",
    "Ans: All the features in a dataset might not be useful. In a dataset some features may contribute no information at all, while some features may contribute similar information as the other features. \n",
    "\n",
    "\n",
    "So selecting the important features is more important than having a high no. of features and that's what feature selection methods help us do.\n",
    "\n",
    "\n",
    "**Definition**\n",
    "\n",
    "Feature selection is the ML process of finding the subset of features that are most relevant for a `better` predictive model.\n",
    "\n",
    "When presented data with very high dimensionality(large no. of features), models usually choke because\n",
    "\n",
    "1. Less training time.\n",
    "\n",
    "2. Risk of overfitting.\n",
    "\n",
    "Feature selection methods can help identify as well as remove redundant and irrelevant attributes from data that do not contribute to the predictive power of the model.\n",
    "\n",
    "The objective of feature selection is three-fold: \n",
    "\n",
    "1. Improving the prediction performance of the predictors.\n",
    "\n",
    "2. Providing faster and more cost-effective predictors.\n",
    "\n",
    "3. Providing a better understanding of the underlying process that generated the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithms of Feature Selection**\n",
    "\n",
    "Following are the categories, the different Feature Selection attributes are broadly divided into:\n",
    "\n",
    "`Filter Method` - Filter Methods are used to find the relationship between features and the target variable. This results in computing the importance of features.\n",
    "\n",
    "\n",
    "![](../images/Filter_method.jpg)\n",
    "\n",
    "`Wrapper Method` - Wrapper Methods selects best subset of features by iteratively checking model performance.\n",
    "\n",
    "![Wrapper Method](../images/Wrapper.jpg)\n",
    "\n",
    "`Embedded Method` - Embedded methods are the methods implemented by algorithms that have a built in feature selection 'embedded' in them. It selects the best subset of features during the building of the model itself.\n",
    "\n",
    "\n",
    "![Embedded Method](../images/embedded_method.jpg)\n",
    "\n",
    "Let's dive into detail of each method in the following  chapters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2 : Filter Method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Correlation Coefficent##\n",
    "\n",
    "**Overview of filter method**\n",
    "\n",
    "\n",
    "![](../images/Filter_method.jpg)\n",
    "\n",
    "    • Filter methods are a set of powerful method of feature selection because selection happens independent of any machine learning algorithms.\n",
    "    \n",
    "    • Features are selected on the basis of scores of statistical tests between features & target variable.\n",
    " \n",
    " \n",
    "Following are some of the statistical tests:\n",
    "\n",
    "- Correlation coefficient\n",
    "\n",
    "- Chi Squared test\n",
    "\n",
    "- Anova(F-Score)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation Coefficent**\n",
    "\n",
    "It makes intuitive sense to choose features that are highly correlated to the target variable.\n",
    "The more correlated the features are to the target, easier it is for machine to predict it\n",
    "\n",
    "Selecting features having correlation coefficents above a certain threshold will result in the model performing better.\n",
    "\n",
    "Additionally one can also filter out redundant features by not selecting certain features that are already strongly correlated with other features. \n",
    "\n",
    "For eg: If x1 and x2 have strong correlation to the target variable but they are also strongly correlated to each other. In that case, a ML model including both x1 and x2 will give almost the  same results compared to the ML models where either only x1 or x2 was included in the dataset. \n",
    "\n",
    "There are different methods to calculate the correlation factor, however, Pearson’s correlation coefficient is most widely used.\n",
    "\n",
    "The Pearson coefficient is a measure of the strength of association between two continuous variables.\n",
    "\n",
    "It has already been covered in `Descriptive Statistics`. \n",
    "\n",
    "Still, let's refresh, \n",
    "\n",
    "Pearson's correlation coefficient is calculated by the formula:\n",
    "\n",
    "$$ corr = \\frac{cov \\ (x,y)}{\\sigma_x\\sigma_y}$$\n",
    "\n",
    "Where,\n",
    "- $cov(x, y)$ - covariance between x and y\n",
    "- $\\sigma_x$ - standard deviation of x\n",
    "- $\\sigma_y$ - standard deviation of y\n",
    "\n",
    "\n",
    "By taking x as our target variable and y as each of the features, we can easily find how much they are correlated to each other.\n",
    "\n",
    "Setting up a threshold after that(for eg: correlation coefficent >0.5 ), we can identify strongly related features and drop the others.\n",
    "\n",
    "\n",
    "Let's see how we can implement the same in python.\n",
    "\n",
    "```python\n",
    "#Sample Dataframe\n",
    "data=pd.DataFrame({'A':[1,2,3,4,5], 'B':[2,2,6,6,6], 'C':[4,4,6,10,10], 'D':[1,1,1,1,5]})\n",
    "print(\"Dataframe:\")\n",
    "print(data)\n",
    "\n",
    "#Finding the pearson's correlation among variables \n",
    "data_corr=data.corr()\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(data_corr)\n",
    "\n",
    "#Subseting and only taking those features which are strongly correlated to 'D'\n",
    "data_corr_d= data_corr[data_corr['D']>0.5]\n",
    "\n",
    "print(\"\\nFeatures closely related to D(Corr>0.5):\")\n",
    "print(data_corr_d.index.values)\n",
    "\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```python\n",
    "Dataframe:\n",
    "   A  B   C  D\n",
    "0  1  2   4  1\n",
    "1  2  2   4  1\n",
    "2  3  6   6  1\n",
    "3  4  6  10  1\n",
    "4  5  6  10  5\n",
    "\n",
    "Correlation Matrix:\n",
    "          A         B         C         D\n",
    "A  1.000000  0.866025  0.938315  0.707107\n",
    "B  0.866025  1.000000  0.842701  0.408248\n",
    "C  0.938315  0.842701  1.000000  0.589768\n",
    "D  0.707107  0.408248  0.589768  1.000000\n",
    "\n",
    "Features closely related to D(Corr>0.5):\n",
    "['A' 'C' 'D']\n",
    "\n",
    "```\n",
    "\n",
    "If you compare values of D with A and C you can clearly see correlation between them.\n",
    "\n",
    "A and D have two same values.\n",
    "\n",
    "C and D have similar pattern distribution for two values[1 maps to 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task : Feature selection using correlation\n",
    "\n",
    "In this task, after loading the housing dataset, we will filter out features based on pearson correlation and then train the model with only the selected features.\n",
    "\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "- Load the dataset from path using the `\"read_csv()\"` method from pandas and store it in a variable called `'ames'`\n",
    "\n",
    "- Store all the features of `'ames_model_data'` in  a variable called `X`\n",
    "\n",
    "\n",
    "- Store the target variable (`SalePrice`) of `'ames_model_data'` in a variable called `y`\n",
    "\n",
    "\n",
    "- Split `'X'` and `'y'` into `X_train,X_test,y_train,y_test` using `train_test_split()` function. Use `test_size = 0.3` and `random_state = 0`\n",
    "\n",
    "**Finding Correlation**\n",
    "\n",
    "- To simplify the process, create a new column in `'X_train'` called `Class` which stores the value of `'y_train'`.\n",
    "\n",
    "\n",
    "- Find the correlation dataframe among all the features using `\"X_train.corr()\"` and store it in a variable called `\"t_corr\"`. Since we are only interested in correlation of features with target variable, only extract the `SalePrice` column of `'t_corr'` and save it back to `'t_corr'` \n",
    "\n",
    "\n",
    "- From `'t_corr'`, extract only those columns whose `absolulte correlation score` is greater than 0.5 using `index` function and store them in a variable called `'corr_columns'`\n",
    "\n",
    "\n",
    "- Create a subset dataframe from `'X_train'` having only the columns stored in `'corr_columns'`. Save the new subsetted dataframe in `'X_train_new'`\n",
    "\n",
    "- Create a subset dataframe from `'X_test'` having only the columns stored in `'corr_columns'`. Save the new subsetted dataframe in `'X_test'`\n",
    "\n",
    "\n",
    "- Initialise a linear regression model with `LinearRegression()` and save it to a variable called `'model'`.\n",
    "\n",
    "\n",
    "- Fit the model on the training data `'X_train_new'` and `'y_train'` using the `'fit()'` method.\n",
    "\n",
    "\n",
    "- Find out the r^2 score between `X_test_new` and `'y_test'` using the `'score()'` method and save it in a variable called `'corr_score'`\n",
    "\n",
    "**Things to ponder**\n",
    "\n",
    "* Did the accuracy increase from the base score of `0.66`?\n",
    "* As a side task, see how many features were actually selected on the basis of pearson correlation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7227056628201058\n",
      "13\n",
      "True\n",
      "True\n",
      "214500\n",
      "85000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Code starts here\n",
    "\n",
    "#Loading of data\n",
    "path='../data/Cleaned_Data.csv'\n",
    "ames = pd.read_csv(path)\n",
    "\n",
    "\n",
    "X=ames.drop(['SalePrice'],1)\n",
    "y=ames['SalePrice'].copy()\n",
    "\n",
    "#Splitting of data\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0) \n",
    "\n",
    "#Creating temp. dataframe\n",
    "X_train['Class']=y_train\n",
    "t_corr=X_train.corr()\n",
    "t_corr=t_corr['Class']\n",
    "\n",
    "#Selecting columns having correlation higher than 0.5\n",
    "corr_columns=t_corr[abs(t_corr)>0.5].index\n",
    "\n",
    "#Dropping the column `Class`\n",
    "corr_columns=corr_columns.drop('Class')\n",
    "\n",
    "#Updating train and test dataframes\n",
    "X_train_new=X_train[corr_columns]\n",
    "\n",
    "X_test_new=X_test[corr_columns]\n",
    "\n",
    "#Initialising the model\n",
    "model=LinearRegression()\n",
    "\n",
    "#Fitting the model\n",
    "model.fit(X_train_new,y_train)\n",
    "\n",
    "#Finding the score of the model\n",
    "corr_score=model.score(X_test_new,y_test)\n",
    "print(corr_score)\n",
    "\n",
    "#Checking how many columns were selected\n",
    "print(len(X_train_new.columns))\n",
    "\n",
    "print(X_train.iloc[25][10]==4.0)\n",
    "print(X_test.iloc[25][5]==1.0)\n",
    "\n",
    "print(y_train.iloc[10]==214500)\n",
    "print(y_test.iloc[5]==85000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Chi Squared Test\n",
    "\n",
    "Another way to identify relationship between features and target variable is the chi-square test statistic.\n",
    "\n",
    "Similar to Pearson correlation, this is another preferred way to check for strongly correlated features.\n",
    "\n",
    "Chi Squared Test has been covered in `Inferential Statistics`. \n",
    "\n",
    "Still let's refresh, \n",
    "\n",
    "The chi-squared test of independence is used to detemine whether the two variables are inter-related to each other.\n",
    "\n",
    "Like any statistical hypothesis test, the Chi-square test has both a null hypothesis and an alternative hypothesis.\n",
    "\n",
    "The hypothesis for a chi-square test of independence are as follows.\n",
    "\n",
    "$H_0$: Variable A and Variable B are independent.\n",
    "\n",
    "$H_1$: Variable A and Variable B are not independent.\n",
    "\n",
    "In this case we will be calculating a chi-square test statistic as follows\n",
    "\n",
    "$$\\chi ^2 = \\sum \\frac{(observed-expected)^2}{expected}$$\n",
    "\n",
    "In the formula, observed is the actual observed count for each category and expected is the expected count based on the distribution of the population for the corresponding category.\n",
    "\n",
    "In the Chi-square context, the word “expected” is equivalent to what you’d expect if the null hypothesis is true. If your observed distribution is sufficiently different than the expected distribution (no relationship), you can reject the null hypothesis and infer that the variables are related.\n",
    "\n",
    "Since the chi-square test measures dependence between variables, using this function “weeds out” the features that are the most likely to be independent of the target variable class and therefore irrelevant for prediction.\n",
    "\n",
    "\n",
    "Q: In Inferential statistics, we learned chi-square tests is an independence test between two `categorical` variables. How are the numerical variables dealt with?\n",
    "\n",
    "A: Binning\n",
    "\n",
    "**Explanation:** Even for categorical variables, the null hypothesis is defined as \n",
    "\n",
    "``The `frequency distribution` of certain events observed in a sample is consistent with a particular theoretical distribution.``\n",
    "\n",
    "\n",
    "As an example look at these sets of variables:\n",
    "\n",
    "a = ['dog', 'cat', 'dog', 'cat']\n",
    "\n",
    "b = ['wild', 'trained', 'trained', 'trained']\n",
    "\n",
    "The categorical variables a and b can be compared by counting the co-occurences, and this is what happens with a chi-squared test:\n",
    "\n",
    "|- |Dog|Cat|\n",
    "|-----|-----|-----|\n",
    "|wild|1|0|\n",
    "|trained|1|2|\n",
    "\n",
    "However, you can also binarise the values of 'a' and get the following variables:\n",
    "\n",
    "a1 = [1, 0, 1, 0]\n",
    "\n",
    "a2 = [0, 1, 0, 1]\n",
    "\n",
    "b = ['wild', 'trained', 'trained', 'trained']\n",
    "\n",
    "Counting the values is now equal to summing the values that correspond to the value of b.\n",
    "\n",
    "|- |a1|a2|\n",
    "|-----|-----|-----|\n",
    "|wild|1|0|\n",
    "|trained|1|2|\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "For feature selection purposes, it has a different python implementation than the one you learned in Inferential Statistics.\n",
    "\n",
    "\n",
    "```python\n",
    "data=pd.DataFrame({'A':[1,2,3,4,5], 'B':[2,2,6,6,6], 'C':[4,4,4,2,10], 'D':[1,1,1,1,5], 'E': [2,2,2,1,5]})\n",
    "print('Dataframe:')\n",
    "print(data)\n",
    "\n",
    "#Using chi square score to calculate best two features\n",
    "test = SelectKBest(score_func=chi2, k=2)\n",
    "\n",
    "#Transforming the data based on chi square(Target variable is E)\n",
    "data_chi= test.fit_transform(data.iloc[:,:4], data.iloc[:,4]) \n",
    "\n",
    "#In the above function, 'data.iloc[:,:4]' are the features, 'data.iloc[:,4]' is the target variable         \n",
    "\n",
    "print(\"\\nTwo columns having the highest chi square score with respect to 'E'\")\n",
    "print(data_chi)\n",
    "\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```python\n",
    "Dataframe:\n",
    "   A  B   C  D  E\n",
    "0  1  2   4  1  2\n",
    "1  2  2   4  1  2\n",
    "2  3  6   4  1  2\n",
    "3  4  6   2  1  1\n",
    "4  5  6  10  5  5\n",
    "\n",
    "Two columns having the highest chi square score with respect to 'E'\n",
    "[[ 4  1]\n",
    " [ 4  1]\n",
    " [ 4  1]\n",
    " [ 2  1]\n",
    " [10  5]]\n",
    "```\n",
    "If you compare values of E with C and D you can clearly see why Chi-Square test score is highest for them.\n",
    "Both C and D have the most similar `frequency distribution` when compared to E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task : Chi Square test\n",
    "\n",
    "\n",
    "- Store all the features of `'ames'`(Loaded in the previous task) in  a variable called `X`\n",
    "\n",
    "\n",
    "- Store the target variable (`SalePrice`) of `'ames'` in a variable called `y`\n",
    "\n",
    "\n",
    "- Initialise a `\"SelectKBest()\"` with the parameters `score_func=chi2` & `k=60` and save it to a variable called `'test'`.\n",
    "\n",
    "\n",
    "- Split `'X'` and `'y'` into `X_train,X_test,y_train,y_test` using `train_test_split()` function. Use `test_size = 0.3` and `random_state = 0`\n",
    "\n",
    "\n",
    "\n",
    "- Fit `'test'` on the training data `'X_train'` and `'y_train'` using the `'fit_transform()'` method. Store the result back into `'X_train'`\n",
    "\n",
    "\n",
    "- Transform `'X_test'`using the `'transform()'` method of `test` .Store the result back into `'X_test'`\n",
    "\n",
    "\n",
    "- Initialise a linear regression model with `LinearRegression()` and save it to a variable called `'model'`.\n",
    "\n",
    "\n",
    "- Fit the model on the training data `'X_train'` and `'y_train'` using the `'fit()'` method.\n",
    "\n",
    "\n",
    "- Find out the r^2 score between `X_test` and `'y_test'` using the `'score()'` method and store it in a variable called `'chi2_score'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7526152480701574\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "X=ames.drop(['SalePrice'],1)\n",
    "y=ames['SalePrice'].copy()\n",
    "\n",
    "\n",
    "#Splitting dataframe into test and train\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0) \n",
    "\n",
    "#Initialising the score function\n",
    "test = SelectKBest(score_func=chi2, k=60)\n",
    "\n",
    "#Fitting and transforming the model on X_train\n",
    "X_train= test.fit_transform(X_train, y_train)\n",
    "\n",
    "#Fitting and transforming the model on X_test\n",
    "X_test= test.transform(X_test)\n",
    "\n",
    "#Initialising the Linear Regression model\n",
    "model=LinearRegression()\n",
    "\n",
    "#Fitting the model\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "#Finding the model score\n",
    "chi2_score=model.score(X_test,y_test)\n",
    "\n",
    "print(chi2_score)\n",
    "# Code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Anova\n",
    "\n",
    "Analysis of variance (ANOVA) is another method to check for close relationship between two variables.\n",
    "\n",
    "Just like any statistical test, it has two hypothesis\n",
    "\n",
    "$H_{0}:$ The mean(average value) is the same for all groups. (NULL Hypothesis)\n",
    "\n",
    "$H_{1}:$ The mean is not the same for all groups.\n",
    "\n",
    "\n",
    "The proportion of variance(or variability) explained by the two variables is calculated using `F-score` to figure out if they are closely related\n",
    "\n",
    "We won't be delving much into the actual Anova Tests and only focusing on its Feature Selection application.\n",
    "\n",
    "\n",
    "**Intuition**\n",
    "Consider a binary classification problem :\n",
    "\n",
    "~~~python\n",
    "\n",
    "x1  x2  x3  ... class\n",
    "0.3 0.5 0.1 ... A\n",
    "0.1 0.7 0.4 ... B\n",
    "0.1 0.1 0.2 ... A\n",
    "0.2 0.4 0.2 ... A\n",
    "0.5 0.7 0.8 ... B\n",
    "~~~\n",
    "\n",
    "Assume the plot for target variable with respect to x1 looks like:\n",
    "\n",
    "![](../images/x1_features.jpg)\n",
    "\n",
    "Assume the plot for target variable with respect to x2 looks like:\n",
    "\n",
    "![](../images/x2_feature.jpg)\n",
    "\n",
    "\n",
    "Q: If one has to predict say Class A based on x1 and x2, which feature do you think you can predict better?\n",
    "A: x1\n",
    "\n",
    "Reason: There is no overlap of target variable with respect to x1 data points.\n",
    "\n",
    "Though x1 and x2 are two extreme cases, it can be seen that the overlap is reduced when\n",
    "\n",
    "1. The means of A and B are more separated(with respect to a feature x_i) \n",
    "\n",
    "2. The variances of A and B are small(with respect to a feature x_i)\n",
    "\n",
    "\n",
    "The F-score captures these two properties, such that a high F-score reflects a small overlap.\n",
    "\n",
    "**Definition**\n",
    "\n",
    "F-score is defined as \n",
    "\n",
    "**variance between the samples(of a feature and target variable)**/ **the variance within the samples**\n",
    "\n",
    "Where **Variance within** is found by:\n",
    "\n",
    "$\\frac{\\sum_{i=1}^m \\sum_{j=1}^n (x_{j}- \\overline{x_{i}} )^2}{n-m}$ \n",
    "\n",
    "\n",
    "where **m** is the number of samples(groups) and **n** is the size of the m samples altogether(observations). \n",
    "\n",
    "\n",
    "And **Variance between** is found by:\n",
    "\n",
    "\n",
    "$\\frac{ \\sum_{j=1}^n  n_{j} (\\overline{x_{j}} - \\overline{X})^2 }{m-1}$\n",
    "($\\overline{x_{j}}$ is the mean of sample j, and $\\overline{X}$ is the grand mean)\n",
    "\n",
    "\n",
    "With respect to Feature Selection, Anova uses F-tests to statistically assess among all features which features are more likely to correctly predict the target variable.\n",
    "\n",
    "\n",
    "**Working**\n",
    "\n",
    "Let's try to understand Feature Selection using ANOVA with an example:\n",
    "***\n",
    "A large car company recently extended its working days to include Sunday from the previous schedule of Mon- Sat. The company now wants to optimise the sales on Sunday.\n",
    "\n",
    "Large amount of important sales happens during the days of Friday & Saturday and therfore the data science team wants to know between Friday and Saturday Sales, which day will help predict the sales on Sunday.\n",
    "\n",
    "The company collects data for the number of car sales each day for two months. \n",
    "\n",
    "Following is the data:\n",
    "\n",
    "Fridays: 288, 292, 310, 267, 243, 293, 255, 273\n",
    "\n",
    "Saturdays: 276, 323, 298, 256, 277, 309, 312, 265, 311\n",
    "\n",
    "Sundays: 243, 279, 301, 285, 274, 243, 228, 298, 255\n",
    "\n",
    "Let's find which of the groups explains greater variance of the data:\n",
    "\n",
    "***\n",
    "#### Group 1(Sat, Sun):\n",
    "\n",
    "\n",
    "\n",
    "$mean_{sat}$ = 291.8\n",
    "\n",
    "$mean_{sun}$ = 267.3\n",
    "\n",
    "mean of means = 279.55\n",
    "\n",
    "\n",
    "**Step 1:** Calculating the Sum of Squares within groups:\n",
    "\n",
    "[(276-291.8)^2+…+(311-291.8)^2+ (243-267.3)^2+…+(255-267.3)^2]=10002.4\n",
    "\n",
    "\n",
    "**Step 2:** Calculating degrees of freedom:\n",
    "\n",
    "Here, degrees of freedom= No. of observation - No. of groups= 18-2= 16\n",
    "\n",
    "\n",
    "**Step 3:** Computing variance within: \n",
    "\n",
    " Sum of squares within groups/degrees of freedom\n",
    "\n",
    "=[(276-291.8)^2+…+(311-291.8)^2+ (243-267.3)^2+…+(255-267.3)^2/[18-2]=625.185\n",
    "\n",
    "\n",
    "**Step 4:** Calculating the Sum of Squares between groups:\n",
    "\n",
    "[9(291.8-279.55)^2 + 9(267.3-279.55)^2]= 2701.125\n",
    "\n",
    "**Step 5:** Calculating degrees of freedom:\n",
    "\n",
    "Here, degrees of freedom= No. of groups- 1= 2-1= 1\n",
    "\n",
    "**Step 6:** Computing variance between:\n",
    "\n",
    "[9(291.8-279.55)^2+9(267.3-279.55)^2]/[2-1] = 2701.125\n",
    "\n",
    "**Step 7:** Computing the F-score: \n",
    "\n",
    "we will get,\n",
    "\n",
    "F-score=$\\frac {2701.125}{625.18}= 4.32$\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "#### Group 2(Fri, Sun):\n",
    "\n",
    "$mean_{fri}$ = 277.6\n",
    "\n",
    "$mean_{sun}$ = 267.3\n",
    "\n",
    "mean of means = 272.45\n",
    "\n",
    "\n",
    "**Step 1:** Calculating the Sum of Squares within groups:\n",
    "[(288-277.6)^2+…+(273-277.6)^2 + (243-267.3)^2+…+(255-267.3)^2] = 8893.8\n",
    "\n",
    "**Step 2:** Calculating degrees of freedom:\n",
    "\n",
    "Here, degrees of freedom= No. of observation - No. of groups=17 -2= 15\n",
    "\n",
    "\n",
    "**Step 3:** Computing variance within: \n",
    "\n",
    " Sum of squares within groups/degrees of freedom\n",
    "\n",
    "=[(288-277.6)^2+…+(273-277.6)^2 + (243-267.3)^2+…+(255-267.3)^2/[17-2]= 592.92\n",
    "\n",
    "\n",
    "**Step 4:** Calculating the Sum of Squares between groups:\n",
    "\n",
    "[8(277.6-272.45)^2 + 9(267.3-272.45)^2]= 450.88\n",
    "\n",
    "**Step 5:** Calculating degrees of freedom:\n",
    "\n",
    "Here, degrees of freedom= No. of groups- 1= 2-1= 1\n",
    "\n",
    "**Step 6:** Computing variance between:\n",
    "\n",
    "[8(277.6-278.9)^2 + 9(291.8-279.55)^2+ 9(267.3-279.55)2]/[2-1] = 450.88\n",
    "\n",
    "**Step 7:** Computing the F-score: \n",
    "\n",
    "we will get,\n",
    "\n",
    "F-score=$\\frac {450.88}{592.2}= 0.76$\n",
    "\n",
    "\n",
    "It can be seen from the F-Score that group 1 explains more variance than group 2.\n",
    "\n",
    "Conclusion: Saturday Sales will be a better predictor than Friday Sales \n",
    "\n",
    "\n",
    "Similar to the above example, ANOVA table tells the proportion of variance explained by the features with respect to the target variable\n",
    "\n",
    "Obviously the features that explain the largest proportion of the variance should be retained.\n",
    "\n",
    "\n",
    "It has two python implementations in the form of `f_classif` and `f_regression`\n",
    "\n",
    "Since in our Ames Dataset problem we are dealing with a regression problem, we will learn how to implement f_regression score from sklearn library.\n",
    "\n",
    "It's implementation is very similar to the implementation of 'chi-square' score\n",
    "\n",
    "~~~Python\n",
    "data=pd.DataFrame({'A':[1,2,3,4,5], 'B':[2,2,6,6,6], 'C':[4,4,6,10,10], 'D':[1,1,1,1,5], 'E':[2,2,2,1,5]})\n",
    "print('Dataframe:')\n",
    "print(data)\n",
    "\n",
    "#Using ANOVA score to calculate best two features\n",
    "test = SelectKBest(score_func=f_regression, k=2)\n",
    "\n",
    "#Transforming the data based on ANOVA(Target variable is E)\n",
    "data_anova= test.fit_transform(data.iloc[:,:4], data.iloc[:,4])\n",
    "\n",
    "\n",
    "print(\"\\nTwo columns having the highest ANOVA score with respect to 'E'\")\n",
    "print(data_anova)\n",
    "   \n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "Output:\n",
    "\n",
    "```python\n",
    "Dataframe:\n",
    "   A  B   C  D  E\n",
    "0  1  2   4  1  2\n",
    "1  2  2   4  1  2\n",
    "2  3  6   6  1  2\n",
    "3  4  6  10  1  1\n",
    "4  5  6  10  5  5\n",
    "\n",
    "Two columns having the highest ANOVA score with respect to 'E'\n",
    "[[1 1]\n",
    " [2 1]\n",
    " [3 1]\n",
    " [4 1]\n",
    " [5 5]]\n",
    "\n",
    "```\n",
    "\n",
    "If you compare values of E with A and D you can clearly see why Anova Score is highest for them.\n",
    "A has the `highest variance between` when compared to other columns whereas D has the `lowest variance within` when compared to other columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task : Anova Score\n",
    "\n",
    "- Store all the features of `'ames'`(Loaded in the first task) in  a variable called `X`\n",
    "\n",
    "\n",
    "- Store the target variable (`SalePrice`) of `'ames'` in a variable called `y`\n",
    "\n",
    "\n",
    "- Initialise a `\"SelectKBest()\"` with the parameters `score_func=f_regression` & `k=60` and save it to a variable called `'test'`.\n",
    "\n",
    "\n",
    "- Split `'X'` and `'y'` into `X_train,X_test,y_train,y_test` using `train_test_split()` function. Use `test_size = 0.3` and `random_state = 0`\n",
    "\n",
    "\n",
    "- Fit `'test'` on the training data `'X_train'` and `'y_train'` using the `'fit_transform()'` method. Store the result back into `'X_train'`\n",
    "\n",
    "\n",
    "- Transform `'X_test'`using the `'transform()'` method of `test` .Store the result back into `'X_test'`\n",
    "\n",
    "\n",
    "- Initialise a linear regression model with `LinearRegression()` and save it to a variable called `'model'`.\n",
    "\n",
    "\n",
    "- Fit the model on the training data `'X_train'` and `'y_train'` using the `'fit()'` method.\n",
    "\n",
    "\n",
    "- Find out the r^2 score between `X_test` and `'y_test'` using the `'score()'` method and store it in a variable called `'f_regress_score'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7566701199447429\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Code starts here\n",
    "X=ames.drop(['Id','SalePrice'],1)\n",
    "y=ames['SalePrice'].copy()\n",
    "\n",
    "# Splitting the dataframe into train and test\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0) \n",
    "\n",
    "#Initalising the score function\n",
    "test = SelectKBest(score_func=f_regression, k=60)\n",
    "\n",
    "\n",
    "#Fitting and transforming the model on X_train\n",
    "X_train= test.fit_transform(X_train, y_train)\n",
    "\n",
    "#Fitting and transforming the model on X_test\n",
    "X_test= test.transform(X_test)\n",
    "\n",
    "#Initialising the Linear Regression Model\n",
    "model=LinearRegression()\n",
    "\n",
    "#Fitting the model\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "#Finding the model score\n",
    "f_regress_score=model.score(X_test,y_test)\n",
    "print(f_regress_score)\n",
    "# Code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Wrapper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Wrapper methods\n",
    "\n",
    "**Overview of wrapper method**\n",
    "\n",
    "![Wrapper Method](../images/Wrapper.jpg)\n",
    "\n",
    "\n",
    "     * Wrapper Methods generate models with different subsets of feature and gauge their model performances.\n",
    "\n",
    "    * Wrapper methods can give high classification accuracy for particular classifiers, but generally they have high computational complexity\n",
    "\n",
    "Following are the wrapper methods:\n",
    "\n",
    "* Forward Selection\n",
    "\n",
    "* Backward Selection\n",
    "\n",
    "* RFE\n",
    "\n",
    "Before discussing RFE, let's briefly look at Forward Selection and Backward Selection.\n",
    "\n",
    "\n",
    "**Forward Selection**\n",
    "\n",
    "Forward selection is an iterative technique in which we begin with having no features in the model. In every cycle, we continue including features which best enhances our model till an adding of another variable does not enhance the performance of the model.\n",
    "\n",
    "Consider the following python code:\n",
    "```python\n",
    "#Creating a dataframe\n",
    "data=pd.DataFrame({'A':[1,2,3,4,5,6,7,8,9,10],  'B':[4,4,6,10,10,4,4,6,10,10], 'C':[1,1,1,1,5,1,1,1,1,5], 'D':[2,2,2,1,5,2,2,2,1,5]})\n",
    "\n",
    "print('Dataframe:')\n",
    "print(data)\n",
    "\n",
    "#Only selecting feature B\n",
    "X=data[['B']]\n",
    "y=data['D'].copy()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0) \n",
    "model =LinearRegression()\n",
    "\n",
    "#Fitting the model\n",
    "model.fit(X_train,y_train)\n",
    "print(\"Score when features [B] is selected:\", model.score(X_test,y_test))\n",
    "\n",
    "```\n",
    "**Output**\n",
    "\n",
    "```python\n",
    "Dataframe:\n",
    "    A  B   C  D  E\n",
    "0   1  2   4  1  2\n",
    "1   2  2   4  1  2\n",
    "2   3  6   6  1  2\n",
    "3   4  6  10  1  1\n",
    "4   5  6  10  5  5\n",
    "5   6  2   4  1  2\n",
    "6   7  2   4  1  2\n",
    "7   8  6   6  1  2\n",
    "8   9  6  10  1  1\n",
    "9  10  6  10  5  5\n",
    "\n",
    "Score when features [B] is selected: 0.06698063840920998\n",
    "```\n",
    "***\n",
    "\n",
    "Let's add `C` to the model as well\n",
    "\n",
    "***\n",
    "```python\n",
    "\n",
    "\n",
    "#Selecting B,C\n",
    "X=data[['B','C']]\n",
    "y=data['D'].copy()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0) \n",
    "model =LinearRegression()\n",
    "\n",
    "#Fitting the model\n",
    "model.fit(X_train,y_train)\n",
    "print(\"Score when features [B,C] is selected:\", model.score(X_test,y_test))\n",
    "```\n",
    "**Output**\n",
    "\n",
    "```python\n",
    "Score when features [B,C] is selected: 0.9904640813731723\n",
    "\n",
    "```\n",
    "***\n",
    "\n",
    "Let's now add `A` to the model.\n",
    "\n",
    "***\n",
    "```python\n",
    "#Selecting A,B,C\n",
    "X=data[['A','B','C']]\n",
    "y=data['D'].copy()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0) \n",
    "model =LinearRegression()\n",
    "\n",
    "#Fitting the model\n",
    "model.fit(X_train,y_train)\n",
    "print(\"Score when features [A,B,C] is selected:\", model.score(X_test,y_test))\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "**Output**\n",
    "\n",
    "```python**\n",
    "Score when features [A,B,C] is selected: 0.9812539002371345\n",
    "```\n",
    "\n",
    "The score increased when predicting using `B` & `C`  but adding of `A` decreased it. Therefore we stop the iteration process.\n",
    "\n",
    "\n",
    "\n",
    "**Backward Selection**\n",
    "\n",
    "In backward selection, we do the opposite of forward selection. We start with all the features and remove the least significant feature after each iteration which improves the performance of the model. We continue this until no improvement is observed on removal of features.\n",
    "\n",
    "Consider the following python code:\n",
    "\n",
    "```python\n",
    "#Creating a dataframe\n",
    "data=pd.DataFrame({'A':[1,2,3,4,5,6,7,8,9,10], 'B':[4,4,6,10,10,4,4,6,10,10], 'C':[1,1,1,1,5,1,1,1,1,5], 'D':[2,2,2,1,5,2,2,2,1,5]})\n",
    "\n",
    "print('Dataframe:')\n",
    "print(data)\n",
    "\n",
    "#Selecting A,B,C\n",
    "X=data[['A','B','C']]\n",
    "y=data['D'].copy()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0) \n",
    "model =LinearRegression()\n",
    "\n",
    "#Fitting the model\n",
    "model.fit(X_train,y_train)\n",
    "print(\"Score when features [A,B,C] is selected:\", model.score(X_test,y_test))\n",
    "\n",
    "\n",
    "```\n",
    "**Output**\n",
    "\n",
    "```python**\n",
    "Score when features [A,B,C] is selected: 0.9812539002371345\n",
    "```\n",
    "***\n",
    "Let's remove `A` from the model \n",
    "***\n",
    "```python\n",
    "\n",
    "\n",
    "#Removing A from the dataframe\n",
    "X=data[['B','C']]\n",
    "y=data['D'].copy()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0) \n",
    "model =LinearRegression()\n",
    "\n",
    "#Fitting the model\n",
    "model.fit(X_train,y_train)\n",
    "print(\"Score when features [B,C] is selected:\", model.score(X_test,y_test))\n",
    "```\n",
    "**Output**\n",
    "\n",
    "```python\n",
    "Score when features [B,C] is selected: 0.9904640813731723\n",
    "\n",
    "```\n",
    "***\n",
    "Removing `C` from the model\n",
    "***\n",
    "```python\n",
    "\n",
    "#Only selecting feature B\n",
    "X=data[['B']]\n",
    "y=data['D'].copy()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0) \n",
    "model =LinearRegression()\n",
    "\n",
    "#Fitting the model\n",
    "model.fit(X_train,y_train)\n",
    "print(\"Score when features [B] is selected:\", model.score(X_test,y_test))\n",
    "\n",
    "```\n",
    "**Output**\n",
    "\n",
    "```python\n",
    "Score when features [B] is selected: 0.06698063840920998\n",
    "```\n",
    "\n",
    "In the above method, the score increased when we droppped `A`  but dropping of `C` resuled in drastic decrease of score. Therefore we stop the iteration process.\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "Though effective in certain cases, these two methods(Forward and Backward) can provide problems when dealing with especially large or highly-dimensional datasets. \n",
    "\n",
    "\n",
    "Though not popularly used, you can implement the same using [mlxtend](https://rasbt.github.io/mlxtend/api_subpackages/mlxtend.feature_selection/) library\n",
    "\n",
    "\n",
    "**Recursive Feature Elimination**\n",
    "\n",
    "RFE method involves repeatedly constructing a specific model and selecting the most impactful (or least impactful feature), setting that feature aside and then repeating the process with the rest of the features. This process is iterated until all features in the dataset are used up(or other stopping criteria are satisfied).\n",
    "\n",
    "Features are then ranked according to when they were eliminated. In this way, RFE is able to work out the best subset of features that will enhance the performance of the model.\n",
    "\n",
    "\n",
    "![](../images/rfe.jpg)\n",
    "\n",
    "Let's see its python implementation.\n",
    "\n",
    "For this we will use a subset of our original dataset containing only 200 datapoints.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "#Creating a sample of 200 data points\n",
    "df_new=ames.sample(200,random_state=49)\n",
    "\n",
    "X = df_new.drop(['SalePrice'],1)\n",
    "y=df_new['SalePrice'].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0) \n",
    "\n",
    "#Selecting model as Linear Regressor\n",
    "model =LinearRegression()\n",
    "model.fit(X_train,y_train)\n",
    "print(\"Score before RFE:\", model.score(X_test,y_test))\n",
    "\n",
    "#Passing the model along with no. of features you wish to select\n",
    "selector = RFE(model,13)\n",
    "\n",
    "#Fitting the data with the above conditions\n",
    "X_train_rfe = selector.fit_transform(X_train, y_train)\n",
    "X_test_rfe=selector.transform(X_test)\n",
    "model.fit(X_train_rfe,y_train)\n",
    "\n",
    "print(\"Score after RFE:\",model.score(X_test_rfe,y_test))\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Output:\n",
    "```python\n",
    "\n",
    "Score before RFE: 0.599523748467163\n",
    "\n",
    "Score after RFE: 0.8038362233621985\n",
    "    \n",
    "```\n",
    "\n",
    "Following is a image comparing the 13 features(and their correlation score with the target variable `SalePrice`) selected by the `RFE method` and `Pearson coefficent method` for the above sampled dataset: \n",
    "\n",
    "![](../images/pc_v_rfe.png)\n",
    "\n",
    "\n",
    "Its interesting to note that RFE isn't selecting features which have the max correlation with target variable. It's also inherently identifying `correlation between features` as well.\n",
    "\n",
    "\n",
    "RFE will always provide the most optimised features and almost always results in good performance, but it has major drawback. This whole process of going back and forth with the features is a very time consuming process. For datasets with large feature space(which is almost always in any ML problem), this method is not a good choice. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task : RFE\n",
    "\n",
    "Let's now try to implement RFE and see the score in the whole credit dataset.\n",
    "\n",
    "In this task we will also try to identify the optimum no. of features to use\n",
    "\n",
    "- Store all the features of `'ames'`(Loaded in the first task) in  a variable called `X`\n",
    "\n",
    "\n",
    "- Store the target variable (`SalePrice`) of `'ames'` in a variable called `y`\n",
    "\n",
    "\n",
    "- Three variables `'nof_list'`, `'high_score'` and `'nof'` are already defined for you.\n",
    "\n",
    "\n",
    "- Run a `n` loop passing through each element of `'nof_list'`.\n",
    "\n",
    "\n",
    "- Inside the loop, split `'X'` and `'y'` into `X_train,X_test,y_train,y_test` using `train_test_split()` function. Use `test_size = 0.3` and `random_state = 0`\n",
    "\n",
    "\n",
    "- Initialise a linear regression model with `LinearRegression()` and save it to a variable called `'model'`.\n",
    "\n",
    "\n",
    "- Initialise a `RFE()` object with parameters `'model'` & `'n'` and store it to a variable called `'rfe'`.\n",
    "\n",
    "\n",
    "- Fit `'rfe'` on the training data `'X_train'` and `'y_train'` using the `'fit_transform()'` method. Store the result into `'X_train_rfe'`\n",
    "\n",
    "\n",
    "- Transform `'X_test'`using the `'transform()'` method of `rfe` .Store the result into `'X_test_rfe'`\n",
    "\n",
    "\n",
    "- Fit the model on the training data `'X_train_rfe'` and `'y_train'` using the `'fit()'` method.\n",
    "\n",
    "\n",
    "- Write a condition to store the highest R2 score of all `n`. Store the highest R2 score in `'high score'` and the `n` assosciated with it in `'nof'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of features= 30 gives the best score= 0.7627843526086595\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction with RFE\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "#no of features list\n",
    "nof_list=[20,30,40,50,60,70,80]\n",
    "\n",
    "#Variable to store the highest score\n",
    "high_score=0\n",
    "\n",
    "#Variable to store the optimum features\n",
    "nof=0\n",
    "\n",
    "#Code begins here\n",
    "X = ames.drop(['SalePrice'],1)\n",
    "y=ames['SalePrice'].copy()\n",
    "\n",
    "#Loop to select the optimum features\n",
    "for n in nof_list:\n",
    "    X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0) \n",
    "    model = LinearRegression()\n",
    "    rfe = RFE(model, n)\n",
    "    X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "    X_test_rfe = rfe.transform(X_test)\n",
    "    model.fit(X_train_rfe,y_train)\n",
    "    if model.score(X_test_rfe,y_test)>high_score:\n",
    "        high_score=model.score(X_test_rfe,y_test)\n",
    "        feat=n\n",
    "\n",
    "\n",
    "#Printing the no. features with the highest score along with the highest score\n",
    "print(\"No. of features=\",feat, \"gives the best score=\",high_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Embedded Methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 : Overview of embedded method\n",
    "\n",
    "\n",
    "This approach is a hybrid of filter and wrapper methods and is implemented by algorithms that have their own built-in feature selection methods.\n",
    "These methods are thus `embedded` in the algorithm either as its normal or extended functionality. \n",
    "\n",
    "***\n",
    "**Embedded method is a type of filter method because:**\n",
    "\n",
    "This procedure of feature selection can be understood as adding a penalty to reduce the degree of overfitting. This results in weights of features become very small(or 0),therefore filtering out unnecessary features.\n",
    "***\n",
    "***\n",
    "\n",
    "**Embedded method is a type of wrapper method because:**\n",
    "\n",
    "In this approach feature selection is performed during the process of training of the model itself and therefore is specific to the learning algorithms.\n",
    "***\n",
    "\n",
    "\n",
    "Embedded methods usually achieve high accuracy characteristic to wrapper methods and high efficiency characteristic to filter methods\n",
    "\n",
    "Most commonly used embedded feature selection methods are regularization methods namely LASSO and RIDGE.\n",
    "\n",
    "## 4.2 LASSO/RIDGE\n",
    "\n",
    "Both types of regularization have already been covered extensively in the `Advanced Linear Regression` module. \n",
    "\n",
    "Just to refresh,\n",
    "\n",
    "**Lasso (L1):** It stands for *Least Absolute Shrinkage and Selection Operator* and adds **absolute value of magnitude of coefficient** as penalty term to the loss function. Mathematically, the new regularized cost function becomes:  $$J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x_i)-y_i)^2 + \\lambda\\sum_{i=1}^{n}|\\theta_i|$$ \n",
    "\n",
    "\n",
    "\n",
    "**Ridge (L2):** Ridge regression adds **squared magnitude of coefficient** as penalty term to the loss function. The new cost function becomes: \n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x_i)-y_i)^2 + \\lambda\\sum_{i=1}^{n}{\\theta_i}^2$$ \n",
    "\n",
    "In both the above techniques of feature selection, the penalty term helps regularise the feature coefficents and therefore automatically eliminate the unnecessary features and select the relevant ones while building the model itself.\n",
    "\n",
    "Implementing it is the same as implementing a ML model because feature selection happens internally. \n",
    "\n",
    "Let's now apply the above regularization techniques in our ames dataset problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task : LASSO/RIDGE\n",
    "\n",
    "In this task we will try to implement Linear Regression with Lasso and Ridge regularisation.\n",
    "\n",
    "- Split `'X'` and `'y'` into `X_train,X_test,y_train,y_test` using `train_test_split()` function. Use `test_size = 0.3` and `random_state = 0`\n",
    "\n",
    "\n",
    "- Initialise a `\"Lasso()\"` with the parameter `random_state=0`and save it to a variable called `'lasso'`.\n",
    "\n",
    "\n",
    "- Fit the model on the training data `'X_train'` and `'y_train'` using the `'fit()'` method of `'lasso'`.\n",
    "\n",
    "\n",
    "- Find out the r^2 score between `X_test` and `'y_test'` using the `'score()'` method of `'lasso'` and store it in a variable called `'lasso_score'`\n",
    "\n",
    "\n",
    "- Initialise a `\"Ridge()\"` with the parameter `random_state=0`and save it to a variable called `'ridge'`.\n",
    "\n",
    "\n",
    "- Fit the model on the training data `'X_train'` and `'y_train'` using the `'fit()'` method of `'ridge'`.\n",
    "\n",
    "\n",
    "- Find out the r^2 score between `X_test` and `'y_test'` using the `'score()'` method of `'ridge'` and store it in a variable called `'ridge_score'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6671828548745306\n",
      "0\n",
      "0.6671828548745306\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Code starts here\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0) \n",
    "\n",
    "#Initialsing \\ lasso model\n",
    "lasso = Lasso(random_state=0)\n",
    "\n",
    "# Fitting the model with train\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "#Finding the score of model\n",
    "lasso_score=lasso.score(X_test,y_test)\n",
    "print(lasso_score)\n",
    "\n",
    "# checking how many feature coefficients are zero\n",
    "print(sum(lasso.coef_ == 0))\n",
    "\n",
    "#Initialising the ridge model\n",
    "ridge=Ridge(random_state=0)\n",
    "# Fitting the model with train\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "#Finding the score of the model\n",
    "ridge_score = lasso.score(X_test,y_test)\n",
    "print(ridge_score)\n",
    "\n",
    "# checking how many feature coefficients are zero\n",
    "print(sum(ridge.coef_ == 0))\n",
    "# Code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(165.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. PCA\n",
    "\n",
    "Another closely related topic with Feature Selection is Feature Extraction.\n",
    "\n",
    "Feature Extraction refers to the method of reducing the known variables of the data into lesser number of principal variables holding the same amount of information.\n",
    "\n",
    "Though it sounds very similar to Feature Selection(because both are just different techniques of dimensionality reduction), one thing to keep in mind is that in this process we are aiming to capture the variance of the data and nothing else.\n",
    "\n",
    "One of the popular Dimensionality Reduction techniques is Principal Component Analysis(PCA)\n",
    "\n",
    "**5.1 Intuition**\n",
    "\n",
    "Consider the following plot:\n",
    "\n",
    "![2d](../images/2d.png)\n",
    "\n",
    "It has the following x and y axis configuration:\n",
    "\n",
    "\n",
    "![2dpoints](../images/2dpoints.png)\n",
    "\n",
    "\n",
    "It can be intuitively seen that in the above diagram direction pointed by 'red' line looks more important than the 'green'.(The line made in the direction pointed by 'red' is a better fit for the data points)\n",
    "\n",
    "If we transform our x and y axes to red and green arrows, we get the following :\n",
    "\n",
    "![pca](../images/pca.png)\n",
    "\n",
    "If we refer to red as principal component 1 and green as principal component 2, we get the following configuration:\n",
    "\n",
    "![pca](../images/pcapoints.png)\n",
    "\n",
    "It can be clearly seen that the by dropping pc2, we don't lose much variation present in the data.\n",
    "\n",
    "Similar to the above example, for data having multiple features ,we can identify which 'directions' are important & explain the most variance and in effect drop the 'directions' which are least important.\n",
    "\n",
    "\n",
    "**5.2 Definition** \n",
    "\n",
    "Principal Component Analysis or PCA is the method of transforming original variables into a new set of variables such that they are orthogonal (and hence linearly independent) and then ranking according to the variance of data along them. These newly extracted variables are called Principal Components.\n",
    "\n",
    "Principal components are extracted in such a way that the first principal component explains maximum variance in the dataset.\n",
    "\n",
    "Second principal component(uncorrelated to the first) tries to explain the remaining variance(not explained by the first).\n",
    "\n",
    "Third principal component explains the variance not explained by first and second and so on.\n",
    "\n",
    "\n",
    "**5.3 Working of PCA**\n",
    "\n",
    "To understand the working of PCA, it's important for you to have a knowledge of eigenvalues and eigenvectors. You can refresh the concept here(link to be pasted).\n",
    "\n",
    "Follwing are its steps:\n",
    "\n",
    "Step 1:\n",
    "Calculate covariance of matrix X(Feature matrix).\n",
    "\n",
    "Step 2:\n",
    "Calculate eigen vectors and corresponding eigen values of X.\n",
    "\n",
    "Step 3:\n",
    "Sort the eigen vectors according to their eigen value (in decreasing order).\n",
    "\n",
    "Step 4:\n",
    "Use Eigenvectors corresponding to the (k)largest eigenvalues to reconstruct a large fraction of variance of the original data.\n",
    "\n",
    "Let's try to understand the steps better using an example.\n",
    "\n",
    "***\n",
    "Consider a sample of our IOWA housing dataset:\n",
    "\n",
    "\n",
    "\n",
    "It has 80 features and one target variable. \n",
    "\n",
    "Before the first step, let's scale the features.\n",
    "\n",
    "***\n",
    "**Q:** Why do we need to scale it?\n",
    "\n",
    "**A:** The end result of PCA is to calculate a new projection of your data set. \n",
    "\n",
    "As in many other multivariate procedures one needs to ensure that no extra weight is given to the “larger” variables which otherwise will lead to biased outcomes.\n",
    "\n",
    "For eg:\n",
    "In the event we decide to change the scale of one of the features from metres to cm, the resulting variance of the feature would be 100 times more than it previously was. This would result in that feature playing a major role deciding the first PC since the PCA algorithm would be biased towards this new feature, in order to maximize its variance.\n",
    "\n",
    "Scaling the data, all variables will have the same standard deviation, thus none of the variables will have any bias and PCA can calculate relevant axis.\n",
    "\n",
    "***\n",
    "\n",
    "After scaling, our first row of feature matrix will look something like:\n",
    "\n",
    "```python\n",
    "[[-0.38780276 -0.88641057 -0.06590224 -1.73003297  5.61934547  0.07088812\n",
    "   0.02132492 -1.38156245  0.34076569  0.         -1.06331739 -0.17586311\n",
    "   .................................................. \n",
    "   0.09853293  0.5514855  -0.14023183 -0.10655645 -1.24153445 -0.60510586\n",
    "   0.33428219 -1.43182551]\n",
    "  .......................................................... \n",
    "  ..........................................................\n",
    "\n",
    "]]\n",
    "```\n",
    "\n",
    "Converting the above matrix into a covariance matrix, we get,\n",
    "\n",
    "```python\n",
    "\n",
    "Covariance matrix \n",
    "[[ 1.00502513  0.0461804  -0.08984672 ... -0.02916568  0.01314686\n",
    "  -0.08965875]\n",
    " .......................................................\n",
    " \n",
    " [-0.08965875  0.05498568 -0.0809293  ... -0.07523145  0.31478126\n",
    "   1.00502513]]\n",
    "```\n",
    "Finding the eigenvalues and eigenvectors of the above matrix and sorting them in descending order we get, we get,\n",
    "\n",
    "```python\n",
    "\n",
    "Eigenvalues \n",
    "[ 9.91860031e+00  4.92366237e+00  3.78092267e+00  3.56179086e+00\n",
    "  3.39293240e+00  2.95161032e+00  2.74565913e+00  2.33694130e+00\n",
    "  ..............................................................\n",
    "  9.30686007e-02  9.62727861e-02  1.13943247e-01  1.30156014e-01\n",
    "  1.34252254e-01 -6.77986291e-16 -7.78746845e-16  0.00000000e+00]\n",
    "\n",
    "```\n",
    "***\n",
    "**Note:**\n",
    "PCA implementations perform a Singular Vector Decomposition instead of eigendecomposition of the covariance to improve the computational efficiency. \n",
    "\n",
    "The $V^{T}$ in the SVD formula $X=USV^{T}$ is nothing but the above eigenvectors matrix(Why?) \n",
    "\n",
    "***\n",
    "After sorting of the eigenpairs, we need to decide \"how many principal components are we going to choose for our new feature subspace?\". We do this using \"explained variance,\" which can be calculated from the eigenvalues. \n",
    "\n",
    "The explained variance tells us how much information (variance) can be attributed to each of the principal components.\n",
    "\n",
    "\n",
    "Following is the plot for the same:\n",
    "\n",
    "![PCA_plot](../images/pca_graph.png)\n",
    "\n",
    "\n",
    "In the plot above we can observe that most of the variance (More than 80% ) can be explained by the first 30 principal component alone. While the remaining components(50) have very less information to give and therefore can be dropped.\n",
    "\n",
    "Our projection matrix will therefore look like \n",
    "\n",
    "```python\n",
    "\n",
    " \n",
    "Projection Matrix M([80 x 30]) :\n",
    "\n",
    "[[ 0.00448071 -0.01350817  0.12326238 ...  0.19900511  0.05865323\n",
    "   0.05688455]\n",
    " [-0.01759388  0.07024879  0.19174906 ...  0.02688533 -0.06747264\n",
    "  -0.01217759]\n",
    "  ..............................................................\n",
    " [-0.01299174 -0.01049473  0.03013888 ... -0.03344006  0.00668279\n",
    "  -0.12497142]\n",
    " [ 0.06469133  0.10257877 -0.03185315 ... -0.12730551  0.15506105\n",
    "  -0.12573368]]\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Transforming our original matrix X using projection Matrix M, we get:\n",
    "\n",
    "```python\n",
    "Y=X.M                       \n",
    "\n",
    "=[[ 2.19814018  0.34126792 -1.60125569 ...  0.78203228 -0.21044354\n",
    "  -0.62360616]\n",
    " [ 0.35239026 -2.05793391  1.12301188 ...  1.53015364  0.24685173\n",
    "  -0.58152253]\n",
    " ...............................................................\n",
    " [-2.51897468 -3.00342066  1.61685692 ... -0.96805527 -0.28230475\n",
    "   0.19131301]\n",
    " [-0.68028629 -2.64923105  1.59040403 ... -0.38116799 -1.86049732\n",
    "  -0.12103677]]\n",
    "\n",
    "Dimensions of Y: [200 x 30]\n",
    "```\n",
    "\n",
    "**5.4 Scikit implementation**\n",
    "\n",
    "Though we have learned the step by step process, python has a simpler implementation in scikit-learn with all the steps internally taken care of.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "df = pd.read_csv('../data/Cleaned_Data.csv')\n",
    "df_new=df.sample(200,random_state=0)\n",
    "\n",
    "X = df.drop(['SalePrice'],1)\n",
    "\n",
    "print(\"\\nFirst two rows of X matrix:\")\n",
    "print(X.iloc[0:2,])\n",
    "\n",
    "scaler=StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"\\nFirst two rows of scaled X matrix:\")\n",
    "print(X_scaled[0:2,])\n",
    "\n",
    "\n",
    "pca = PCA(n_components=30)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "print(\"\\nFirst two rows of pca transformed X matrix:\")\n",
    "print(X_pca[0:2,])\n",
    "\n",
    "\n",
    "```\n",
    "Output\n",
    "\n",
    "```python\n",
    "\n",
    "First two rows of X matrix:\n",
    "    \n",
    "     MSSubClass  MSZoning  LotFrontage  LotArea  Street  Alley  LotShape  \\\n",
    "0             60         3         65.0     8450       1      1         3   \n",
    "1             20         3         80.0     9600       1      1         3   \n",
    "\n",
    "   LandContour  Utilities      ...        ScreenPorch  PoolArea  PoolQC  \\\n",
    "0            3          0      ...                  0         0       3   \n",
    "1            3          0      ...                  0         0       3   \n",
    "\n",
    "   Fence  MiscFeature  MiscVal  MoSold  YrSold  SaleType  SaleCondition  \n",
    "0      4            1        0       2    2008         8              4  \n",
    "1      4            1        0       5    2007         8              4  \n",
    "\n",
    "[2 rows x 80 columns]\n",
    "\n",
    "First two rows of scaled X matrix:\n",
    "    \n",
    "[[-1.73086488  0.07337496 -0.04553194  0.2128772  -0.20714171  0.06423821\n",
    "   0.02469891  0.75073056  0.31466687 -0.02618016  0.60466978 -0.22571613\n",
    "   ......................................................................\n",
    "   0.06330477  0.45744736 -0.1859753  -0.08768781 -1.5991111   0.13877749\n",
    "   0.31386709  0.2085023 ]\n",
    " [-1.7284922  -0.87256276 -0.04553194  0.64574726 -0.09188637  0.06423821\n",
    "   0.02469891  0.75073056  0.31466687 -0.02618016 -0.62831608 -0.22571613\n",
    "   .....................................................................\n",
    "   0.06330477  0.45744736 -0.1859753  -0.08768781 -0.48911005 -0.61443862\n",
    "   0.31386709  0.2085023 ]]\n",
    "\n",
    "First two rows of pca transformed X matrix:\n",
    "    \n",
    "[[ 2.19814018  0.34126792 -1.60125569 ...  0.78203228 -0.21044354\n",
    "  -0.62360616]\n",
    " [ 0.35239026 -2.05793391  1.12301188 ...  1.53015364  0.24685173\n",
    "  -0.58152253]\n",
    " \n",
    " ...............................................................\n",
    " \n",
    " [-2.51897468 -3.00342066  1.61685692 ... -0.96805527 -0.28230475\n",
    "   0.19131301]\n",
    " [-0.68028629 -2.64923105  1.59040403 ... -0.38116799 -1.86049732\n",
    "  -0.12103677]]\n",
    "\n",
    "```\n",
    "\n",
    "Though PCA does help to perform dimensionality reduction but as as rule of thumb consider PCA only if :\n",
    "\n",
    "1. You want to reduce the data dimensions, but aren’t able to identify variables to remove.\n",
    "2. You are comfortable making the independent variables less interpretable.\n",
    "\n",
    "\n",
    "Let's now try and implement PCA on our whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Task\n",
    "\n",
    "In this task we will try to use PCA for dimensionality reduction on our data and then implement linear regression.\n",
    "\n",
    "\n",
    "- Store all the features of `'ames_model_data'` in  a variable called `X`\n",
    "\n",
    "\n",
    "- Store the target variable (`SalePrice`) of `'ames_model_data'` in a variable called `y`\n",
    "\n",
    "\n",
    "- Split `'X'` and `'y'` into `X_train,X_test,y_train,y_test` using `train_test_split()` function. Use `test_size = 0.3` and `random_state = 0`\n",
    "\n",
    "\n",
    "- Initialise a `\"StandardScaler()\"` object and store it to a variable called `'scaler'`.\n",
    "\n",
    "\n",
    "- Scale `'X_train'` using the `fit_transform()` method of `'scaler'` and store the scaled output in `'X_train_scaled'`\n",
    "\n",
    "\n",
    "- Scale `'X_test'`as well using the `transform()` method of `'scaler'` and store the scaled output in `'X_test_scaled'`\n",
    "\n",
    "\n",
    "- Initialise a `\"PCA()\"`object with the parameter `n_components=35` and   and store it to a variable called `'pca'`.\n",
    "\n",
    "\n",
    "- Transform `'X_train_scaled'` using the `fit_transform()` method of `'pca'` and store the scaled output in `'X_train_pca'`\n",
    "\n",
    "\n",
    "- Transform `'X_test_scaled'`as well using the `transform()` method of `'pca'` and store the scaled output in `'X_test_pca'`\n",
    "\n",
    "\n",
    "- Initialise a linear regression model with `LinearRegression()` and save it to a variable called `'model'`.\n",
    "\n",
    "\n",
    "- Fit the model on the training data `'X_train_pca'` and `'y_train'` using the `'fit()'` method of `'model'`.\n",
    "\n",
    "\n",
    "- Find out the r^2 score between `X_test_pca` and `'y_test'` using the `'score()'` method of `'model'` and store it in a variable called `'pca_score'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7624759749537409\n",
      "0.07\n",
      "1.88\n",
      "2.92\n",
      "2.67\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "# Code starts here\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0) \n",
    " \n",
    "#Initialising standard scaler \n",
    "scaler=StandardScaler()\n",
    "\n",
    "#Scaling the features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#Initialising PCA\n",
    "pca = PCA(n_components=35, random_state=0)\n",
    "\n",
    "#Transforming the features\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca=pca.transform(X_test_scaled)\n",
    "\n",
    "#Initialising the model\n",
    "model=LinearRegression()\n",
    "\n",
    "#Fitting the model\n",
    "model.fit(X_train_pca,y_train)\n",
    "\n",
    "#Scoring the model\n",
    "pca_score=model.score(X_test_pca,y_test)\n",
    "print(pca_score)\n",
    "\n",
    "\n",
    "print(np.round(X_train_scaled[25][4],2))\n",
    "\n",
    "print(np.round(X_test_scaled[25][2],2))\n",
    "\n",
    "print(np.round(X_train_pca[25][4],2))\n",
    "print(np.round(X_test_pca[25][2],2))\n",
    "\n",
    "print(np.round(X_train_scaled[25][4],2)==0.07)\n",
    "\n",
    "print(np.round(X_test_scaled[25][2],2)==(1.88))\n",
    "\n",
    "print(np.round(X_train_pca[25][4],2)==2.92)\n",
    "print(np.round(X_test_pca[25][2],2)==2.67)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion\n",
    "\n",
    "Let's look at the table storing the results of the different feature selection techniques(and PCA) that we applied on our AMES dataset:\n",
    "\n",
    "\n",
    "|Feature Selection Method|No. of features selected|r2 Score|\n",
    "|-----|-----|-----|\n",
    "|Linear Regression|80|0.66|\n",
    "|Pearson Correlation|13|0.72|\n",
    "|Chi-Square|60|0.75|\n",
    "|Anova|35|0.75|\n",
    "|RFE|30|0.76|\n",
    "|LASSO|Internal Selection|0.66|\n",
    "|RIDGE|Internal Selection|0.66|\n",
    "|PCA|35|0.76|\n",
    "\n",
    "Does it mean that RFE or PCA are the best methods for feature selection?\n",
    "**NO**\n",
    "\n",
    "The reason there exists so many methods is that there is no single feature selection method that will give best results across all Machine Learning problems.\n",
    "\n",
    "To make the most of the methods mentioned here, the user should know what kind will work best for him given the domain, data and the problem he is trying to solve.\n",
    "\n",
    "If at some point, you are too confused to decide what to use, following is a checklist you can refer to, to help you ease up the feature selection process:\n",
    "\n",
    "**Feature Selection Check list:**\n",
    "***\n",
    "\n",
    "* `Do you have domain knowledge?` If yes, construct a better set of “ad hoc” features that will provide more information gain.\n",
    "\n",
    "\n",
    "* `Are your features all of the same scale?` If no, consider normalizing them.\n",
    "\n",
    "\n",
    "* `Do you suspect interdependence of features?` If yes, expand your feature set by constructing products of features, as much as your computer resources allow \n",
    "\n",
    "\n",
    "* `Do you need to reduce the count of input variables (e.g. for cost, speed or data understanding reasons)?` If no, then construct disjunctive features or weighted sums of features \n",
    "\n",
    "\n",
    "*  `Do you need to assess features individually (e.g. to understand their influence on the systemor because their number is so large that you need to do a first filtering)? `If yes, use a variable ranking method ; else, do it anyway to get baseline results.\n",
    "\n",
    "\n",
    "* `Do you need a predictor?` If no, stop.\n",
    "\n",
    "\n",
    "* `Do you suspect your data is “dirty” (has a few meaningless input patterns and/or noisyoutputs or wrong class labels)? `If yes, detect the outlier examples using the top ranking variables as representation; check and/or discard them.\n",
    "\n",
    "\n",
    "\n",
    "* `Do you know what to try first?` If no, use a linear predictor. Construct a sequence of predictors of same nature using increasing subsets of features. Can you match or improve performance with a smaller subset? If yes, try a non-linear predictor with that subset.\n",
    "\n",
    "\n",
    "* `Do you have new ideas, time, computational resources, and enough examples?` If yes, compare several feature selection methods, including your new idea, correlation coefficients, backward selection and embedded methods . Use linear and non-linear predictors. Select the best approach with model selection .\n",
    "\n",
    "\n",
    "* `Do you want a stable solution (to improve performance and/or understanding)?` If yes, subsample your data and redo your analysis for several “subsets” \n",
    "\n",
    "***\n",
    "\n",
    "The above checklist is made by Isabelle Guyon and Andre Elisseeff the authors of [“An Introduction to Variable and Feature Selection” (PDF)](http://jmlr.csail.mit.edu/papers/volume3/guyon03a/guyon03a.pdf). You can check out the pdf link to learn more about it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END OF NOTEBOOK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe:\n",
      "   A  B   C  D\n",
      "0  1  2   4  1\n",
      "1  2  2   4  1\n",
      "2  3  6   6  1\n",
      "3  4  6  10  1\n",
      "4  5  6  10  5\n",
      "\n",
      "Correlation Matrix:\n",
      "          A         B         C         D\n",
      "A  1.000000  0.866025  0.938315  0.707107\n",
      "B  0.866025  1.000000  0.842701  0.408248\n",
      "C  0.938315  0.842701  1.000000  0.589768\n",
      "D  0.707107  0.408248  0.589768  1.000000\n",
      "\n",
      "Features closely related to D(>0.5):\n",
      "['A' 'C' 'D']\n"
     ]
    }
   ],
   "source": [
    "#Sample Dataframe\n",
    "data=pd.DataFrame({'A':[1,2,3,4,5], 'B':[2,2,6,6,6], 'C':[4,4,6,10,10], 'D':[1,1,1,1,5]})\n",
    "print(\"Dataframe:\")\n",
    "print(data)\n",
    "\n",
    "#Finding the pearson's correlation among variables \n",
    "data_corr=data.corr()\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(data_corr)\n",
    "\n",
    "#Subseting and only taking those features which are strongly correlated to 'D'\n",
    "data_corr_d= data_corr[data_corr['D']>0.5]\n",
    "\n",
    "print(\"\\nFeatures closely related to D(>0.5):\")\n",
    "print(data_corr_d.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe:\n",
      "   A  B   C  D  E\n",
      "0  1  2   4  1  2\n",
      "1  2  2   4  1  3\n",
      "2  3  6   6  1  4\n",
      "3  4  6  10  1  5\n",
      "4  5  6  10  5  1\n",
      "\n",
      "Two columns having the highest chi square score with respect to 'E'\n",
      "[[ 4  1]\n",
      " [ 4  1]\n",
      " [ 6  1]\n",
      " [10  1]\n",
      " [10  5]]\n"
     ]
    }
   ],
   "source": [
    "data=pd.DataFrame({'A':[1,2,3,4,5], 'B':[2,2,6,6,6], 'C':[4,4,6,10,10], 'D':[1,1,1,1,5], 'E':[2,3,4,5,1]})\n",
    "print('Dataframe:')\n",
    "print(data)\n",
    "\n",
    "#Using chi square score to calculate best two features\n",
    "test = SelectKBest(score_func=chi2, k=2)\n",
    "\n",
    "#Transforming the data based on chi square(Target variable is E)\n",
    "data_chi= test.fit_transform(data.iloc[:,:4], data.iloc[:,4])\n",
    "\n",
    "\n",
    "print(\"\\nTwo columns having the highest chi square score with respect to 'E'\")\n",
    "print(data_chi)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe:\n",
      "   A  B   C  D  E\n",
      "0  1  2   4  1  2\n",
      "1  2  2   4  1  3\n",
      "2  3  6   6  1  4\n",
      "3  4  6  10  1  5\n",
      "4  5  6  10  5  1\n",
      "\n",
      "Two columns having the highest ANOVA score with respect to 'E'\n",
      "[[2 1]\n",
      " [2 1]\n",
      " [6 1]\n",
      " [6 1]\n",
      " [6 5]]\n"
     ]
    }
   ],
   "source": [
    "data=pd.DataFrame({'A':[1,2,3,4,5], 'B':[2,2,6,6,6], 'C':[4,4,6,10,10], 'D':[1,1,1,1,5], 'E':[2,3,4,5,1]})\n",
    "print('Dataframe:')\n",
    "print(data)\n",
    "\n",
    "#Using ANOVA score to calculate best two features\n",
    "test = SelectKBest(score_func=f_regression, k=2)\n",
    "\n",
    "#Transforming the data based on ANOVA score(Target variable is E)\n",
    "data_anova= test.fit_transform(data.iloc[:,:4], data.iloc[:,4])\n",
    "\n",
    "\n",
    "print(\"\\nTwo columns having the highest ANOVA score with respect to 'E'\")\n",
    "print(data_anova)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe:\n",
      "   A  B   C  D  E\n",
      "0  1  2   4  1  2\n",
      "1  2  2   4  1  3\n",
      "2  3  6   6  1  4\n",
      "3  4  6  10  1  5\n",
      "4  5  6  10  5  1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mutual_info_regression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-57f63b21b77f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Using mutual info score to calculate best two features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmutual_info_regression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#Transforming the data based on mutual info score(Target variable is E)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mutual_info_regression' is not defined"
     ]
    }
   ],
   "source": [
    "data=pd.DataFrame({'A':[1,2,3,4,5], 'B':[2,2,6,6,6], 'C':[4,4,6,10,10], 'D':[1,1,1,1,5], 'E':[2,3,4,5,1]})\n",
    "print('Dataframe:')\n",
    "print(data)\n",
    "\n",
    "#Using mutual info score to calculate best two features\n",
    "test = SelectKBest(score_func=mutual_info_regression, k=2)\n",
    "\n",
    "#Transforming the data based on mutual info score(Target variable is E)\n",
    "data_anova= test.fit_transform(data.iloc[:,:4], data.iloc[:,4])\n",
    "\n",
    "\n",
    "print(\"\\nTwo columns having the highest mutual information score with respect to 'E'\")\n",
    "print(data_anova)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# #Creating a dummy dataset\n",
    "# X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n",
    "\n",
    "# print(\"X has following no. of features:\", X.shape[1])\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/Cleaned_Data.csv')\n",
    "df_new=df.sample(200,random_state=0)\n",
    "\n",
    "X = df.drop(['SalePrice'],1)\n",
    "y=df['SalePrice'].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0) \n",
    "\n",
    "#Selecting model as Linear Regressor\n",
    "model =LinearRegression()\n",
    "\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "print(\"Score before RFE:\", model.score(X_test,y_test))\n",
    "\n",
    "\n",
    "#Passing the model along with no. of features=30\n",
    "selector = RFE(model,30)\n",
    "\n",
    "#Fitting the data with the above conditions\n",
    "X_train_rfe = selector.fit_transform(X_train, y_train)\n",
    "X_test_rfe=selector.transform(X_test)\n",
    "\n",
    "model.fit(X_train_rfe,y_train)\n",
    "print(\"Score after RFE:\",model.score(X_test_rfe,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "A young bike seller wants to find the optimum bikes to keep at his store each day. He knows that he needs to have more bikes on Fridays than on other days, but he is trying to find if the need for bikes is same also for Mondays. He collects data for the number of transactions each day for two months. Following is the data:\n",
    "\n",
    "Mondays: 276, 323, 298, 256, 277, 309, 312, 265, 311\n",
    "\n",
    "Fridays: 243, 279, 301, 285, 274, 243, 228, 298, 255\n",
    "\n",
    "Our null hypothesis is that across all days the mean value is the same:\n",
    "\n",
    "$H_{o}: mean_{m}=mean_{f} and he decides to use α – .05. \n",
    "\n",
    "He finds:\n",
    "\n",
    "$mean_{m}$ = 291.8\n",
    "\n",
    "$mean_{f}$ = 267.3\n",
    "\n",
    "and the grand mean = 279.55\n",
    "\n",
    "Computing variance within:\n",
    "\n",
    "[(276-291.8)2+…+(311-291.8)2+ (243-267.3)2+…+(255-267.3)2/[18-2]=6251.85\n",
    "\n",
    "Computing variance between:\n",
    "\n",
    "[9(291.8-279.55)2+9(267.3-279.55)2]/[2-1] = 2701.125\n",
    "\n",
    "Computing the F-score, we will get,\n",
    "\n",
    "F-score=$\\frac {6251.8}{2701.125}= 2.31$\n",
    "\n",
    "Critical F-Value based on the degrees of freedom(1,30) would be 4.17(Based on DF Table)\n",
    "\n",
    "Since the F-score is smaller than the critical F-value(Alternatively he could also have compared the p-value with alpha), he concludes that the mean number of bicycles required is equal on both days.\n",
    "\n",
    "\n",
    "Similarly, using this score, the predictor(feature) from the model is removed if it is not statistically significant and we end up having features that are statistically similar to target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "A young bike seller wants to find the optimum bikes to keep at his store each day. He knows that he needs to have more bikes on Saturdays than on other days, but he is trying to find if the need for bikes is constant across the rest of the week. He collects data for the number of transactions each day for two months. Following is the data:\n",
    "\n",
    "Mondays: 276, 323, 298, 256, 277, 309, 312, 265, 311\n",
    "\n",
    "Tuesdays: 243, 279, 301, 285, 274, 243, 228, 298, 255\n",
    "\n",
    "Wednesdays: 288, 292, 310, 267, 243, 293, 255, 273\n",
    "\n",
    "Thursdays: 254, 279, 241, 227, 278, 276, 256, 262\n",
    "\n",
    "\n",
    "Our null hypothesis is that across all days the mean value is the same:\n",
    "\n",
    "$H_{o}: mean_{m}=mean_{tu}=mean_{w}=mean_{th}$ \n",
    "\n",
    "and decides to use α – .05. \n",
    "\n",
    "He finds:\n",
    "\n",
    "$mean_{m}$ = 291.8\n",
    "\n",
    "$mean_{tu}$ = 267.3\n",
    "\n",
    "$mean_{w}$ = 277.6\n",
    "\n",
    "$mean_{th}$ = 259.1\n",
    "\n",
    "and the grand mean = 274.3\n",
    "\n",
    "Computing variance within:\n",
    "\n",
    "[(276-291.8)2+(323-291.8)2+…+(243-267.6)2+…+(288-277.6)2+…+(254-259.1)2]/[34-4]=15887.6/30=529.6\n",
    "\n",
    "Computing variance between:\n",
    "\n",
    "[9(291.8-274.3)2+9(267.3-274.3)2+8(277.6-274.3)2+8(259.1-274.3)2]/[4-1]\n",
    "\n",
    "= 5151.8/3 = 1717.3\n",
    "\n",
    "Computing the F-score, we will get,\n",
    "\n",
    "F-score=$\\frac {1717.3}{529.6}= 3.24$\n",
    "\n",
    "Critical F-Value based on the degrees of freedom(3,30) would be 2.92\n",
    "\n",
    "Since the F-score is larger than the critical F-value(Alternatively he could also have compared the p-value with alpha), he concludes that the mean number of bicycles required is not equal on different days of the week, or at least there is one day that is different from others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "import plotly.tools as tls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    filepath_or_buffer='https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n",
    "    header=None, \n",
    "    sep=',')\n",
    "\n",
    "df.columns=['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid', 'class']\n",
    "df.dropna(how=\"all\", inplace=True) # drops the empty line at file-end\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = df.ix[:,0:4].values\n",
    "y = df.ix[:,4].values\n",
    "\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "print(X_std[0:5,])\n",
    "import numpy as np\n",
    "mean_vec = np.mean(X_std, axis=0)\n",
    "cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)\n",
    "# print('Covariance matrix \\n%s' %cov_mat)\n",
    "\n",
    "cov_mat = np.cov(X_std.T)\n",
    "\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "print('Eigenvectors \\n%s' %eig_vecs)\n",
    "# print('\\nEigenvalues \\n%s' %eig_vals)\n",
    "\n",
    "\n",
    "u,s,v = np.linalg.svd(X_std.T)\n",
    "# print(u)\n",
    "\n",
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "\n",
    "eig_pairs.sort()\n",
    "print(\"sadas\",eig_pairs)\n",
    "eig_pairs.reverse()\n",
    "\n",
    "# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n",
    "print('Eigenvalues in descending order:')\n",
    "for i in eig_pairs:\n",
    "    print(i[0])\n",
    "    \n",
    "    \n",
    "matrix_w = np.hstack((eig_pairs[0][1].reshape(4,1), \n",
    "                      eig_pairs[1][1].reshape(4,1)))\n",
    "print('Projection Matrix M :\\n', matrix_w)\n",
    "\n",
    "\n",
    "Y = X_std.dot(matrix_w)\n",
    "print('\\nY:\\n',Y[0:5,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "print(cum_var_exp)\n",
    "print(var_exp)\n",
    "\n",
    "pca_names=['PCA1','PCA2','PCA3','PCA4']\n",
    "plt.bar(pca_names,var_exp)\n",
    "plt.title(\"Explained Variance by principal components\")\n",
    "# plt.plot(pca_names,cum_var_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "df = pd.read_csv(\n",
    "    filepath_or_buffer='https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n",
    "    header=None, \n",
    "    sep=',')\n",
    "\n",
    "df.columns=['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid', 'class']\n",
    "df.dropna(how=\"all\", inplace=True) \n",
    "\n",
    "X = df.iloc[:,0:4].values\n",
    "print(\"\\nFirst five rows of X matrix:\")\n",
    "print(X[0:5,])\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "print(\"\\nFirst five rows of scaled X matrix:\")\n",
    "print(X_scaled[0:5,])\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "print(\"\\nFirst five rows of PCA transformed X matrix:\")\n",
    "print(X_pca[0:5,])\n",
    "\n",
    "# u,s,v = np.linalg.svd(X_scaled)\n",
    "# print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "import plotly.tools as tls\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/Cleaned_Data.csv')\n",
    "df_new=df.sample(200,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = df_new.drop('SalePrice',1)\n",
    "y = df_new['SalePrice']\n",
    "\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "# print(X_std[0:2,])\n",
    "import numpy as np\n",
    "mean_vec = np.mean(X_std, axis=0)\n",
    "cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)\n",
    "# print('Covariance matrix \\n%s' %cov_mat)\n",
    "\n",
    "cov_mat = np.cov(X_std.T)\n",
    "\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "# print('Eigenvectors \\n%s' %eig_vecs)\n",
    "# print('\\nEigenvalues \\n%s' %eig_vals)\n",
    "\n",
    "\n",
    "u,s,v = np.linalg.svd(X_std.T)\n",
    "# print(u)\n",
    "\n",
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort()\n",
    "\n",
    "eig_pairs.reverse()\n",
    "\n",
    "# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n",
    "# print('Eigenvalues in descending order:')\n",
    "# for i in eig_pairs:\n",
    "#     print(i[0])\n",
    "\n",
    "matrix_w=(eig_pairs[0][1].reshape(80,1))\n",
    "for i in range(1,30):   \n",
    "    matrix_w=np.concatenate((matrix_w,eig_pairs[i][1].reshape(80,1)),1)\n",
    "    \n",
    "\n",
    "# matrix_w = np.hstack((eig_pairs[0][1].reshape(80,1), \n",
    "#                       eig_pairs[1][1].reshape(80,1)))\n",
    "\n",
    "print('Projection Matrix M :\\n', matrix_w)\n",
    "print(matrix_w.shape)\n",
    "\n",
    "\n",
    "Y = X_std.dot(matrix_w)\n",
    "print(Y.shape)\n",
    "# print(Y)\n",
    "print('\\nTransformed Matrix Y :\\n',Y[0:5,])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "plt.figure(figsize=(18,10))\n",
    "# print(cum_var_exp)\n",
    "# print(var_exp)\n",
    "pca_names=[]\n",
    "for i in range(80):\n",
    "    pca_names.append(str(i))\n",
    "# plt.bar(pca_names,var_exp)\n",
    "plt.title(\"Explained Variance by principal components\")\n",
    "plt.plot(pca_names,cum_var_exp)\n",
    "plt.plot([30]*83, range(83),color='green')\n",
    "plt.plot(range(0,31), [82]*31,color='green')\n",
    "plt.xticks([0,25,50,75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "df = pd.read_csv('../data/Cleaned_Data.csv')\n",
    "df_new=df.sample(200,random_state=0)\n",
    "\n",
    "X = df.drop(['SalePrice'],1)\n",
    "\n",
    "print(\"\\nFirst two rows of X matrix:\")\n",
    "print(X.iloc[0:2,])\n",
    "\n",
    "scaler=StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"\\nFirst two rows of scaled X matrix:\")\n",
    "print(X_scaled[0:2,])\n",
    "\n",
    "\n",
    "pca = PCA(n_components=30)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "print(\"\\nFirst two rows of pca transformed X matrix:\")\n",
    "print(X_pca[0:2,])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "How do we know which features help and which don't?\n",
    "\n",
    "For that we need to understand the information contributed by the features:\n",
    "\n",
    "\n",
    "1) Entropy of a feature\n",
    "\n",
    "Entropy is characterized as the proportion of the irregularity in the data being handled. The higher the entropy, the harder it is to reach any inferences from that data. \n",
    "\n",
    "For eg: Flipping a coin is an activity that gives data that is irregular. \n",
    "\n",
    "In Data Science, entropy of a feature say x1 is figured by excluding x1 and afterward ascertaining the entropy of the remaining features. \n",
    "\n",
    "Lower the entropy  (excluding x1) the higher will be the information content of x1. \n",
    "\n",
    "At the end, features are selected based on some threshold value. Thus entropy of the features can give good information.\n",
    "\n",
    "\n",
    "2) Mutual Information between features\n",
    "\n",
    "The mutual information (MI) of two random variables is defined as the measure of the mutual dependence between the two variables. It quantifies the amount of information obtained about one random variable through observing the other random variable. \n",
    "\n",
    "The concept of mutual information is intricately linked to that of entropy of a random variable. \n",
    "\n",
    "The features which have high mutual information value with respect to the target variable are considered optimal since they can influence the predictive model towards making the right prediction and hence increase the performance of the model.\n",
    " \n",
    "\n",
    "3443.8\n",
    "4552.96\n",
    "5450.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# %matplotlib inline\n",
    "# from matplotlib import pyplot as plt\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# # Code starts here\n",
    "\n",
    "\n",
    "# ames = pd.read_csv('../data/train.csv')\n",
    "# print(ames.head())\n",
    "# obj_df = ames.select_dtypes(include=['object']).copy()\n",
    "\n",
    "# for col in obj_df.columns:\n",
    "#     ames[col].fillna('NA',inplace=True)\n",
    "#     le=LabelEncoder()\n",
    "#     ames[col]=le.fit_transform(ames[col]) \n",
    "\n",
    "# num_df = ames.select_dtypes(include=['number']).copy()\n",
    "\n",
    "# for col in num_df.columns:\n",
    "#     ames[col].fillna(0,inplace=True)\n",
    "    \n",
    "    \n",
    "# ames.to_csv(\"../images/Cleaned_Data.csv\", index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "\n",
    "ames = pd.read_csv('../data/Cleaned_Data.csv')\n",
    "# ames_train=ames.sample(600, random_state=0)\n",
    "# ames_test=ames[~ames['Id'].isin(ames_train['Id'].values)]\n",
    "\n",
    "\n",
    "\n",
    "X=ames.drop(['Id','SalePrice'],1)\n",
    "y=ames['SalePrice'].copy()\n",
    "\n",
    "    \n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0) \n",
    "\n",
    "\n",
    "model=LinearRegression()\n",
    "model.fit(X_train,y_train)\n",
    "score=model.score(X_test,y_test)\n",
    "print(score)\n",
    "\n",
    "\n",
    "# code ends here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame({'A':[1,2,3,4,5], 'B':[2,2,6,6,6], 'C':[4,4,6,10,10], 'D':[1,1,1,1,5], 'E':[2,2,2,1,5]})\n",
    "print('Dataframe:')\n",
    "print(data)\n",
    "\n",
    "#Using ANOVA score to calculate best two features\n",
    "test = SelectKBest(score_func=f_regression, k=2)\n",
    "\n",
    "#Transforming the data based on ANOVA(Target variable is E)\n",
    "data_anova= test.fit_transform(data.iloc[:,:4], data.iloc[:,4])\n",
    "\n",
    "\n",
    "print(\"\\nTwo columns having the highest ANOVA score with respect to 'E'\")\n",
    "print(data_anova)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe\n",
    "data=pd.DataFrame({'A':[1,2,3,4,5,6,7,8,9,10],  'B':[4,4,6,10,10,4,4,6,10,10], 'C':[1,1,1,1,5,1,1,1,1,5], 'D':[2,2,2,1,5,2,2,2,1,5]})\n",
    "\n",
    "print('Dataframe:')\n",
    "print(data)\n",
    "\n",
    "\n",
    "#Selecting A,C,D\n",
    "X=data[['A','C','D']]\n",
    "y=data['E'].copy()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0) \n",
    "model =LinearRegression()\n",
    "\n",
    "#Fitting the model\n",
    "model.fit(X_train,y_train)\n",
    "print(\"Score when features [A,C,D] is selected:\", model.score(X_test,y_test))\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "#Selecting C,D\n",
    "X=data[['C','D']]\n",
    "y=data['E'].copy()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0) \n",
    "model =LinearRegression()\n",
    "\n",
    "#Fitting the model\n",
    "model.fit(X_train,y_train)\n",
    "print(\"Score when features [C,D] is selected:\", model.score(X_test,y_test))\n",
    "\n",
    "############################################################################\n",
    "\n",
    "#Only selecting feature C\n",
    "X=data[['C']]\n",
    "y=data['E'].copy()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0) \n",
    "model =LinearRegression()\n",
    "\n",
    "#Fitting the model\n",
    "model.fit(X_train,y_train)\n",
    "print(\"Score when features [C] is selected:\", model.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "#Creating a sample of 200 data points\n",
    "df_new=ames.sample(200,random_state=49)\n",
    "\n",
    "X = df_new.drop(['SalePrice'],1)\n",
    "y=df_new['SalePrice'].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0) \n",
    "\n",
    "#Selecting model as Linear Regressor\n",
    "model =LinearRegression()\n",
    "model.fit(X_train,y_train)\n",
    "print(\"Score before RFE:\", model.score(X_test,y_test))\n",
    "\n",
    "#Passing the model along with no. of features you wish to select\n",
    "selector = RFE(model,13)\n",
    "\n",
    "#Fitting the data with the above conditions\n",
    "X_train_rfe = selector.fit_transform(X_train, y_train)\n",
    "X_test_rfe=selector.transform(X_test)\n",
    "model.fit(X_train_rfe,y_train)\n",
    "\n",
    "print(\"Score after RFE:\",model.score(X_test_rfe,y_test))\n",
    "\n",
    "# print(selector.support_)  # The mask of selected features.\n",
    "# print(selector.ranking_)  # The feature ranking.\n",
    "\n",
    "List=[]\n",
    "for i in range(len(df_new.columns)-1):\n",
    "    if selector.support_[i]:\n",
    "        List.append(df_new.columns[i])\n",
    "print(List)\n",
    "score={}\n",
    "for i in range(len(List)):\n",
    "\n",
    "    score[List[i]]=abs(y_train.corr(X_train[List[i]],method='pearson'))\n",
    "    \n",
    "fig, (ax_1, ax_2) = plt.subplots(1,2, figsize=(20,8))    \n",
    "# print(score)    \n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter   \n",
    "s1=OrderedDict(sorted(score.items(), key = itemgetter(1), reverse = False))\n",
    "# plt.bar(range(len(score)), list(score.values()), align='center')\n",
    "ax_1.bar(list(s1.keys()), list(s1.values()), align='center')\n",
    "ax_1.set_title('Features selected by RFE method')\n",
    "# ax_1.set_xticks()\n",
    "\n",
    "####################################\n",
    "\n",
    "X_train['Class']=y_train\n",
    "t_corr=X_train.corr()\n",
    "t_corr=t_corr['Class']\n",
    "\n",
    "corr_columns=t_corr[abs(t_corr)>0.5].index\n",
    "corr_columns=corr_columns.drop('Class')\n",
    "\n",
    "X_train_new=X_train[corr_columns]\n",
    "\n",
    "X_test_new=X_test[corr_columns]\n",
    "\n",
    "# X_train_new=\n",
    "\n",
    "model=LinearRegression()\n",
    "model.fit(X_train_new,y_train)\n",
    "corr_score=model.score(X_test_new,y_test)\n",
    "print(corr_score)\n",
    "print(len(X_train_new.columns))\n",
    "\n",
    "\n",
    "List_2=X_train_new.columns\n",
    "score_2={}\n",
    "for i in range(len(List_2)):\n",
    "#     score_2[List_2[i]]=abs(df_new['SalePrice'].corr(df_new[List_2[i]],method='pearson'))\n",
    "        score_2[List_2[i]]=abs(y_train.corr(X_train[List_2[i]],method='pearson'))\n",
    "# print(score_2)\n",
    "\n",
    "# n=['Utilities','Condition2','PoolArea','PoolQC','Street']\n",
    "# for i in n:\n",
    "#     score_2[i]=0\n",
    "s2=OrderedDict(sorted(score_2.items(), key = itemgetter(1), reverse = False))\n",
    "print(s2)\n",
    "# plt.bar(range(len(score_2)), list(score_2.values()), align='center')\n",
    "ax_2.bar(list(s2.keys()), list(s2.values()), align='center')\n",
    "ax_2.set_title('Features selected by Correlation method')\n",
    "\n",
    "ax_2.set_xticks(list(s2.keys()))\n",
    "\n",
    "for ax in fig.axes:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# print(list(s2.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz\n",
    "\n",
    "1. Which of the following are good reasons to implement PCA?\n",
    "\n",
    "a) As a method to implement regularisation by reducing features and preventing overfitting\n",
    "\n",
    "b) To visualise high dimensional data(by choosing k=2 or k=3) [Answer]\n",
    "\n",
    "c) As a method to speed up a learning algorithm that lags with high dimensional data [Answer]\n",
    "\n",
    "d) To compress the data to take up less disk space [Answer]\n",
    "\n",
    "2. If the input features are on very different scales, PCA automatically takes care of it and performs dimensionality reduction.\n",
    "\n",
    "a) True\n",
    "\n",
    "b) False [Answer]\n",
    "\n",
    "3. F-score is defined as \n",
    "\n",
    "a) Variance between the samples/ Variance within the samples [Answer]\n",
    "\n",
    "b) Variance within the samples/ Variance between the samples\n",
    "\n",
    "c) Variance within the samples * Variance between the samples\n",
    "\n",
    "d) Variance between the samples * Variance within the samples\n",
    "\n",
    "\n",
    "4. Which among the following is not a filter method?\n",
    "\n",
    "a) Correlation Coefficient \n",
    "\n",
    "b) Anova\n",
    "\n",
    "c) L1 Regularization [Answer]\n",
    "\n",
    "\n",
    "5. PCA is a method of feature selection\n",
    "\n",
    "a) True\n",
    "\n",
    "b) False [Answer]\n",
    "\n",
    "\n",
    "\n",
    "6. Which one of the following methods should be preferred when dealing with large amounts of data?\n",
    "\n",
    "\n",
    "a) Filter methods [Answer]\n",
    "\n",
    "b) Wrapper methods\n",
    "\n",
    "\n",
    "7. Which of the following methods takes into account the information dependency between the evaluated features?\n",
    "\n",
    "a) Wrapper methods [Answer]\n",
    "\n",
    "b) Filter methods\n",
    "\n",
    "\n",
    "\n",
    "8. PCA method is a feature selection method \n",
    "\n",
    "a) True \n",
    "\n",
    "b) False [Answer]\n",
    "\n",
    "\n",
    "9. Embedded methods will always perform better than the filter methods and wrapper methods because of its hybrid nature.\n",
    "\n",
    "a) True\n",
    "\n",
    "b) False [Answer]\n",
    "\n",
    "\n",
    "10. You should implement feature selection methods before the data cleaning methods because that would result in lesser features to clean and preprocess\n",
    "\n",
    "a) False [Answer]\n",
    "\n",
    "b) True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
