{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "\n",
    "## Description\n",
    "\n",
    "Although a traditional subject in classical statistics, you can also view **Regression** from a machine learning point of view. You'll learn more about the predictive capabilities and performance of regression algorithms. At the end of this chapter you'll be acquainted with simple linear regression, multi-linear regression.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- Motivation for Linear Regression through the **Ames Housing dataset**\n",
    "- Assumptions for Linear Regression\n",
    "- Ordinary Least Squares method\n",
    "- Error metrics like RMSE, R-squared, MAE\n",
    "\n",
    "\n",
    "## Pre-requisite\n",
    "\n",
    "- Python (along with NumPy and pandas libraries)\n",
    "- Basic statistics (knowledge of central tendancy)\n",
    "\n",
    "\n",
    "## Learning Outcomes\n",
    "\n",
    "- Validating linear regression assumptions\n",
    "- Determining co-efficients of best fit line with the help of scikit-learn (using OLS) \n",
    "- Understanding different error metrics   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Why linear regression? \n",
    "\n",
    "### Description: In this chapter you will be introduced to the problem statement at hand i.e. the Ames Housing dataset and how you can leverage linear regression to address the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Introduction to the problem statement: <font color='green'> Predict the Sale Price of the houses</font>\n",
    "\n",
    "\n",
    "**What is the problem?**\n",
    "\n",
    "Let’s get started to make a prediction on our first machine learning algorithm with a rich dataset on housing prices from Ames, Iowa.Each row in the dataset describes the properties of a single house as well as the amount it was sold for. In this concept, we'll build models that predict the final sale price of a house based on its other attributes. The original data set contains 82 features and 2930 data points.You can read more about this dataset [here](http://ww2.amstat.org/publications/jse/v19n3/decock.pdf).\n",
    " \n",
    "However, for the purpose of understanding how Linear Regression works, we will specifically, work on the following features of the house\n",
    "\n",
    "- ExterQual\n",
    "- AllFlrsSF \n",
    "- GrLivArea\n",
    "- SimplOverallCond\n",
    "- GarageArea\n",
    "- TotRmsAbvGrd\n",
    "- LotFrontage\n",
    "\n",
    "**Brief explanation of the dataset & features**\n",
    "\n",
    "* `ExterQual (Ordinal)`: Evaluates the quality of the material on the exterior \n",
    "      5: Excellent\n",
    "      4: Good\n",
    "      3: Average/Typical\n",
    "      2: Fair\n",
    "      1: Poor\n",
    "    \n",
    "* `AllFlrsSF(Continuous)`: Total square feet for 1st and 2nd floor combined\n",
    " \n",
    "* `GrLivArea (Continuous)`: Above grade (ground) living area square feet\n",
    "\n",
    "* `SimplOverallCond (Ordinal)`: Rates the overall condition of the house\n",
    "      1: Bad\n",
    "      2: Average\n",
    "      3: Good\n",
    "      \n",
    "* `Garage Area (Continuous)`: Size of garage in square feet\n",
    "* `TotRmsAbvGrd (Nominal)`: Total rooms above grade (does not include bathrooms)\n",
    "* `LotFrontage (Continuous)`: Linear feet of street connected to property\n",
    "\n",
    "\n",
    "**What we want as outcome?**\n",
    "\n",
    "Using the set of some basic attributes that are related to the price of the house, predict the sale price for a new  house using Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Need for Linear Regression\n",
    "\n",
    "***\n",
    "\n",
    "**Intuition for Linear regression**\n",
    "\n",
    "Let's start with what information we have: The main goal is to build a machine learning model which can predict the selling price of the house given some its features like `GrLivArea`, `Garage Area` etc. If you do a scatter plot our data with `SalePrice` which is the target variable and `GrLivArea`, a feature of the house, we might get something similar to the following:\n",
    "\n",
    "<img src='../images/plt.png'>\n",
    "\n",
    "\n",
    "**Know Your Linear Regression**\n",
    "\n",
    "In simple linear regression, we establish a relationship between target variable and input variables by fitting a line, known as the regression line.\n",
    "\n",
    "In general, a line can be represented by linear equation $y = mx + b$. Where, $y$ is the dependent variable, $x$ is the independent variable, $m$ is the slope, $b$ is the intercept.\n",
    "\n",
    "In machine learning, we rewrite our equation as $y(x) = \\theta_0 + \\theta_1x$ where $\\theta_i$s are the parameters of the model, $x$ is the input, and $y$ is the target variable. This is the standard notation in machine learning, and makes it easier to add more dimensions. We can simply add variables $\\theta_2, \\theta_3, ..., \\theta_n$ and $x_2, x_3, ...$ as we add more dimensions. \n",
    "\n",
    "**Different values of $\\theta_0$ and $\\theta_1$ will give us different lines**.\n",
    "\n",
    "\n",
    "Each of the values of the parameters determine what predictions the model will make. For example, let's consider $(\\theta_0, \\theta_1) = (0.0, 0.2)$, and the first data point, where $x = 3456$ and $y_{true} = 600$. The prediction made by the model, $y(x) = 0.0 + 0.2*3456 = 691.2$. If instead the weights were $(\\theta_0, \\theta_1) = (80.0, 0.15)$, then the prediction would be $y(x) = 80.0 + 0.15*3456 = 598.4$, which is much closer to the $y_{true} = 600$.\n",
    "\n",
    "This difference between the actual value of the target and the predicted value is called the **residual**. You should always select the line with the least residual as the line fits the data points appropriately.\n",
    "\n",
    "\n",
    "\n",
    "**Why Linear Regression for this data**\n",
    "\n",
    "We observed linear relationship between the features to the price of the house (which is a continuous real valued number), so, it is a regression task. So, `Linear Regression` will do a good job in predicting our target i.e. **Sale Price**. \n",
    "\n",
    "\n",
    "In our dataset the linear relationship between the features and target will be captured in the following form:\n",
    "\n",
    "\n",
    "***\n",
    "$Sales Price = \\theta_0 + \\theta_1*ExterQual + \\theta_2*AllFlrsSF + \\theta_3*GarageArea + \\theta_4*SimplOverall Cond + \\theta_5*GrLivArea + \\theta_6*TotRmsAbvGrd + \\theta_7*LotFrontage$ \n",
    "***\n",
    "\n",
    "\n",
    "Here, our main goal is to find the values of the co-efficients $\\theta_0$, $\\theta_1$, $\\theta_2$, $\\theta_3$ and $\\theta_4$, $\\theta_5$, $\\theta_6$ and $\\theta_7$ to find the best fit line representing our data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a look at the data set \n",
    "\n",
    "In this task you will load the training and test datasets named as `Train.csv` and `Test.csv` from their respective filepath variables and have a look at them. \n",
    "\n",
    "### Instructions\n",
    "- Load the datasets from their filepaths\n",
    "- Split each one of them into features and target. The target variable is the last column in both the dataframes \n",
    "- Print out the first $5$ instances for training and test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ExterQual  AllFlrsSF  GrLivArea  SimplOverallCond  GarageArea  \\\n",
      "0          4       1316       1316                 2         397   \n",
      "1          4       2028       2028                 2         880   \n",
      "2          3       1072       1072                 2         525   \n",
      "3          3       1048       1048                 2         286   \n",
      "4          3       1779       1779                 2         546   \n",
      "\n",
      "   TotRmsAbvGrd  LotFrontage  \n",
      "0             6         65.0  \n",
      "1             7         88.0  \n",
      "2             5         35.0  \n",
      "3             6         72.0  \n",
      "4             6         80.0  \n",
      "0    169990\n",
      "1    369900\n",
      "2    140000\n",
      "3    135000\n",
      "4    197900\n",
      "Name: SalePrice, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "train_path = '../data/train.csv'\n",
    "test_path = '../data/test.csv'\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "# read data\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "\n",
    "\n",
    "# split into features and target\n",
    "X_train, y_train = train.iloc[:,:7], train.iloc[:,7]\n",
    "X_test, y_test = test.iloc[:,:7], test.iloc[:,7]\n",
    "\n",
    "# display first five rows of training features and target\n",
    "print(X_train.head())\n",
    "print(y_train.head())\n",
    "\n",
    "# Code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "1. What is the slope of a line parallel to the X-axis?\n",
    "\n",
    "    a. 1\n",
    "    \n",
    "    b. 0\n",
    "  \n",
    "**ANS:** 0\n",
    "\n",
    "**Explaination:** Obviously it is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Assumptions of Linear Regression\n",
    "\n",
    "### Description: Before proceeding further, its important to know about the key assumptions that are made whilst dealing with Linear Regression. These are pretty intuitive and very essential to understand as they play an important role in finding out some relationships in our dataset too! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Linear Relationship Assumption\n",
    "\n",
    "***\n",
    "\n",
    "According to this assumption the relationship between response (Dependent Variables) and feature variables (Independent Variables) should be linear.\n",
    "\n",
    "\n",
    "**Why it is important?**\n",
    "- Linear regression only captures the linear relationship, as its trying to fit a linear model to the data.\n",
    "  \n",
    "  \n",
    "**How to validate it?**\n",
    "- The linearity assumption can be tested using scatter plots. \n",
    "  \n",
    "\n",
    "Below shown in a scatter plot for `SalePrice` vs `GarageArea`. You can clearly see that a linear pattern is evident here i.e. as the value of `GarageArea` increases the `SalePrice` also increases and vice-versa.\n",
    " \n",
    " <img src='../images/plt.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Little or No Multicollinearity Assumption\n",
    "\n",
    "***\n",
    "\n",
    "**What is multicollinearity?**\n",
    "\n",
    "It is assumed that there is little or no multicollinearity in the data. But what do we mean by multicollinearity? Well, multicollinearity occurs when **independent variables in a regression model are correlated**. This correlation is a problem because independent variables should be independent. If the degree of correlation between variables is high enough, it can cause problems when you fit the model and interpret the results. \n",
    "\n",
    "  \n",
    "**Why sweat over multicollinearity?**\n",
    "\n",
    "The interpretation of a regression coefficient is that it represents the mean change in the dependent variable for each unit change in an independent variable when you hold all of the other independent variables constant. However, when independent variables are correlated, changes in one variable in turn shifts another variable/variables. The stronger the correlation, the more difficult it is to change one variable without changing another. It becomes difficult for the model to estimate the relationship between each independent variable and the dependent variable independently because the independent variables tend to change in unison.\n",
    "\n",
    "\n",
    "\n",
    "**Effects of multicollinearity**\n",
    "\n",
    "- It results in unstable parameter estimates which makes it very difficult to assess the effect of independent variables.\n",
    "- Weakens the statistical power of regression model\n",
    "\n",
    "     \n",
    "**How to validate it?**\n",
    "\n",
    "- Multicollinearity occurs when the features (or independent variables) are not independent from each other. Pair plots of features help validate. \n",
    "- You can also calculate correlation coefficient (Pearson or Spearman) to figure out which features are correlated.\n",
    "\n",
    "\n",
    "**Treating multicollinearity**\n",
    "\n",
    "- Remove some of the highly correlated independent variables.\n",
    "- Linearly combine the independent variables, such as adding them together.\n",
    "\n",
    "\n",
    "Below shown is a pair plot for all the features we have at hand. Feel free to investigate it. \n",
    "    \n",
    "<img src='../images/pairplot.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Homoscedasticity Assumption\n",
    "\n",
    "***\n",
    "\n",
    "Homoscedasticity describes a situation in which the error term (that is, the “noise” or random disturbance in the relationship between the independent variables and the dependent variable) is the same across all values of the independent variables. \n",
    "\n",
    "\n",
    "**Why it is important**:\n",
    "- Generally, non-constant variance arises in presence of outliers or extreme leverage values.\n",
    "\n",
    "\n",
    "**How to validate**:\n",
    "- Plot between error (residuals) vs fitted/predicted values. If there is no fan-shaped pattern visible, then it satisfies this assumption.\n",
    "\n",
    "In the image below, shown are three different plots for residuals ($\\text{True value} - \\text{Predicted value}$) and $\\text{Predicted value}$. Lets discuss these plots in detail:\n",
    "\n",
    "- The left two plots are instances of heteroscedasticity where the variance either increases (left plot) or decreases (right plot). So, it violates the assumption of constant variance. \n",
    "\n",
    "- The rightmost plot is an instance satisfying the assumption of constant variance the residuals. \n",
    "\n",
    "**The residual plot i.e. Residuals** ($\\text{True value} - \\text{Predicted value}$) vs **Predicted value plot should aspire to achieve the third plot for linear regression.** \n",
    "    \n",
    "<img src='../images/homos.jpg'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Little or No autocorrelation in residuals\n",
    "\n",
    "***\n",
    "\n",
    "There should be little or no autocorrelation in the data. Autocorrelation occurs when the residual errors are not independent from each other.\n",
    "\n",
    "\n",
    "**Why it is important**:\n",
    "- The presence of correlation in error terms drastically reduces model's accuracy. This usually occurs in time series models. If the error terms are correlated, the estimated standard errors tend to underestimate the true standard error.\n",
    "\n",
    "\n",
    "**How to validate**:\n",
    "- Residual vs Time plot: Look for the seasonal or correlated pattern in residual values.\n",
    "\n",
    "\n",
    "In the plot below shown are two plots where the regression line (right plot) deviates from the true trend. Hence, it is necessary to take care of it.\n",
    "\n",
    "\n",
    "<img src=\"https://image.ibb.co/cZTk28/assumption4.png\" alt=\"drawing\" width=\"500px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Normal Distribution of error terms\n",
    "\n",
    "***\n",
    "\n",
    "A common misconception about linear regression is that it assumes that the outcome $Y$ is normally distributed. Actually, linear regression assumes normality for the residual errors ϵ, which represent variation in $Y$ which is not explained by the predictors. \n",
    "\n",
    "\n",
    "**Why it is important**:\n",
    "- Due to the Central Limit Theorem, we may assume that there are lots of underlying facts affecting the process and the sum of these individual errors will tend to behave like in a zero mean normal distribution. In practice, it seems to be so.\n",
    "\n",
    "**How to validate**:\n",
    "- You can look at QQ plot of the residuals\n",
    "- You should observe a normal curve on plotting a histogram of residuals\n",
    "- The residual plot \n",
    "\n",
    "The following graphs were taken after we have calculated the residuals and made a Q-Q plot and a histogram for residuals. Observing them we can tell that they are very close to satisfying the normal distribution assumption.\n",
    "\n",
    "<img align=left src='../images/hist.png' width=450> \n",
    "\n",
    "<img align=right src='../images/Qplot.png' width=450>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why you should care about these assumptions?**\n",
    "\n",
    "In a nutshell, your linear model should produce residuals that have a constant variance and are normally distributed, features are not correlated with themselves or other features etc. If these assumptions hold true, the OLS procedure (discussed in the next chapter) creates the best possible estimates for the coefficients of linear regression. \n",
    "\n",
    "Another benefit of satisfying these assumptions is that as the sample size increases to infinity, the coefficient estimates converge on the actual population parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Ordinary Least Squares\n",
    "\n",
    "***\n",
    "\n",
    "### Description: In this chapter you will learn about how to determine the best fit line using the method of Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Mathematical notation\n",
    "\n",
    "***\n",
    "\n",
    "**Least Squares** \n",
    "\n",
    "Let's recall the equation we have considered for our problem statement earlier. \n",
    "$$\\mathbf{y} = \\begin{bmatrix} \\mathbf{1} & \\mathbf{x}_1 & \\mathbf{x}_2 & \\mathbf{x}_3 & \\mathbf{x}_4 & \\mathbf{x}_5 & \\mathbf{x}_6 & \\mathbf{x}_7 \\\\ \\end{bmatrix} \\times \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\\\ \\theta_3 \\\\ \\theta_4 \\\\ \\theta_5 \\\\ \\theta_6 \\\\ \\theta_7 \\\\ \\end{bmatrix} + \\begin{bmatrix}  \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\varepsilon_3 \\\\ \\varepsilon_4 \\\\ \\varepsilon_5 \\\\ \\varepsilon_6 \\\\ \\varepsilon_7 \\\\   \\end{bmatrix}$$\n",
    "where  $\\mathbf{x}_i$  corresponds to the feature of the house,  $\\theta_i$  corresponds to the parameters/coefficients that need to be calculated,   $\\varepsilon_i$  is the error term and  $\\mathbf{y}$  is the target variable - Sales Price. Now this was for our specific problem, let's generalize it. \n",
    "\n",
    "$$X = \\begin{bmatrix} \\mathbf{1} & \\mathbf{x}_1 & \\mathbf{x}_2 & \\cdots & \\mathbf{x}_k \\\\ \\end{bmatrix}$$\n",
    "Now this is a generalized matrix for k features (plus the constant 1 column). If we expand on each of the k features for n observations we get,\n",
    "$$X = \\begin{bmatrix} 1 & x_{11} & x_{12} & \\cdots & x_{1k}\\\\ 1 & x_{21} & x_{22} & \\cdots & x_{2k}\\\\ \\vdots & \\vdots & \\vdots & \\cdots & \\vdots \\\\ \\vdots & \\vdots & \\vdots & \\cdots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & \\cdots & x_{nk}\\\\ \\end{bmatrix}_{n\\times (k+1)}$$ which is a  $n\\times (k+1)$  matrix (k+1 to accomodate for column of 1). \n",
    "\n",
    "Then  $\\mathbf{y}$  can be generalized to  $n\\times 1$  vector of observations on the target variable.\n",
    "$$\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ \\vdots \\\\ y_n \\\\ \\end{bmatrix}_{n\\times1}$$\n",
    "\n",
    "Let  $\\varepsilon$  be the error term associated with each observation. \n",
    "$$\\varepsilon =  \\begin{bmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\vdots \\\\ \\varepsilon_n \\\\ \\end{bmatrix}_{n\\times1}$$\n",
    "\n",
    "Let  $\\mathbf{\\theta}$  be the coefficient of each of the features. These are the parameters that need to be estimated by the model and there would be k parameters for k features. \n",
    "$$\\mathbf{\\theta} = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\vdots \\\\ \\theta_k \\\\ \\end{bmatrix}_{(k+1)\\times1}$$\n",
    "\n",
    "Combining all these we get, \n",
    "\n",
    "$$\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ \\vdots \\\\ y_n \\\\ \\end{bmatrix}_{n\\times1} = \\begin{bmatrix} 1 & x_{11} & x_{12} & \\cdots & x_{1k}\\\\ 1 & x_{21} & x_{22} & \\cdots & x_{2k}\\\\ \\vdots & \\vdots & \\vdots & \\cdots & \\vdots \\\\ \\vdots & \\vdots & \\vdots & \\cdots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & \\cdots & x_{nk}\\\\ \\end{bmatrix}_{n\\times (k+1)} \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\vdots \\\\ \\theta_k \\\\ \\end{bmatrix}_{(k+1)\\times1} + \\begin{bmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\vdots \\\\ \\varepsilon_n \\\\ \\end{bmatrix}_{n\\times1}$$\n",
    "\n",
    "Or in a compressed form we can express it as \n",
    "$$\\mathbf{y} = X\\theta + \\varepsilon$$\n",
    "\n",
    "Our goal is to estimate the parameters in the  $\\beta$  vector. The vector of residuals is given by  $\\varepsilon = \\mathbf{y} - X\\theta$ . If we take the actual value of the residuals, they might be negative. To avoid that, we take the squared sum of the residuals. \n",
    "\n",
    "$$\\varepsilon_0^2 + \\varepsilon_1^2+\\cdots+\\varepsilon_n^2 = \\begin{bmatrix} \\varepsilon_0& \\varepsilon_1& \\cdots & \\varepsilon_n \\\\ \\end{bmatrix} \\begin{bmatrix} \\varepsilon_0 \\\\ \\varepsilon_1 \\\\ \\vdots \\\\ \\varepsilon_n \\\\ \\end{bmatrix} = \\varepsilon^T\\varepsilon$$\n",
    "\n",
    " $\\varepsilon^T\\varepsilon$  is also known as **cost function** which we want to minimize.\n",
    "\n",
    "Substituting   $\\varepsilon = \\mathbf{y} - X\\theta$  we get \n",
    "\n",
    "$$ \\varepsilon^T\\varepsilon = (\\mathbf{y} - X\\theta)^T (\\mathbf{y} - X\\theta)$$ \n",
    " $$ \\varepsilon^T\\varepsilon  = (\\mathbf{y}^T-(X\\theta)^T)(\\mathbf{y}-X\\theta) $$ \n",
    "$$\\varepsilon^T\\varepsilon = (\\mathbf{y}^T- \\theta^TX^T)(\\mathbf{y}-X\\theta) $$\n",
    "$$\\varepsilon^T\\varepsilon= \\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^TX\\theta - \\theta^TX^T\\mathbf{y} + \\theta^TX^TX\\theta $$\n",
    "$$\\varepsilon^T\\varepsilon= \\mathbf{y}^T\\mathbf{y} - 2\\theta^TX^T\\mathbf{y} + \\theta^TX^TX\\theta  $$\n",
    "\n",
    "\n",
    " $\\mathbf{y}^TX\\theta = \\theta^TX^T\\mathbf{y}$  because the transpose of a scalar is a scalar. \n",
    "\n",
    "Now we need to find the  $\\theta$  that minimizes the sum of squared residuals. (That is why the name - Ordinary Least Squares). To find the  $\\theta$ , we take derivative w.r.t  $\\theta$ . \n",
    "\n",
    "$$ \\frac{\\partial\\varepsilon^T\\varepsilon}{\\partial\\theta} = -2X^T\\mathbf{y} + 2X^TX\\theta = 0 \\\\ (X^TX)\\theta = X^T\\mathbf{y} $$\n",
    "\n",
    " $X^TX$  is a square matrix ( $k+1\\times k+1$ ) and it is also symmetric. By multiplying  $(X^TX)^{-1}$  both sides, we get\n",
    "$$(X^TX)^{-1}(X^TX)\\theta = (X^TX)^{-1}X^T\\mathbf{y} $$\n",
    "$$ \\theta = (X^TX)^{-1}X^T\\mathbf{y} $$\n",
    "\n",
    "Let's consider a bivariate case. Here we have a single feature  $\\mathbf{x}$  which influences a target variable  $\\mathbf{y}$ . Let's consider n observations of the same. Then our linear regression model is  $\\mathbf{y} = \\theta_0 + \\theta_1\\mathbf{x}$ . In the matrix form, it becomes \n",
    "$$\\mathbf{y} = X\\theta + \\varepsilon$$\n",
    "$$\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix} \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\end{bmatrix} +  \\begin{bmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\\\ \\end{bmatrix}$$\n",
    "Solving using  $\\theta = (X^TX)^{-1}X^T\\mathbf{y}$ , we get \n",
    "\n",
    "$$\\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\end{bmatrix} = \\begin{bmatrix} \\bar{y} - \\theta_1\\bar{x} \\\\ \\frac{\\sum_{i=1}^{n} (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{n} (x_i-\\bar{x})^2 } \\\\ \\end{bmatrix}$$\n",
    "\n",
    "You can simplify representing the 2nd term as follows:\n",
    "\n",
    " $$ \\theta_1 = \\frac{SS_{xy}}{SS_{xx}} $$\n",
    "\n",
    "where  $SS_{xy}$  is the sum of cross-deviations of y and x:\n",
    "\n",
    "$$ SS_{xy} = \\sum_{i=1}^{n} (x_i-\\bar{x})(y_i-\\bar{y}) =  \\sum_{i=1}^{n} y_ix_i - n\\bar{x}\\bar{y} $$\n",
    "\n",
    "and  $SS_{xx}$  is the sum of squared deviations of x:\n",
    "\n",
    "$$SS_{xx} = \\sum_{i=1}^{n} (x_i-\\bar{x})^2 =  \\sum_{i=1}^{n}x_i^2 - n(\\bar{x})^2$$\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Discrete Example to understand OLS\n",
    "\n",
    "Lets take an example to clarify the understanding of the above calculation. For simplicity we will consider only one feature i.e. `GarageArea` as our feature vector `X` and we have 'SalePrice' as our target vector `y`. Hence, we have data points as follows\n",
    "\n",
    "\n",
    "| GarageArea (X) | SalePrice (y) |  \n",
    "|----------------|---------------|\n",
    "| 548          | 208500        |\n",
    "| 460          |181500         |\n",
    "| 608          | 223500       |\n",
    "| 642          |140000        |\n",
    "| 836          | 250000        |\n",
    "| 480          |143000        |\n",
    "| 636          | 307000        |\n",
    "| 484          |200000        |\n",
    "| 468          |129900        |\n",
    "| 205          |118000         |\n",
    "\n",
    "Let us first calculate mean of `X` and `y`\n",
    "\n",
    "$$ \\bar{x} = \\frac{548+460+608+642+836+480+636+484+468+205}{10} = 536$$\n",
    "\n",
    "\n",
    "$$ \\bar{y} = \\frac{208500+181500+223500+140000+250000+143000+307000+20000+129900+11800}{10} = 190140$$\n",
    "\n",
    "We now need to calculate $\\sum{y*x}$\n",
    "$$\\sum{y*x} = (208500*548) + (181500*460) + ... + (118000*205) = 1078191200$$\n",
    "\n",
    "Next we need to calculate $\\sum{x^2}$\n",
    "$$\\sum{x^2} = 548^2 + 460^2 + ... + 205^2 = 3122829$$\n",
    "\n",
    "Now we will calculate sum of cross deviations and sum of squared deviations\n",
    "\n",
    "\n",
    "$$ SS_{xy} = \\sum_{i=1}^{n} y_ix_i - n\\bar{x}\\bar{y} =59040800 $$\n",
    "\n",
    "$$SS_{xx} = \\sum_{i=1}^{n} (x_i-\\bar{x})^2 =  \\sum_{i=1}^{n}x_i^2 - n(\\bar{x})^2 = 3122829 - 10*536^2 = 249869$$\n",
    "\n",
    "\n",
    "\n",
    "Now that we have all the values let us calculate slope and intercept\n",
    "\n",
    "$$\\text{slope} = \\theta_1 = \\frac{SS_{xy}}{SS_{xx}} = \\frac{59040800}{249869} = 236 $$\n",
    "$$\\text{intercept} = \\theta_0 = \\bar{y} - \\theta_1\\bar{x} = 190140 - 236*536 = 63644$$\n",
    "\n",
    "So now if we want to predict house price using a linear regression model fit on these 10 data points we can use following equation\n",
    "\n",
    "$$\\text{SalePrice} = 63644 + 236*\\text{GarageArea}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find regression co-efficients for `GarageArea` using OLS\n",
    "\n",
    "In this task you will be calculating the regression coefficients as well as plotting a regression line for `GarageArea` against the target variable `SalePrice`\n",
    "\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- First define a function `estimate_coef()` to estimate the co-efficients of regression which takes two arguments; a feature and a target variable and returns a tuple containing the two coefficents for regression. Consult the material in the topic if you get stuck at any point\n",
    "- Now, define a function `plot_regression_line()` which will plot the regression line and takes three arguments; a feature, a target variable and a tuple which consists of the regression co-efficients\n",
    "- Now first estimate the coefficients of regression line for the feature `GarageArea` with respect to the target using `estimate_coef()` function that you had defined\n",
    "- Using the output of the above step plot the regression line and with the help of `plot_regression_line()` and display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import packages\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Code starts here\n",
    " \n",
    "def estimate_coef(x, y):\n",
    "    # number of observations/points\n",
    "    n = np.size(x)\n",
    " \n",
    "    # mean of x and y vector\n",
    "    m_x, m_y = np.mean(x), np.mean(y)\n",
    " \n",
    "    # calculating cross-deviation and deviation about x\n",
    "    SS_xy = np.sum(y*x - n*m_y*m_x)\n",
    "    SS_xx = np.sum(x*x - n*m_x*m_x)\n",
    " \n",
    "    # calculating regression coefficients\n",
    "    b_1 = SS_xy / SS_xx\n",
    "    b_0 = m_y - b_1*m_x\n",
    " \n",
    "    return(b_0, b_1)\n",
    " \n",
    "def plot_regression_line(x, y, b):\n",
    "    # plotting the actual points as scatter plot\n",
    "    plt.scatter(x, y, color = \"m\",\n",
    "               marker = \"o\", s = 30)\n",
    " \n",
    "    # predicted response vector\n",
    "    y_pred = b[0] + b[1]*x\n",
    " \n",
    "    # plotting the regression line\n",
    "    plt.plot(x, y_pred, color = \"g\")\n",
    " \n",
    "    # putting labels\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    " \n",
    "    # function to show plot\n",
    "    plt.show()\n",
    "\n",
    "# Visualize your results\n",
    "values = estimate_coef(X_train['GarageArea'], y_train)\n",
    "plot_regression_line(X_train['GarageArea'], y_train, values)\n",
    "\n",
    "# Code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "1. The coefficients of the least squares regression line are determined by minimizing the sum of the squares of the\n",
    "\n",
    "    a. X-coordinates\n",
    "    \n",
    "    b. Y-coordinates\n",
    "    \n",
    "    c. residuals\n",
    "    \n",
    "**ANS:** c.residuals\n",
    "\n",
    "**Explaination:** If you didn't get this right, head back to the OLS section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Implementing Linear Regression with scikit-learn\n",
    "\n",
    "### Description: In this chapter you will be using scikit learn to make predictions using linear regression and also evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Model building with scikit-learn\n",
    "\n",
    "***\n",
    "\n",
    "Now that you have gone through the mathematical concept of **Ordinary Least Squares** let us see how we can implement the same in Python using sklearn library. The steps involved for building a model will be as follows: \n",
    "\n",
    "- Train the model using sklearn\n",
    "- Test it on the test set using sklearn\n",
    "\n",
    "The code snippet for model building with scikit learn is described below:\n",
    "```python\n",
    "# import packages\n",
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "# instantiate linear regression model\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# fit model on training data\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on test data\n",
    "pred = linreg.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict `SalePrice` with scikit-learn\n",
    "\n",
    "In this task you are going to make predictions and then evaluate based on different error metrics like RMSE, MAE and R-squared using scikit-learn.\n",
    "\n",
    "### Instructions\n",
    "- Instantiate a linear regression model with `LinearRegression()` and save it to a variable\n",
    "- Transform the target `SalePrice` using logarithmic transformation using `np.log()` to normalize it\n",
    "- Then fit this model on the training data (both features and target) using `.fit()` method of the model\n",
    "- After that make predictions on the test features using `.predict()` method of the model. Save these predictions for calculating the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.11227082 12.19875709 12.17873549 12.09451926 11.79496421 11.62810742\n",
      " 12.23094302 11.66779875 12.83933008 11.42802489 13.03969447 11.48996121\n",
      " 11.96267792 12.15572954 11.84253379 11.80049608 11.63719563 11.95712008\n",
      " 12.33129216 12.18239258 12.02229166 12.0255932  12.23719561 11.87200608\n",
      " 12.44798664 12.643851   12.47162239 12.5982682  11.8215798  11.50148537\n",
      " 11.789251   13.22072258 11.50903908 11.87175444 12.20648447 12.23039239\n",
      " 11.69069064 12.89000353 11.7661653  12.30551334 12.29771967 11.68183125\n",
      " 12.84865739 11.97590572 12.48417858 12.05078167 11.76415223 11.68701845\n",
      " 12.25244997 12.23563131 11.51096238 12.6868023  11.79805816 11.80839364\n",
      " 12.05803133 12.79445006 12.15644059 12.02083321 11.5972928  12.61133737\n",
      " 12.17873549 12.98542578 12.19936653 11.39135943 11.90586361 11.8061422\n",
      " 11.79982215 11.72513481 11.82121504 11.56637513 11.81933974 12.42686173\n",
      " 11.68135512 11.73767058 11.78233782 12.74326629 12.23338334 11.7939831\n",
      " 12.11819825 12.03048394 11.68266492 12.12712056 11.90938147 12.29955162\n",
      " 12.45304782 12.1158874  11.87991759 12.24021247 11.91744569 11.8183186\n",
      " 12.50615175 12.1375194  11.58971462 11.72636454 11.63459047 11.88480366\n",
      " 12.15526438 12.26590528 13.04953318 12.28186559 12.32674689 11.84409995\n",
      " 11.93199572 11.69211204 11.70018651 12.11670782 12.51640236 12.57485591\n",
      " 12.32705351 11.92622652 11.6862094  12.36477339 12.07473212 11.78271\n",
      " 11.32985799 11.73571905 12.62594874 12.81379998 12.20129924 12.51632293\n",
      " 12.21841653 12.5823989  11.71435727 11.89019067 11.96550613 11.5280544\n",
      " 11.67516422 12.4678313  11.58752619 12.48170867 12.47862706 11.84850904\n",
      " 11.69121489 12.29370377 12.44371486 12.28054085 12.24909651 11.61319402\n",
      " 11.91487052 11.86840427 11.64726503 12.59520997 12.16486312 11.84101696\n",
      " 12.06636477 11.93760694 12.3307774  11.70575984 11.63296814 11.89129321\n",
      " 11.82982961 11.90139964 12.19672361 11.85108493 12.92933893 11.98491553\n",
      " 11.57635224 11.58499973 11.50805023 11.89922198 12.04396888 12.04744607\n",
      " 11.95895367 12.21663376 11.67068064 11.70030147 12.40205447 12.42036159\n",
      " 11.51955271 11.80310958 12.32654451 12.11743224 12.00879437 11.93764444\n",
      " 11.99675635 12.10883467 13.14701159 11.85470772 11.63193185 11.75102253\n",
      " 11.80323969 12.70259578 12.15545884 11.93794973 12.12127975 12.36037884\n",
      " 11.82521344 12.00418425 12.07759098 11.82645624 11.44907288 12.17814015\n",
      " 12.04679459 11.57678353 11.66550057 11.8399987  12.00347863 11.5136164\n",
      " 12.7779323  11.9549082  12.40986742 12.07929626 12.6713032  11.91885273\n",
      " 12.17589121 11.85394798 11.75840026 11.46815018 11.84547767 11.94838787\n",
      " 13.18652715 12.55796296 11.70135142 12.48875471 12.55406488 11.77174461\n",
      " 12.27118603 12.14117083 11.69371398 11.93298946 11.7384902  12.970298\n",
      " 11.62537494 11.59287262 12.36147802 12.14998731 11.71136005 12.27409416\n",
      " 11.8740295  11.7751253  12.21957196 12.28853847 12.11493594 11.71193265\n",
      " 12.33250862 12.05561594 12.7067329  12.56709692 12.4100063  11.99481002\n",
      " 12.19385718 11.85050997 11.83435354 11.97667415 11.5138426  12.41560667\n",
      " 11.73020719 12.04863283 12.49596194 12.11497669 11.85274241 11.93520819\n",
      " 12.12038933 11.84397818 12.27161042 12.58709183 11.97757653 12.41630734\n",
      " 11.54758587 12.73344354 11.69102664 11.70250058 12.70721356 12.27331177\n",
      " 12.15188932 12.13637047 11.91545409 12.2461217  12.17794617 12.7493585\n",
      " 11.86406128 11.76375043 11.98230344 12.42741362 12.28738732 12.90041852\n",
      " 12.0158569  11.92495846 11.67720943 11.5858002  11.95900144 11.74488456\n",
      " 11.89465559 11.76096931 12.07657923 11.6994464  11.96580469 12.78918226\n",
      " 12.54892387 12.09242315 12.25502625 11.61421594]\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "# Code starts here\n",
    "\n",
    "# instantiate linear model\n",
    "model = LinearRegression()\n",
    "\n",
    "# fit model on training data\n",
    "model.fit(X_train, np.log(y_train))\n",
    "\n",
    "# predict on test features\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# display predictions\n",
    "print(y_pred)\n",
    "\n",
    "# Code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Mean Absolute Error\n",
    "\n",
    "***\n",
    "\n",
    "Now that you have made your predictions on unseen data, time to test it on unseen data. You also need to have some kind of measure to quantify the performance of the model. This measure is captured by what we call the error metrics and there are many different forms depending on the problem statement. \n",
    "\n",
    "We have a regression problem at hand and the different types of error metric that we can use here are:\n",
    "- Root Mean Squared Error (RMSE)\n",
    "- Mean Absolute Error (MAE)\n",
    "- R-Squared\n",
    "\n",
    "Lets go through them one by one and simultaneously also see how you can calculate it with the help of scikit-learn.\n",
    "\n",
    "\n",
    "**Mean Absolute Error**\n",
    "\n",
    "So what is residual? It is the difference between our prediction and the true value. Mean absolute error is nothing by average of absolute values of these residuals. We can write a simple formula for Mean Absolute Error (MAE) as follows. \n",
    "\n",
    "$$MAE= \\frac{1} {N}{\\sum_{i=1}^N |y_{i} - \\hat{y}_{i}|}$$\n",
    "\n",
    "\n",
    "**Calculating MAE with scikit-learn**\n",
    "\n",
    "Scikit-learn provides a very easy way to calculate MAE. The code snippet is shown below:\n",
    "```python\n",
    "# import packages\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# MAE calculation\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "```\n",
    "\n",
    "The variable `mae` gives us the Mean Absolute Error for our predictions `y_pred` and true target `y_test`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate MAE\n",
    "\n",
    "In this task you will be calculating the MAE using scikit-learn \n",
    "\n",
    "### Instructions\n",
    "- Calculate the MAE for the original target using `mean_absolute_error()` with the test target and prediction as arguments and save it to a variable `mae`\n",
    "- Keep in mind to transform the predicted target (transformed before using log transformation) to its original scale using `np.exp()` on the predicted target\n",
    "- Print it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29158.29503663651\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "# MAE calculation\n",
    "mae = mean_absolute_error(y_test, np.exp(y_pred))\n",
    "print(mae)\n",
    "\n",
    "# Code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Evaluate your model with RMSE\n",
    "\n",
    "***\n",
    "\n",
    "**Root Mean Square Error**\n",
    "\n",
    "- The square root of the mean/average of the square of all of the error.\n",
    "- RMSE is very commonly used and makes for an excellent general purpose error metric for numerical predictions.\n",
    "- Compared to the similar Mean Absolute Error, RMSE amplifies and severely punishes large errors.\n",
    " \n",
    " If $y_{i}$ are the actual values and $\\hat{y}_{i}$ are the predicted values then,\n",
    " \n",
    " $$RMSE = {\\sqrt {\\frac{1} {N}{\\sum\\limits_{i = 1}^N {(y_{i} - \\hat{y}_{i} } })^{2} } }$$\n",
    "\n",
    "\n",
    " **Why are we squaring the Residuals and then taking a root?**\n",
    "   - Residuals are basically the difference between Actual Values & Predicted Values\n",
    "   - Residuals can be positive or negative as the predicted value underestimates or overestimates the actual value\n",
    "   - Thus to just focus on the magnitude of the error we take the square of the difference as it's always positive\n",
    "     \n",
    "   \n",
    " **So what is the advantage of RMS when we could just take the absolute difference instead of squaring**\n",
    "   - This **severely punishes large differences** in prediction. This is the reason why RMSE is powerful as compared to Absolute Error.\n",
    "   - Evaluating the RMSE and tuning our model to minimize it results in a more robust model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate RMSE\n",
    "\n",
    "In this task you will be calculating the RMSE for the fitted model (done in the previous task)\n",
    "\n",
    "### Instructions\n",
    "- First calculate the mean squared error  with `mean_squared_error()` with the test target and prediction as arguments\n",
    "- Then find its square-root which will give the value of **RMSE** and save it to a variable `rmse`\n",
    "- Here also keep in mind to transform the predicted target (transformed before using log transformation) to its original scale using `np.exp()` on the predicted target\n",
    "- Print it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42842.8688912249\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, np.exp(y_pred)))\n",
    "print(rmse)\n",
    "\n",
    "# Code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4.4 $R^2$ \n",
    "\n",
    "***\n",
    "\n",
    "**What is R-Squared?**\n",
    "\n",
    "R-squared is a statistical measure of how close the data are to the fitted regression line i.e. **it measures the goodness of fit of a straight line**. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression. \n",
    "\n",
    "It is a  **measure of the proportion of variability in the response that is explained by the regression model.** Mathematically,\n",
    " \n",
    "$$R-squared = \\frac {\\text{Explained variation}}{\\text{Total variation}}$$\n",
    "\n",
    "R-squared is always between 0 and 100%:\n",
    "   \n",
    " 1. $0$% indicates that the model explains none of the variability of the response data around its mean.\n",
    " \n",
    " <img src='../images/r21.png'>\n",
    "    \n",
    " 2. $100$% indicates that the model explains all the variability of the response data around its mean.\n",
    "\n",
    "<img src='../images/r22.png'>\n",
    "  \n",
    "**Graphical Intuition behind $R^2$** \n",
    "\n",
    "<img src='../images/rsquared.gif'>\n",
    "\n",
    "In the above figure you are given two plots for two different regression models which has fitted responses (predicted) on the Y-axis and observed responses (true) on the Y-axis. In linear regression, you want the predicted values to be close to the actual values. So to have a good fit, that plot should resemble a straight line at $45$ degrees.\n",
    "\n",
    "The regression model on the left accounts for around 30.0% of the variance while the one on the right accounts for around 90%. **So, the more variance that is accounted for by the regression model the closer the data points will fall to the fitted regression line.** \n",
    "\n",
    "Theoretically, if a model could explain 100% of the variance, the fitted values would always equal the observed values and, therefore, all the data points would fall on the fitted regression line.\n",
    "\n",
    "\n",
    "**Mathematical Intuition behind $R^2$**\n",
    "\n",
    "Before we get into calculating $R^2$, lets understand what the different Sum of Squares for our model are: \n",
    "\n",
    "- In RMSE, we have already been introduced to Squared Residuals which is also called Error Sum of Squares (SSE) \n",
    "   $$SSE = {\\sum\\limits_{i = 1}^N {(y_{i} - \\hat{y}_{i} } })^{2} $$\n",
    "   \n",
    "- Additionally, we have the Total Sum of Squares (SST) which is nothing but the Squared difference between the Actual Values (${y}_{i}$) and the Mean of our dataset ($\\bar{y}_{i}$). This is also the baseline model where we predict all the values as the mean of the true values. It doesn’t make use of any independent variables to predict the value of dependent variable $Y$. Instead it uses the mean of the observed responses of dependent variable $Y$ and always predicts this mean as the value of $Y$.\n",
    "\n",
    "    **R-squared simply explains how good is your model when compared to this baseline model.**\n",
    "   \n",
    "   $$SS(Total) = {\\sum\\limits_{i = 1}^N {(y_{i} - \\bar{y}_{i} } })^{2} $$\n",
    "\n",
    "- And we also have Regression Sum of Squares, which is the squared difference between the Predicted values ($\\hat{y}_{i}$) and the Mean ($\\bar{y}_{i}$)\n",
    "    $$SS(Regression) = {\\sum\\limits_{i = 1}^N {(\\hat{y}_{i} - \\bar{y}_{i} }})^{2}$$\n",
    "      \n",
    "Now, intuitively, we can see that: \n",
    "     $$SS(Total) = SS(Regression) + SSE$$ \n",
    "     \n",
    "Thus $R^2$ is defined as: \n",
    "$$ R^2 = \\frac{SS(Regression)}{SS(Total)} = 1 -\\frac{\\text{SSE}}{\\text{SS (Total)}} $$\n",
    "\n",
    "\n",
    "**Pitfalls of R-squared**\n",
    "\n",
    "- R-squared can be made artificially high by adding more number of independent variables although they might be irrelevant. These features enable the model to learn more. As a result the model will fit the data points better or will remain the same; resulting in increased value of R-squared or remaining the same.\n",
    "\n",
    "- R-squared cannot determine whether the coefficient estimates and predictions are biased, which is why you must assess the residual plots.\n",
    "\n",
    "\n",
    "**Adjusted R-squared**\n",
    "\n",
    "To counter the issue of adding more independent variables, you should consider using the metric Adjusted R-squared instead of R-squared. Simply put it penalizes the model for adding irrelevant explanatory variables. Mathematically, $$\\text{Adjusted R-squared} = 1 - (1-R^2)(\\frac{n - 1}{n - p - 1})$$ \n",
    "where \n",
    "\n",
    "$n$ = Number of data points\n",
    "\n",
    "$p$ = Number of explanatory/independent variables\n",
    "\n",
    "So how is R-squared different from Adjusted R-squared? **R-squared tells you how well your model fits the data points whereas Adjusted R-squared tells you how important is a particular feature to your model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate R2 score\n",
    "\n",
    "In this task you will be calculating the R2 score for the model that you had built\n",
    "\n",
    "### Instructions\n",
    "- Calculate the R2 score for the original target using `r2_score()` and store it in a variable `rsquared` \n",
    "    The syntax for the same is \n",
    "    ```python\n",
    "    # import packages\n",
    "    from sklearn.metrics import r2_score\n",
    "\n",
    "    # R-squared calculation\n",
    "    rsqaured = r2_score(y_test, y_pred)\n",
    "\n",
    "    ```\n",
    "    where `y_test` is the original target and `y_pred` is the predicted target\n",
    "    \n",
    "- Here also keep in mind to transform the predicted target (transformed before using log transformation) to its original scale using `np.exp()` on the predicted target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7378446924962254\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "# R-squared calculation\n",
    "rsquared = r2_score(y_test, np.exp(y_pred))\n",
    "print(rsquared)\n",
    "\n",
    "# Code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "1. What will happen to the value of R-squared if we increase the number of features?\n",
    "\n",
    "    a. Increase\n",
    "\n",
    "    b. Remain the same\n",
    "\n",
    "    c. Decrease\n",
    "\n",
    "**ANS:** a. or b.\n",
    "\n",
    "**Explaination:** R-squared can never decrease on addition of features\n",
    "\n",
    "\n",
    "2. A geometric interpretation of a residual is the\n",
    "\n",
    "    a. perpendicular distance from the data point to the regression line.\n",
    "\n",
    "    b. horizontal distance from data point to regression line\n",
    "\n",
    "    c. vertical distance from data point to regression line\n",
    "\n",
    "**ANS:** c.\n",
    "\n",
    "**Explaination:** If you didn't get this right, head back to the R-squared section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
